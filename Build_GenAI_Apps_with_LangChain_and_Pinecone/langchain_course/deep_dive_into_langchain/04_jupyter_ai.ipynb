{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea0e5a9b-5292-43ec-9df9-43281b993237",
   "metadata": {},
   "source": [
    "# Jupyter AI\n",
    "\n",
    "- [Jupyter AI](https://jupyter-ai.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9207850d-159f-4dcd-ad32-ddeaf7ecfad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade pip\n",
    "!pip install jupyter-ai[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2797cb9-6acb-438c-8c86-7a744f5b2abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd4e60ce-26b4-4114-b6ec-a10d26a20107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorial(n):\n",
    "    \"\"\"\n",
    "    Calculate the factorial of a non-negative integer n.\n",
    "\n",
    "    The factorial of a non-negative integer n is the product of all positive integers less than or equal to n.\n",
    "    It is denoted by n! and is defined as follows:\n",
    "    - n! = n * (n-1) * (n-2) * ... * 1 for n > 0\n",
    "    - 0! = 1 by definition\n",
    "\n",
    "    Parameters:\n",
    "    n (int): A non-negative integer whose factorial is to be calculated.\n",
    "\n",
    "    Returns:\n",
    "    int: The factorial of n. Returns \"Undefined for negative numbers\" if n is negative.\n",
    "    \"\"\"\n",
    "\n",
    "    if n < 0:\n",
    "        return \"Undefined for negative numbers\"\n",
    "    elif n == 0 or n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55de3c84-f504-4ed7-bf01-775f0b9ea5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "print(factorial(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0ed7182-af8b-446e-b595-260ba351c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import getpass\n",
    "from pinecone import Pinecone, ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "769847e1-9e47-4d25-a469-690e566ad472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your OPENAI_API_KEY:  ········\n",
      "Enter your LANGCHAIN_API_KEY:  ········\n",
      "Enter your PINECONE_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OPENAI_API_KEY: \")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LANGCHAIN_API_KEY: \")\n",
    "os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your PINECONE_API_KEY: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a753f8ac-8115-4a66-8c59-41053c378f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f44848b-3a0d-4db1-ae1d-4f071fba54c0",
   "metadata": {},
   "source": [
    "## Loading the Ipython Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4855ba38-f511-42a9-8db0-9577bb28fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_ai_magics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aeeba3-8b80-46ba-b91f-75cc6be2872d",
   "metadata": {},
   "source": [
    "## Getting help with the %%ai command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac022d4b-02c2-43a9-8f85-083d49b944e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: %%ai [OPTIONS] [MODEL_ID]\n",
      "\n",
      "  Invokes a language model identified by MODEL_ID, with the prompt being\n",
      "  contained in all lines after the first. Both local model IDs and global\n",
      "  model IDs (with the provider ID explicitly prefixed, followed by a colon)\n",
      "  are accepted.\n",
      "\n",
      "  To view available language models, please run `%ai list`.\n",
      "\n",
      "Options:\n",
      "  -f, --format [code|html|image|json|markdown|math|md|text]\n",
      "                                  IPython display to use when rendering\n",
      "                                  output. [default=\"markdown\"]\n",
      "  -n, --region-name TEXT          AWS region name, e.g. 'us-east-1'. Required\n",
      "                                  for SageMaker provider; does nothing with\n",
      "                                  other providers.\n",
      "  -q, --request-schema TEXT       The JSON object the endpoint expects, with\n",
      "                                  the prompt being substituted into any value\n",
      "                                  that matches the string literal '<prompt>'.\n",
      "                                  Required for SageMaker provider; does\n",
      "                                  nothing with other providers.\n",
      "  -p, --response-path TEXT        A JSONPath string that retrieves the\n",
      "                                  language model's output from the endpoint's\n",
      "                                  JSON response. Required for SageMaker\n",
      "                                  provider; does nothing with other providers.\n",
      "  -m, --model-parameters TEXT     A JSON value that specifies extra values\n",
      "                                  that will be passed to the model. The\n",
      "                                  accepted value parsed to a dict, unpacked\n",
      "                                  and passed as-is to the provider class.\n",
      "  --help                          Show this message and exit.\n",
      "------------------------------------------------------------------------------\n",
      "Usage: %ai [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "  Invokes a subcommand.\n",
      "\n",
      "Options:\n",
      "  --help  Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  delete    Delete an alias. See `%ai delete --help` for options.\n",
      "  error     Explains the most recent error.\n",
      "  help      Show this message and exit.\n",
      "  list      List language models. See `%ai list --help` for options.\n",
      "  register  Register a new alias. See `%ai register --help` for options.\n",
      "  reset     Clear the conversation transcript.\n",
      "  update    Update the target of an alias. See `%ai update --help` for\n",
      "            options.\n",
      "  version   Prints Jupyter-AI version\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%ai help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01063bb8-bd5c-4e9b-addf-272e30c7e075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: %ai list [OPTIONS] [PROVIDER_ID]\n",
      "\n",
      "  List language models, optionally scoped to PROVIDER_ID.\n",
      "\n",
      "Options:\n",
      "  --help  Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "%ai list --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3ee91d-51db-4429-997b-eebffe7f0977",
   "metadata": {},
   "source": [
    "## Listing all providers and models available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "309113ab-e9c0-4371-8172-7977acfc0aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Provider | Environment variable | Set? | Models |\n",
       "|----------|----------------------|------|--------|\n",
       "| `ai21` | `AI21_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`ai21:j1-large`</li><li>`ai21:j1-grande`</li><li>`ai21:j1-jumbo`</li><li>`ai21:j1-grande-instruct`</li><li>`ai21:j2-large`</li><li>`ai21:j2-grande`</li><li>`ai21:j2-jumbo`</li><li>`ai21:j2-grande-instruct`</li><li>`ai21:j2-jumbo-instruct`</li></ul> |\n",
       "| `bedrock` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | <ul><li>`bedrock:amazon.titan-text-express-v1`</li><li>`bedrock:amazon.titan-text-lite-v1`</li><li>`bedrock:amazon.titan-text-premier-v1:0`</li><li>`bedrock:ai21.j2-ultra-v1`</li><li>`bedrock:ai21.j2-mid-v1`</li><li>`bedrock:ai21.jamba-instruct-v1:0`</li><li>`bedrock:cohere.command-light-text-v14`</li><li>`bedrock:cohere.command-text-v14`</li><li>`bedrock:cohere.command-r-v1:0`</li><li>`bedrock:cohere.command-r-plus-v1:0`</li><li>`bedrock:meta.llama2-13b-chat-v1`</li><li>`bedrock:meta.llama2-70b-chat-v1`</li><li>`bedrock:meta.llama3-8b-instruct-v1:0`</li><li>`bedrock:meta.llama3-70b-instruct-v1:0`</li><li>`bedrock:meta.llama3-1-8b-instruct-v1:0`</li><li>`bedrock:meta.llama3-1-70b-instruct-v1:0`</li><li>`bedrock:meta.llama3-1-405b-instruct-v1:0`</li><li>`bedrock:mistral.mistral-7b-instruct-v0:2`</li><li>`bedrock:mistral.mixtral-8x7b-instruct-v0:1`</li><li>`bedrock:mistral.mistral-large-2402-v1:0`</li><li>`bedrock:mistral.mistral-large-2407-v1:0`</li></ul> |\n",
       "| `bedrock-chat` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | <ul><li>`bedrock-chat:amazon.titan-text-express-v1`</li><li>`bedrock-chat:amazon.titan-text-lite-v1`</li><li>`bedrock-chat:amazon.titan-text-premier-v1:0`</li><li>`bedrock-chat:anthropic.claude-v2`</li><li>`bedrock-chat:anthropic.claude-v2:1`</li><li>`bedrock-chat:anthropic.claude-instant-v1`</li><li>`bedrock-chat:anthropic.claude-3-sonnet-20240229-v1:0`</li><li>`bedrock-chat:anthropic.claude-3-haiku-20240307-v1:0`</li><li>`bedrock-chat:anthropic.claude-3-opus-20240229-v1:0`</li><li>`bedrock-chat:anthropic.claude-3-5-haiku-20241022-v1:0`</li><li>`bedrock-chat:anthropic.claude-3-5-sonnet-20240620-v1:0`</li><li>`bedrock-chat:anthropic.claude-3-5-sonnet-20241022-v2:0`</li><li>`bedrock-chat:meta.llama2-13b-chat-v1`</li><li>`bedrock-chat:meta.llama2-70b-chat-v1`</li><li>`bedrock-chat:meta.llama3-8b-instruct-v1:0`</li><li>`bedrock-chat:meta.llama3-70b-instruct-v1:0`</li><li>`bedrock-chat:meta.llama3-1-8b-instruct-v1:0`</li><li>`bedrock-chat:meta.llama3-1-70b-instruct-v1:0`</li><li>`bedrock-chat:meta.llama3-1-405b-instruct-v1:0`</li><li>`bedrock-chat:mistral.mistral-7b-instruct-v0:2`</li><li>`bedrock-chat:mistral.mixtral-8x7b-instruct-v0:1`</li><li>`bedrock-chat:mistral.mistral-large-2402-v1:0`</li><li>`bedrock-chat:mistral.mistral-large-2407-v1:0`</li></ul> |\n",
       "| `bedrock-custom` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | Specify the ARN (Amazon Resource Name) of the custom/provisioned model as the model ID. For more information, see the [Amazon Bedrock model IDs documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html).\n",
       "\n",
       "The model provider must also be specified below. This is the provider of your foundation model *in lowercase*, e.g. `amazon`, `anthropic`, `meta`, or `mistral`. |\n",
       "| `anthropic-chat` | `ANTHROPIC_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`anthropic-chat:claude-2.0`</li><li>`anthropic-chat:claude-2.1`</li><li>`anthropic-chat:claude-3-opus-20240229`</li><li>`anthropic-chat:claude-3-sonnet-20240229`</li><li>`anthropic-chat:claude-3-haiku-20240307`</li><li>`anthropic-chat:claude-3-5-haiku-20241022`</li><li>`anthropic-chat:claude-3-5-sonnet-20240620`</li><li>`anthropic-chat:claude-3-5-sonnet-20241022`</li></ul> |\n",
       "| `azure-chat-openai` | `AZURE_OPENAI_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | This provider does not define a list of models. |\n",
       "| `cohere` | `COHERE_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`cohere:command`</li><li>`cohere:command-nightly`</li><li>`cohere:command-light`</li><li>`cohere:command-light-nightly`</li><li>`cohere:command-r-plus`</li><li>`cohere:command-r`</li></ul> |\n",
       "| `gemini` | `GOOGLE_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`gemini:gemini-1.5-pro`</li><li>`gemini:gemini-1.5-flash`</li><li>`gemini:gemini-1.0-pro`</li><li>`gemini:gemini-1.0-pro-001`</li><li>`gemini:gemini-1.0-pro-latest`</li><li>`gemini:gemini-1.0-pro-vision-latest`</li><li>`gemini:gemini-pro`</li><li>`gemini:gemini-pro-vision`</li></ul> |\n",
       "| `gpt4all` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | <ul><li>`gpt4all:ggml-gpt4all-j-v1.2-jazzy`</li><li>`gpt4all:ggml-gpt4all-j-v1.3-groovy`</li><li>`gpt4all:ggml-gpt4all-l13b-snoozy`</li><li>`gpt4all:mistral-7b-openorca.Q4_0`</li><li>`gpt4all:mistral-7b-instruct-v0.1.Q4_0`</li><li>`gpt4all:gpt4all-falcon-q4_0`</li><li>`gpt4all:wizardlm-13b-v1.2.Q4_0`</li><li>`gpt4all:nous-hermes-llama2-13b.Q4_0`</li><li>`gpt4all:gpt4all-13b-snoozy-q4_0`</li><li>`gpt4all:mpt-7b-chat-merges-q4_0`</li><li>`gpt4all:orca-mini-3b-gguf2-q4_0`</li><li>`gpt4all:starcoder-q4_0`</li><li>`gpt4all:rift-coder-v0-7b-q4_0`</li><li>`gpt4all:em_german_mistral_v01.Q4_0`</li></ul> |\n",
       "| `huggingface_hub` | `HUGGINGFACEHUB_API_TOKEN` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | See [https://huggingface.co/models](https://huggingface.co/models) for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`. |\n",
       "| `mistralai` | `MISTRAL_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`mistralai:open-mistral-7b`</li><li>`mistralai:open-mixtral-8x7b`</li><li>`mistralai:open-mixtral-8x22b`</li><li>`mistralai:mistral-small-latest`</li><li>`mistralai:mistral-medium-latest`</li><li>`mistralai:mistral-large-latest`</li><li>`mistralai:codestral-latest`</li></ul> |\n",
       "| `nvidia-chat` | `NVIDIA_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`nvidia-chat:playground_llama2_70b`</li><li>`nvidia-chat:playground_nemotron_steerlm_8b`</li><li>`nvidia-chat:playground_mistral_7b`</li><li>`nvidia-chat:playground_nv_llama2_rlhf_70b`</li><li>`nvidia-chat:playground_llama2_13b`</li><li>`nvidia-chat:playground_steerlm_llama_70b`</li><li>`nvidia-chat:playground_llama2_code_13b`</li><li>`nvidia-chat:playground_yi_34b`</li><li>`nvidia-chat:playground_mixtral_8x7b`</li><li>`nvidia-chat:playground_neva_22b`</li><li>`nvidia-chat:playground_llama2_code_34b`</li></ul> |\n",
       "| `ollama` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | See [https://www.ollama.com/library](https://www.ollama.com/library) for a list of models. Pass a model's name; for example, `deepseek-coder-v2`. |\n",
       "| `openai` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | <ul><li>`openai:babbage-002`</li><li>`openai:davinci-002`</li><li>`openai:gpt-3.5-turbo-instruct`</li></ul> |\n",
       "| `openai-chat` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | <ul><li>`openai-chat:gpt-3.5-turbo`</li><li>`openai-chat:gpt-3.5-turbo-0125`</li><li>`openai-chat:gpt-3.5-turbo-0301`</li><li>`openai-chat:gpt-3.5-turbo-0613`</li><li>`openai-chat:gpt-3.5-turbo-1106`</li><li>`openai-chat:gpt-3.5-turbo-16k`</li><li>`openai-chat:gpt-3.5-turbo-16k-0613`</li><li>`openai-chat:gpt-4`</li><li>`openai-chat:gpt-4-turbo`</li><li>`openai-chat:gpt-4-turbo-preview`</li><li>`openai-chat:gpt-4-0613`</li><li>`openai-chat:gpt-4-32k`</li><li>`openai-chat:gpt-4-32k-0613`</li><li>`openai-chat:gpt-4-0125-preview`</li><li>`openai-chat:gpt-4-1106-preview`</li><li>`openai-chat:gpt-4o`</li><li>`openai-chat:gpt-4o-mini`</li></ul> |\n",
       "| `openrouter` | `OPENROUTER_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | This provider does not define a list of models. |\n",
       "| `qianfan` | `QIANFAN_AK`, `QIANFAN_SK` | <abbr title=\"You have not set all of these environment variables, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`qianfan:ERNIE-Bot`</li><li>`qianfan:ERNIE-Bot-4`</li></ul> |\n",
       "| `sagemaker-endpoint` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | Specify an endpoint name as the model ID. In addition, you must specify a region name, request schema, and response path. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deploy-models.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints). |\n",
       "| `togetherai` | `TOGETHER_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`togetherai:Austism/chronos-hermes-13b`</li><li>`togetherai:DiscoResearch/DiscoLM-mixtral-8x7b-v2`</li><li>`togetherai:EleutherAI/llemma_7b`</li><li>`togetherai:Gryphe/MythoMax-L2-13b`</li><li>`togetherai:Meta-Llama/Llama-Guard-7b`</li><li>`togetherai:Nexusflow/NexusRaven-V2-13B`</li><li>`togetherai:NousResearch/Nous-Capybara-7B-V1p9`</li><li>`togetherai:NousResearch/Nous-Hermes-2-Yi-34B`</li><li>`togetherai:NousResearch/Nous-Hermes-Llama2-13b`</li><li>`togetherai:NousResearch/Nous-Hermes-Llama2-70b`</li></ul> |\n",
       "\n",
       "Aliases and custom commands:\n",
       "\n",
       "| Name | Target |\n",
       "|------|--------|\n",
       "| `gpt2` | `huggingface_hub:gpt2` |\n",
       "| `gpt3` | `openai:davinci-002` |\n",
       "| `chatgpt` | `openai-chat:gpt-3.5-turbo` |\n",
       "| `gpt4` | `openai-chat:gpt-4` |\n",
       "| `ernie-bot` | `qianfan:ERNIE-Bot` |\n",
       "| `ernie-bot-4` | `qianfan:ERNIE-Bot-4` |\n",
       "| `titan` | `bedrock:amazon.titan-tg1-large` |\n",
       "| `openrouter-claude` | `openrouter:anthropic/claude-3.5-sonnet:beta` |\n"
      ],
      "text/plain": [
       "ai21\n",
       "Requires environment variable: AI21_API_KEY (not set)\n",
       "* ai21:j1-large\n",
       "* ai21:j1-grande\n",
       "* ai21:j1-jumbo\n",
       "* ai21:j1-grande-instruct\n",
       "* ai21:j2-large\n",
       "* ai21:j2-grande\n",
       "* ai21:j2-jumbo\n",
       "* ai21:j2-grande-instruct\n",
       "* ai21:j2-jumbo-instruct\n",
       "\n",
       "bedrock\n",
       "* bedrock:amazon.titan-text-express-v1\n",
       "* bedrock:amazon.titan-text-lite-v1\n",
       "* bedrock:amazon.titan-text-premier-v1:0\n",
       "* bedrock:ai21.j2-ultra-v1\n",
       "* bedrock:ai21.j2-mid-v1\n",
       "* bedrock:ai21.jamba-instruct-v1:0\n",
       "* bedrock:cohere.command-light-text-v14\n",
       "* bedrock:cohere.command-text-v14\n",
       "* bedrock:cohere.command-r-v1:0\n",
       "* bedrock:cohere.command-r-plus-v1:0\n",
       "* bedrock:meta.llama2-13b-chat-v1\n",
       "* bedrock:meta.llama2-70b-chat-v1\n",
       "* bedrock:meta.llama3-8b-instruct-v1:0\n",
       "* bedrock:meta.llama3-70b-instruct-v1:0\n",
       "* bedrock:meta.llama3-1-8b-instruct-v1:0\n",
       "* bedrock:meta.llama3-1-70b-instruct-v1:0\n",
       "* bedrock:meta.llama3-1-405b-instruct-v1:0\n",
       "* bedrock:mistral.mistral-7b-instruct-v0:2\n",
       "* bedrock:mistral.mixtral-8x7b-instruct-v0:1\n",
       "* bedrock:mistral.mistral-large-2402-v1:0\n",
       "* bedrock:mistral.mistral-large-2407-v1:0\n",
       "\n",
       "bedrock-chat\n",
       "* bedrock-chat:amazon.titan-text-express-v1\n",
       "* bedrock-chat:amazon.titan-text-lite-v1\n",
       "* bedrock-chat:amazon.titan-text-premier-v1:0\n",
       "* bedrock-chat:anthropic.claude-v2\n",
       "* bedrock-chat:anthropic.claude-v2:1\n",
       "* bedrock-chat:anthropic.claude-instant-v1\n",
       "* bedrock-chat:anthropic.claude-3-sonnet-20240229-v1:0\n",
       "* bedrock-chat:anthropic.claude-3-haiku-20240307-v1:0\n",
       "* bedrock-chat:anthropic.claude-3-opus-20240229-v1:0\n",
       "* bedrock-chat:anthropic.claude-3-5-haiku-20241022-v1:0\n",
       "* bedrock-chat:anthropic.claude-3-5-sonnet-20240620-v1:0\n",
       "* bedrock-chat:anthropic.claude-3-5-sonnet-20241022-v2:0\n",
       "* bedrock-chat:meta.llama2-13b-chat-v1\n",
       "* bedrock-chat:meta.llama2-70b-chat-v1\n",
       "* bedrock-chat:meta.llama3-8b-instruct-v1:0\n",
       "* bedrock-chat:meta.llama3-70b-instruct-v1:0\n",
       "* bedrock-chat:meta.llama3-1-8b-instruct-v1:0\n",
       "* bedrock-chat:meta.llama3-1-70b-instruct-v1:0\n",
       "* bedrock-chat:meta.llama3-1-405b-instruct-v1:0\n",
       "* bedrock-chat:mistral.mistral-7b-instruct-v0:2\n",
       "* bedrock-chat:mistral.mixtral-8x7b-instruct-v0:1\n",
       "* bedrock-chat:mistral.mistral-large-2402-v1:0\n",
       "* bedrock-chat:mistral.mistral-large-2407-v1:0\n",
       "\n",
       "bedrock-custom\n",
       "* Specify the ARN (Amazon Resource Name) of the custom/provisioned model as the model ID. For more information, see the [Amazon Bedrock model IDs documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html).\n",
       "\n",
       "The model provider must also be specified below. This is the provider of your foundation model *in lowercase*, e.g. `amazon`, `anthropic`, `meta`, or `mistral`.\n",
       "\n",
       "anthropic-chat\n",
       "Requires environment variable: ANTHROPIC_API_KEY (not set)\n",
       "* anthropic-chat:claude-2.0\n",
       "* anthropic-chat:claude-2.1\n",
       "* anthropic-chat:claude-3-opus-20240229\n",
       "* anthropic-chat:claude-3-sonnet-20240229\n",
       "* anthropic-chat:claude-3-haiku-20240307\n",
       "* anthropic-chat:claude-3-5-haiku-20241022\n",
       "* anthropic-chat:claude-3-5-sonnet-20240620\n",
       "* anthropic-chat:claude-3-5-sonnet-20241022\n",
       "\n",
       "azure-chat-openai\n",
       "Requires environment variable: AZURE_OPENAI_API_KEY (not set)\n",
       "* This provider does not define a list of models.\n",
       "\n",
       "cohere\n",
       "Requires environment variable: COHERE_API_KEY (not set)\n",
       "* cohere:command\n",
       "* cohere:command-nightly\n",
       "* cohere:command-light\n",
       "* cohere:command-light-nightly\n",
       "* cohere:command-r-plus\n",
       "* cohere:command-r\n",
       "\n",
       "gemini\n",
       "Requires environment variable: GOOGLE_API_KEY (not set)\n",
       "* gemini:gemini-1.5-pro\n",
       "* gemini:gemini-1.5-flash\n",
       "* gemini:gemini-1.0-pro\n",
       "* gemini:gemini-1.0-pro-001\n",
       "* gemini:gemini-1.0-pro-latest\n",
       "* gemini:gemini-1.0-pro-vision-latest\n",
       "* gemini:gemini-pro\n",
       "* gemini:gemini-pro-vision\n",
       "\n",
       "gpt4all\n",
       "* gpt4all:ggml-gpt4all-j-v1.2-jazzy\n",
       "* gpt4all:ggml-gpt4all-j-v1.3-groovy\n",
       "* gpt4all:ggml-gpt4all-l13b-snoozy\n",
       "* gpt4all:mistral-7b-openorca.Q4_0\n",
       "* gpt4all:mistral-7b-instruct-v0.1.Q4_0\n",
       "* gpt4all:gpt4all-falcon-q4_0\n",
       "* gpt4all:wizardlm-13b-v1.2.Q4_0\n",
       "* gpt4all:nous-hermes-llama2-13b.Q4_0\n",
       "* gpt4all:gpt4all-13b-snoozy-q4_0\n",
       "* gpt4all:mpt-7b-chat-merges-q4_0\n",
       "* gpt4all:orca-mini-3b-gguf2-q4_0\n",
       "* gpt4all:starcoder-q4_0\n",
       "* gpt4all:rift-coder-v0-7b-q4_0\n",
       "* gpt4all:em_german_mistral_v01.Q4_0\n",
       "\n",
       "huggingface_hub\n",
       "Requires environment variable: HUGGINGFACEHUB_API_TOKEN (not set)\n",
       "* See [https://huggingface.co/models](https://huggingface.co/models) for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`.\n",
       "\n",
       "mistralai\n",
       "Requires environment variable: MISTRAL_API_KEY (not set)\n",
       "* mistralai:open-mistral-7b\n",
       "* mistralai:open-mixtral-8x7b\n",
       "* mistralai:open-mixtral-8x22b\n",
       "* mistralai:mistral-small-latest\n",
       "* mistralai:mistral-medium-latest\n",
       "* mistralai:mistral-large-latest\n",
       "* mistralai:codestral-latest\n",
       "\n",
       "nvidia-chat\n",
       "Requires environment variable: NVIDIA_API_KEY (not set)\n",
       "* nvidia-chat:playground_llama2_70b\n",
       "* nvidia-chat:playground_nemotron_steerlm_8b\n",
       "* nvidia-chat:playground_mistral_7b\n",
       "* nvidia-chat:playground_nv_llama2_rlhf_70b\n",
       "* nvidia-chat:playground_llama2_13b\n",
       "* nvidia-chat:playground_steerlm_llama_70b\n",
       "* nvidia-chat:playground_llama2_code_13b\n",
       "* nvidia-chat:playground_yi_34b\n",
       "* nvidia-chat:playground_mixtral_8x7b\n",
       "* nvidia-chat:playground_neva_22b\n",
       "* nvidia-chat:playground_llama2_code_34b\n",
       "\n",
       "ollama\n",
       "* See [https://www.ollama.com/library](https://www.ollama.com/library) for a list of models. Pass a model's name; for example, `deepseek-coder-v2`.\n",
       "\n",
       "openai\n",
       "Requires environment variable: OPENAI_API_KEY (set)\n",
       "* openai:babbage-002\n",
       "* openai:davinci-002\n",
       "* openai:gpt-3.5-turbo-instruct\n",
       "\n",
       "openai-chat\n",
       "Requires environment variable: OPENAI_API_KEY (set)\n",
       "* openai-chat:gpt-3.5-turbo\n",
       "* openai-chat:gpt-3.5-turbo-0125\n",
       "* openai-chat:gpt-3.5-turbo-0301\n",
       "* openai-chat:gpt-3.5-turbo-0613\n",
       "* openai-chat:gpt-3.5-turbo-1106\n",
       "* openai-chat:gpt-3.5-turbo-16k\n",
       "* openai-chat:gpt-3.5-turbo-16k-0613\n",
       "* openai-chat:gpt-4\n",
       "* openai-chat:gpt-4-turbo\n",
       "* openai-chat:gpt-4-turbo-preview\n",
       "* openai-chat:gpt-4-0613\n",
       "* openai-chat:gpt-4-32k\n",
       "* openai-chat:gpt-4-32k-0613\n",
       "* openai-chat:gpt-4-0125-preview\n",
       "* openai-chat:gpt-4-1106-preview\n",
       "* openai-chat:gpt-4o\n",
       "* openai-chat:gpt-4o-mini\n",
       "\n",
       "openrouter\n",
       "Requires environment variable: OPENROUTER_API_KEY (not set)\n",
       "* This provider does not define a list of models.\n",
       "\n",
       "qianfan\n",
       "Requires environment variables: QIANFAN_AK (not set), QIANFAN_SK (not set)\n",
       "* qianfan:ERNIE-Bot\n",
       "* qianfan:ERNIE-Bot-4\n",
       "\n",
       "sagemaker-endpoint\n",
       "* Specify an endpoint name as the model ID. In addition, you must specify a region name, request schema, and response path. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deploy-models.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints).\n",
       "\n",
       "togetherai\n",
       "Requires environment variable: TOGETHER_API_KEY (not set)\n",
       "* togetherai:Austism/chronos-hermes-13b\n",
       "* togetherai:DiscoResearch/DiscoLM-mixtral-8x7b-v2\n",
       "* togetherai:EleutherAI/llemma_7b\n",
       "* togetherai:Gryphe/MythoMax-L2-13b\n",
       "* togetherai:Meta-Llama/Llama-Guard-7b\n",
       "* togetherai:Nexusflow/NexusRaven-V2-13B\n",
       "* togetherai:NousResearch/Nous-Capybara-7B-V1p9\n",
       "* togetherai:NousResearch/Nous-Hermes-2-Yi-34B\n",
       "* togetherai:NousResearch/Nous-Hermes-Llama2-13b\n",
       "* togetherai:NousResearch/Nous-Hermes-Llama2-70b\n",
       "\n",
       "\n",
       "Aliases and custom commands:\n",
       "gpt2 - huggingface_hub:gpt2\n",
       "gpt3 - openai:davinci-002\n",
       "chatgpt - openai-chat:gpt-3.5-turbo\n",
       "gpt4 - openai-chat:gpt-4\n",
       "ernie-bot - qianfan:ERNIE-Bot\n",
       "ernie-bot-4 - qianfan:ERNIE-Bot-4\n",
       "titan - bedrock:amazon.titan-tg1-large\n",
       "openrouter-claude - openrouter:anthropic/claude-3.5-sonnet:beta\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ai list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3774ad78-a143-4e81-8916-b5f115a803e8",
   "metadata": {},
   "source": [
    "## List all models offered by a specific provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "533a3e93-b5e6-48aa-b1e7-eec7b887660f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Provider | Environment variable | Set? | Models |\n",
       "|----------|----------------------|------|--------|\n",
       "| `openai` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | <ul><li>`openai:babbage-002`</li><li>`openai:davinci-002`</li><li>`openai:gpt-3.5-turbo-instruct`</li></ul> |\n"
      ],
      "text/plain": [
       "openai\n",
       "Requires environment variable: OPENAI_API_KEY (set)\n",
       "* openai:babbage-002\n",
       "* openai:davinci-002\n",
       "* openai:gpt-3.5-turbo-instruct\n",
       "\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ai list openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0981bfaa-2f38-4e73-bb81-c6cb697df073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Provider | Environment variable | Set? | Models |\n",
       "|----------|----------------------|------|--------|\n",
       "| `openai-chat` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | <ul><li>`openai-chat:gpt-3.5-turbo`</li><li>`openai-chat:gpt-3.5-turbo-0125`</li><li>`openai-chat:gpt-3.5-turbo-0301`</li><li>`openai-chat:gpt-3.5-turbo-0613`</li><li>`openai-chat:gpt-3.5-turbo-1106`</li><li>`openai-chat:gpt-3.5-turbo-16k`</li><li>`openai-chat:gpt-3.5-turbo-16k-0613`</li><li>`openai-chat:gpt-4`</li><li>`openai-chat:gpt-4-turbo`</li><li>`openai-chat:gpt-4-turbo-preview`</li><li>`openai-chat:gpt-4-0613`</li><li>`openai-chat:gpt-4-32k`</li><li>`openai-chat:gpt-4-32k-0613`</li><li>`openai-chat:gpt-4-0125-preview`</li><li>`openai-chat:gpt-4-1106-preview`</li><li>`openai-chat:gpt-4o`</li><li>`openai-chat:gpt-4o-mini`</li></ul> |\n"
      ],
      "text/plain": [
       "openai-chat\n",
       "Requires environment variable: OPENAI_API_KEY (set)\n",
       "* openai-chat:gpt-3.5-turbo\n",
       "* openai-chat:gpt-3.5-turbo-0125\n",
       "* openai-chat:gpt-3.5-turbo-0301\n",
       "* openai-chat:gpt-3.5-turbo-0613\n",
       "* openai-chat:gpt-3.5-turbo-1106\n",
       "* openai-chat:gpt-3.5-turbo-16k\n",
       "* openai-chat:gpt-3.5-turbo-16k-0613\n",
       "* openai-chat:gpt-4\n",
       "* openai-chat:gpt-4-turbo\n",
       "* openai-chat:gpt-4-turbo-preview\n",
       "* openai-chat:gpt-4-0613\n",
       "* openai-chat:gpt-4-32k\n",
       "* openai-chat:gpt-4-32k-0613\n",
       "* openai-chat:gpt-4-0125-preview\n",
       "* openai-chat:gpt-4-1106-preview\n",
       "* openai-chat:gpt-4o\n",
       "* openai-chat:gpt-4o-mini\n",
       "\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ai list openai-chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af3f91a-eb6b-4415-addc-55438ebb16e1",
   "metadata": {},
   "source": [
    "## Making a request to gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43dc73ea-a62d-4bd1-a530-d9b7758a3390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```markdown\n",
       "# Lists vs Tuples vs Sets in Python\n",
       "\n",
       "## 1. Lists\n",
       "- **Definition**: A list is an ordered collection of items that can hold a variety of object types.\n",
       "- **Syntax**: Defined using square brackets `[]`.\n",
       "- **Mutability**: Mutable (modifiable after creation).\n",
       "- **Duplicates**: Allows duplicate elements.\n",
       "- **Access**: Elements can be accessed by their index.\n",
       "- **Example**:\n",
       "  ```python\n",
       "  my_list = [1, 2, 3, 2, 4]\n",
       "  ```\n",
       "\n",
       "## 2. Tuples\n",
       "- **Definition**: A tuple is an ordered collection of items, similar to a list, but immutable.\n",
       "- **Syntax**: Defined using parentheses `()`.\n",
       "- **Mutability**: Immutable (cannot be modified after creation).\n",
       "- **Duplicates**: Allows duplicate elements.\n",
       "- **Access**: Elements can be accessed by their index.\n",
       "- **Example**:\n",
       "  ```python\n",
       "  my_tuple = (1, 2, 3, 2, 4)\n",
       "  ```\n",
       "\n",
       "## 3. Sets\n",
       "- **Definition**: A set is an unordered collection of unique items.\n",
       "- **Syntax**: Defined using curly braces `{}` or the `set()` function.\n",
       "- **Mutability**: Mutable (can add or remove items).\n",
       "- **Duplicates**: Does not allow duplicate elements.\n",
       "- **Access**: Elements cannot be accessed by index due to unordered nature.\n",
       "- **Example**:\n",
       "  ```python\n",
       "  my_set = {1, 2, 3, 2, 4}  # Result will be {1, 2, 3, 4}\n",
       "  ```\n",
       "\n",
       "## Summary Table\n",
       "\n",
       "| Feature      | Lists         | Tuples        | Sets         |\n",
       "|--------------|---------------|---------------|--------------|\n",
       "| Ordered      | Yes           | Yes           | No           |\n",
       "| Mutable      | Yes           | No            | Yes          |\n",
       "| Duplicates   | Yes           | Yes           | No           |\n",
       "| Syntax       | `[]`          | `()`          | `{}` or `set()` |\n",
       "| Access       | By index      | By index      | Not by index  |\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-4o-mini",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai openai-chat:gpt-4o-mini\n",
    "list vs tuples vs sets in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83a7b9d8-573a-46b4-9616-74f2888692d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```markdown\n",
       "# Sorting Algorithms in Python\n",
       "\n",
       "Sorting algorithms are used to arrange elements in a specific order, usually in ascending or descending order. There are various sorting algorithms available, each with its own advantages and disadvantages. Here are some common sorting algorithms in Python with examples:\n",
       "\n",
       "## 1. Bubble Sort\n",
       "- **Description**: Bubble sort repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order.\n",
       "- **Example**:\n",
       "  ```python\n",
       "  def bubble_sort(arr):\n",
       "      n = len(arr)\n",
       "      for i in range(n):\n",
       "          for j in range(0, n - i - 1):\n",
       "              if arr[j] > arr[j + 1]:\n",
       "                  arr[j], arr[j + 1] = arr[j + 1], arr[j]\n",
       "      return arr\n",
       "\n",
       "  my_list = [64, 34, 25, 12, 22, 11, 90]\n",
       "  sorted_list = bubble_sort(my_list)\n",
       "  print(sorted_list)\n",
       "  ```\n",
       "\n",
       "## 2. Selection Sort\n",
       "- **Description**: Selection sort finds the minimum element from the unsorted portion and swaps it with the first unsorted element.\n",
       "- **Example**:\n",
       "  ```python\n",
       "  def selection_sort(arr):\n",
       "      n = len(arr)\n",
       "      for i in range(n):\n",
       "          min_index = i\n",
       "          for j in range(i + 1, n):\n",
       "              if arr[j] < arr[min_index]:\n",
       "                  min_index = j\n",
       "          arr[i], arr[min_index] = arr[min_index], arr[i]\n",
       "      return arr\n",
       "\n",
       "  my_list = [64, 34, 25, 12, 22, 11, 90]\n",
       "  sorted_list = selection_sort(my_list)\n",
       "  print(sorted_list)\n",
       "  ```\n",
       "\n",
       "## 3. Merge Sort\n",
       "- **Description**: Merge sort is a divide and conquer algorithm that divides the input list into two halves, sorts them, and then merges them back together.\n",
       "- **Example**:\n",
       "  ```python\n",
       "  def merge_sort(arr):\n",
       "      if len(arr) <= 1:\n",
       "          return arr\n",
       "      mid = len(arr) // 2\n",
       "      left = merge_sort(arr[:mid])\n",
       "      right = merge_sort(arr[mid:])\n",
       "      \n",
       "      return merge(left, right)\n",
       "\n",
       "  def merge(left, right):\n",
       "      result = []\n",
       "      i = j = 0\n",
       "      while i < len(left) and j < len(right):\n",
       "          if left[i] < right[j]:\n",
       "              result.append(left[i])\n",
       "              i += 1\n",
       "          else:\n",
       "              result.append(right[j])\n",
       "              j += 1\n",
       "      result.extend(left[i:])\n",
       "      result.extend(right[j:])\n",
       "      return result\n",
       "\n",
       "  my_list = [64, 34, 25, 12, 22, 11, 90]\n",
       "  sorted_list = merge_sort(my_list)\n",
       "  print(sorted_list)\n",
       "  ```\n",
       "\n",
       "These are just a few examples of sorting algorithms in Python. Each algorithm has its own complexity and performance characteristics, so it's important to choose the right algorithm based on the size of the input data and the desired outcome.\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt\n",
    "explain sorting algorithms in Python. show examples in each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "270a2767-3282-4f7f-833a-1708a7b97bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```markdown\n",
       "# Sorting Algorithms in Python\n",
       "\n",
       "Sorting algorithms are methods for arranging elements in a list or array in a specified order, typically ascending or descending. Below are descriptions and examples of some common sorting algorithms implemented in Python.\n",
       "\n",
       "## 1. Bubble Sort\n",
       "- **Description**: Bubble Sort repeatedly compares adjacent elements and swaps them if they are in the wrong order. This process is repeated until the list is sorted.\n",
       "- **Time Complexity**: O(n^2)\n",
       "- **Example**:\n",
       "  ```python\n",
       "  def bubble_sort(arr):\n",
       "      n = len(arr)\n",
       "      for i in range(n):\n",
       "          for j in range(0, n - i - 1):\n",
       "              if arr[j] > arr[j + 1]:\n",
       "                  arr[j], arr[j + 1] = arr[j + 1], arr[j]\n",
       "      return arr\n",
       "\n",
       "  my_list = [64, 34, 25, 12, 22, 11, 90]\n",
       "  sorted_list = bubble_sort(my_list)\n",
       "  print(\"Bubble Sort:\", sorted_list)\n",
       "  ```\n",
       "\n",
       "## 2. Selection Sort\n",
       "- **Description**: Selection Sort divides the input list into two parts: a sorted part and an unsorted part. It repeatedly selects the smallest (or largest) element from the unsorted part and moves it to the sorted part.\n",
       "- **Time Complexity**: O(n^2)\n",
       "- **Example**:\n",
       "  ```python\n",
       "  def selection_sort(arr):\n",
       "      n = len(arr)\n",
       "      for i in range(n):\n",
       "          min_index = i\n",
       "          for j in range(i + 1, n):\n",
       "              if arr[j] < arr[min_index]:\n",
       "                  min_index = j\n",
       "          arr[i], arr[min_index] = arr[min_index], arr[i]\n",
       "      return arr\n",
       "\n",
       "  my_list = [64, 34, 25, 12, 22, 11, 90]\n",
       "  sorted_list = selection_sort(my_list)\n",
       "  print(\"Selection Sort:\", sorted_list)\n",
       "  ```\n",
       "\n",
       "## 3. Insertion Sort\n",
       "- **Description**: Insertion Sort builds a sorted array one element at a time. It picks an element from the unsorted part and finds the correct position in the sorted part, shifting elements as necessary.\n",
       "- **Time Complexity**: O(n^2)\n",
       "- **Example**:\n",
       "  ```python\n",
       "  def insertion_sort(arr):\n",
       "      for i in range(1, len(arr)):\n",
       "          key = arr[i]\n",
       "          j = i - 1\n",
       "          while j >= 0 and key < arr[j]:\n",
       "              arr[j + 1] = arr[j]\n",
       "              j -= 1\n",
       "          arr[j + 1] = key\n",
       "      return arr\n",
       "\n",
       "  my_list = [64, 34, 25, 12, 22, 11, 90]\n",
       "  sorted_list = insertion_sort(my_list)\n",
       "  print(\"Insertion Sort:\", sorted_list)\n",
       "  ```\n",
       "\n",
       "## 4. Merge Sort\n",
       "- **Description**: Merge Sort is a divide-and-conquer algorithm that splits the array into halves, sorts each half, and then merges them back together.\n",
       "- **Time Complexity**: O(n log n)\n",
       "- **Example**:\n",
       "  ```python\n",
       "  def merge_sort(arr):\n",
       "      if len(arr) <= 1:\n",
       "          return arr\n",
       "      mid = len(arr) // 2\n",
       "      left = merge_sort(arr[:mid])\n",
       "      right = merge_sort(arr[mid:])\n",
       "      return merge(left, right)\n",
       "\n",
       "  def merge(left, right):\n",
       "      result = []\n",
       "      i = j = 0\n",
       "      while i < len(left) and j < len(right):\n",
       "          if left[i] < right[j]:\n",
       "              result.append(left[i])\n",
       "              i += 1\n",
       "          else:\n",
       "              result.append(right[j])\n",
       "              j += 1\n",
       "      result.extend(left[i:])\n",
       "      result.extend(right[j:])\n",
       "      return result\n",
       "\n",
       "  my_list = [64, 34, 25, 12, 22, 11, 90]\n",
       "  sorted_list = merge_sort(my_list)\n",
       "  print(\"Merge Sort:\", sorted_list)\n",
       "  ```\n",
       "\n",
       "## 5. Quick Sort\n",
       "- **Description**: Quick Sort is a highly efficient sorting algorithm that uses a divide-and-conquer strategy. It selects a 'pivot' element and partitions the array into elements less than the pivot and those greater than the pivot, recursively sorting the partitions.\n",
       "- **Time Complexity**: Average O(n log n), Worst O(n^2)\n",
       "- **Example**:\n",
       "  ```python\n",
       "  def quick_sort(arr):\n",
       "      if len(arr) <= 1:\n",
       "          return arr\n",
       "      pivot = arr[len(arr) // 2]\n",
       "      left = [x for x in arr if x < pivot]\n",
       "      middle = [x for x in arr if x == pivot]\n",
       "      right = [x for x in arr if x > pivot]\n",
       "      return quick_sort(left) + middle + quick_sort(right)\n",
       "\n",
       "  my_list = [64, 34, 25, 12, 22, 11, 90]\n",
       "  sorted_list = quick_sort(my_list)\n",
       "  print(\"Quick Sort:\", sorted_list)\n",
       "  ```\n",
       "\n",
       "## Conclusion\n",
       "These are some of the most commonly used sorting algorithms in Python. Each algorithm has its own strengths and weaknesses, and the choice of which to use can depend on the specific requirements of your application, such as the size of the data set and the need for stability.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-4o-mini",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai openai-chat:gpt-4o-mini\n",
    "explain sorting algorithms in Python. show examples in each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c0d0429-d661-4f3b-ba66-6664c1bf67fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```markdown\n",
       "# `dict.update()` in Python\n",
       "\n",
       "The `dict.update()` method in Python is used to update a dictionary with elements from another dictionary or from an iterable of key-value pairs. This method modifies the original dictionary in place and does not return a new dictionary.\n",
       "\n",
       "## Syntax\n",
       "\n",
       "```python\n",
       "dict.update([other])\n",
       "```\n",
       "\n",
       "### Parameters\n",
       "\n",
       "- **other**: This can be another dictionary object or an iterable of key-value pairs (like a list of tuples). If a key in `other` already exists in the dictionary, its value will be updated; if it does not exist, the key-value pair will be added.\n",
       "\n",
       "## Examples\n",
       "\n",
       "### Example 1: Updating with Another Dictionary\n",
       "\n",
       "```python\n",
       "# Original dictionary\n",
       "dict1 = {'a': 1, 'b': 2}\n",
       "\n",
       "# Dictionary to update with\n",
       "dict2 = {'b': 3, 'c': 4}\n",
       "\n",
       "# Update dict1 with dict2\n",
       "dict1.update(dict2)\n",
       "\n",
       "print(dict1)  # Output: {'a': 1, 'b': 3, 'c': 4}\n",
       "```\n",
       "\n",
       "### Example 2: Updating with an Iterable of Key-Value Pairs\n",
       "\n",
       "```python\n",
       "# Original dictionary\n",
       "dict1 = {'x': 10, 'y': 20}\n",
       "\n",
       "# Iterable of key-value pairs\n",
       "pairs = [('y', 30), ('z', 40)]\n",
       "\n",
       "# Update dict1 with pairs\n",
       "dict1.update(pairs)\n",
       "\n",
       "print(dict1)  # Output: {'x': 10, 'y': 30, 'z': 40}\n",
       "```\n",
       "\n",
       "### Example 3: Using Keyword Arguments\n",
       "\n",
       "```python\n",
       "# Original dictionary\n",
       "dict1 = {'name': 'Alice', 'age': 25}\n",
       "\n",
       "# Update using keyword arguments\n",
       "dict1.update(city='New York', age=30)\n",
       "\n",
       "print(dict1)  # Output: {'name': 'Alice', 'age': 30, 'city': 'New York'}\n",
       "```\n",
       "\n",
       "## Important Notes\n",
       "\n",
       "- The `dict.update()` method modifies the original dictionary and does not create a new one.\n",
       "- If a key exists in both the original dictionary and the dictionary being used for the update, the value from the latter will overwrite the value in the former.\n",
       "- This method can be a convenient way to merge dictionaries or add multiple key-value pairs at once.\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "The `dict.update()` method is a powerful and flexible way to update dictionaries in Python. It can take other dictionaries, iterables of key-value pairs, or keyword arguments, making it a versatile tool for managing dictionary data.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-4o-mini",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai openai-chat:gpt-4o-mini\n",
    "explain dict.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16d69e7-0bf1-4ece-b2c8-582549ae0e8f",
   "metadata": {},
   "source": [
    "## Genrate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "193c9b4e-622e-4ffd-847b-3c800c541a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "AI generated code inserted below &#11015;&#65039;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "text/html": {
       "jupyter_ai": {
        "model_id": "gpt-4o-mini",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai openai-chat:gpt-4o-mini -f code \n",
    "define a function that calculates the factorial of n. \n",
    "call the function with 10 as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4969a7e2-b721-40cf-9059-95fc270e7c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3628800\n"
     ]
    }
   ],
   "source": [
    "def factorial(n):\n",
    "    if n == 0 or n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n - 1)\n",
    "\n",
    "result = factorial(10)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff74872c-68f0-482d-8ddb-78db5d2bd31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "AI generated code inserted below &#11015;&#65039;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "text/html": {
       "jupyter_ai": {
        "model_id": "gpt-4o-mini",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai openai-chat:gpt-4o-mini -f code \n",
    "change the function to be iteration one. \n",
    "call the function with 8 as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8af5d98d-9e55-4a17-ab53-d8537eea87f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40320\n"
     ]
    }
   ],
   "source": [
    "def factorial(n):\n",
    "    result = 1\n",
    "    for i in range(2, n + 1):\n",
    "        result *= i\n",
    "    return result\n",
    "\n",
    "result = factorial(8)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bbceb8-290b-462c-81a5-04a11b586178",
   "metadata": {},
   "source": [
    "## Clear the chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03dd713b-3c5b-4394-90c0-7b00c1c1b557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Chat history cleared.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-4o-mini",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai openai-chat:gpt-4o-mini\n",
    "clear the chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952384ab-2a50-4428-b00f-c079e8db1f93",
   "metadata": {},
   "source": [
    "## Explaining the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9fc04fc-11a3-4175-930d-a9565aaefafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [n**2 for n in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4db94d14-320e-4217-abdc-807b21aeae3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```markdown\n",
       "The code `x = [n**2 for n in range(10)]` is a Python list comprehension that generates a list of the squares of integers from 0 to 9. \n",
       "\n",
       "Here's a breakdown of the components:\n",
       "\n",
       "- `range(10)`: This generates an iterable sequence of numbers from 0 to 9 (inclusive of 0 and exclusive of 10).\n",
       "- `for n in range(10)`: This is the loop that iterates over each number `n` in the range.\n",
       "- `n**2`: This computes the square of the current number `n`.\n",
       "- `[n**2 for n in range(10)]`: This constructs a new list containing the squares of all numbers produced by the loop.\n",
       "\n",
       "As a result, after executing this line of code, `x` will contain the following list:\n",
       "```\n",
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n",
       "``` \n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-4o-mini",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai openai-chat:gpt-4o-mini\n",
    "Please explain the code above:\n",
    "{In[20]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8ef64a-29a1-4cfc-9ba7-a5db9cc9be0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
