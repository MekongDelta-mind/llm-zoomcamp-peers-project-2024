{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdd98958-e6ca-4803-a5b4-86a2633d38b5",
   "metadata": {},
   "source": [
    "# Project: Q & A on private documents (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f9c21a5-aff6-4daa-89ec-4c8716bab554",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11f93cf9-1c13-4037-b4b9-3462b183baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a6734f-475d-4192-9de5-988731b0171b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your OPENAI_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OPENAI_API_KEY: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc07ce7e-014d-4ed4-8a1b-c6701016e2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your PINECONE_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your PINECONE_API_KEY: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5959e608-c825-43bd-acd8-46ec183c77f8",
   "metadata": {},
   "source": [
    "## Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a7acbb-04eb-43cc-b8bd-f2c0f2dfca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading PDF, DOCX and TXT files as LangChain Documents\n",
    "def load_documents(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    elif extension == '.txt':\n",
    "        from langchain.document_loaders import TextLoader\n",
    "        loader = TextLoader(file)\n",
    "    else:\n",
    "        print('Document format is not supported!')\n",
    "        return None\n",
    "\n",
    "    data = loader.load()\n",
    "    return data\n",
    "\n",
    "\n",
    "# wikipedia\n",
    "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd244c52-b6a7-402b-8ea8-a5fb55d8058b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files/us_constitution.pdf\n"
     ]
    }
   ],
   "source": [
    "data = load_documents(\"files/us_constitution.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c84d81c4-1077-42f6-b962-536b8cffc0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'files/us_constitution.pdf', 'page': 1}, page_content='The House of Representatives shall be composed of Members chosen \\n every second Y ear by the People of the several States, and the \\n Electors in each State shall have the Qualifications requisite for \\n Electors of the most numerous Branch of the State Legislature. \\n No Person shall be a Representative who shall not have attained to the \\n Age of twenty five Y ears, and been seven Y ears a Citizen of the United \\n States, and who shall not, when elected, be an Inhabitant of that State \\n in which he shall be chosen. \\n Representatives and direct T axes shall be apportioned among the \\n several States which may be included within this Union, according to \\n their respective Numbers, which shall be determined by adding to the \\n whole Number of free Persons, including those bound to Service for a \\n T erm of Y ears, and excluding Indians not taxed, three fifths of all other \\n Persons. The actual Enumeration shall be made within three Y ears \\n after the first Meeting of the Congress of the United States, and within \\n every subsequent T erm of ten Y ears, in such Manner as they shall by \\n Law direct.The number of Representatives shall not exceed one for \\n every thirty Thousand, but each State shall have at Least one \\n Representative; and until such enumeration shall be made, the State of \\n New Hampshire shall be entitled to chuse three, Massachusetts eight, \\n Rhode-Island and Providence Plantations one, Connecticut five, \\n New-Y ork six, New Jersey four , Pennsylvania eight, Delaware one, ')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d027de00-57c3-4e60-bf5e-c4d44a099d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The House of Representatives shall be composed of Members chosen\\nevery second Year by the People of the several States, and the\\nElectors in each State shall have the Qualifications requisite for\\nElectors of the most numerous Branch of the State Legislature.\\nNo Person shall be a Representative who shall not have attained to the\\nAge of twenty five Years, and been seven Years a Citizen of the United\\nStates, and who shall not, when elected, be an Inhabitant of that State\\nin which he shall be chosen.\\nRepresentatives and direct Taxes shall be apportioned among the\\nseveral States which may be included within this Union, according to\\ntheir respective Numbers, which shall be determined by adding to the\\nwhole Number of free Persons, including those bound to Service for a\\nTerm of Years, and excluding Indians not taxed, three fifths of all other\\nPersons. The actual Enumeration shall be made within three Years\\nafter the first Meeting of the Congress of the United States, and within\\nevery subsequent Term of ten Years, in such Manner as they shall by\\nLaw direct.The number of Representatives shall not exceed one for\\nevery thirty Thousand, but each State shall have at Least one\\nRepresentative; and until such enumeration shall be made, the State of\\nNew Hampshire shall be entitled to chuse three, Massachusetts eight,\\nRhode-Island and Providence Plantations one, Connecticut five,\\nNew-York six, New Jersey four, Pennsylvania eight, Delaware one,\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01ac46fe-5ad3-4362-ae03-2eb537847978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1501"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "45de3eab-1f54-4e96-8c82-8628cc544783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[1].page_content.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52787a62-1b03-447f-9ce9-1cf0a0cf1c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'files/us_constitution.pdf',\n",
       " 'file_path': 'files/us_constitution.pdf',\n",
       " 'page': 1,\n",
       " 'total_pages': 41,\n",
       " 'format': 'PDF 1.4',\n",
       " 'title': 'us_constitution',\n",
       " 'author': '',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'creator': '',\n",
       " 'producer': 'Skia/PDF m115 Google Docs Renderer',\n",
       " 'creationDate': '',\n",
       " 'modDate': '',\n",
       " 'trapped': ''}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10ce9928-dd1c-4f36-be48-d97cd5262d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b931174-c523-401d-af05-44508edf2ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files/the_great_gatsby.docx\n"
     ]
    }
   ],
   "source": [
    "docs_data = load_documents(\"files/the_great_gatsby.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76f76737-bad7-441a-bfdf-7e4fc39530ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f97ae87-6145-4cf7-92b6-7f19bd6dbee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs_data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7f5b97e-b98c-4776-b242-a9b06ccd4eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data  = load_documents(\"files/state_of_the_union.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e93ffb6-c202-494b-9c12-fb321c410bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b680ad47-1b1f-462e-9d02-11e3bc97d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "902fe218-c6f4-4751-8842-ec4c200c4210",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data = load_from_wikipedia(\"LLM Agents and GPT-4\", load_max_docs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2607204-01c9-4036-be2a-ae55f52f8d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ac5cf9d-caa9-4b24-9906-a9fd3735a33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generative Pre-trained Transformer 4 (GPT-4) is a multimodal large language model created by OpenAI, and the fourth in its series of GPT foundation models. It was launched on March 14, 2023, and made publicly available via the paid chatbot product ChatGPT Plus, via OpenAI\\'s API, and via the free chatbot Microsoft Copilot.  As a transformer-based model, GPT-4 uses a paradigm where pre-training using both public data and \"data licensed from third-party providers\" is used to predict the next token. After this step, the model was then fine-tuned with reinforcement learning feedback from humans and AI for human alignment and policy compliance.:\\u200a2\\u200a\\nObservers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous iteration based on GPT-3.5, with the caveat that GPT-4 retains some of the problems with earlier revisions. GPT-4, equipped with vision capabilities (GPT-4V), is capable of taking images as input on ChatGPT. OpenAI has declined to reveal various technical details and statistics about GPT-4, such as the precise size of the model.\\n\\n\\n== Background ==\\n \\n\\nOpenAI introduced the first GPT model (GPT-1) in 2018, publishing a paper called \"Improving Language Understanding by Generative Pre-Training.\" It was based on the transformer architecture and trained on a large corpus of books. The next year, they introduced GPT-2, a larger model that could generate coherent text. In 2020, they introduced GPT-3, a model with over 100 times as many parameters as GPT-2, that could perform various tasks with few examples. GPT-3 was further improved into GPT-3.5, which was used to create the chatbot product ChatGPT.\\nRumors claim that GPT-4 has 1.76 trillion parameters, which was first estimated by the speed it was running and by George Hotz.\\n\\n\\n== Capabilities ==\\nOpenAI stated that GPT-4 is \"more reliable, creative, and able to handle much more nuanced instructions than GPT-3.5.\" They produced two versions of GPT-4, with context windows of 8,192 and 32,768 tokens, a significant improvement over GPT-3.5 and GPT-3, which were limited to 4,096 and 2,049 tokens respectively. Some of the capabilities of GPT-4 were predicted by OpenAI before training it, although other capabilities remained hard to predict due to breaks in downstream scaling laws. Unlike its predecessors, GPT-4 is a multimodal model: it can take images as well as text as input; this gives it the ability to describe the humor in unusual images, summarize text from screenshots, and answer exam questions that contain diagrams. It can now interact with users through spoken words and respond to images, allowing for more natural conversations and the ability to provide suggestions or answers based on photo uploads.\\nTo gain further control over GPT-4, OpenAI introduced the \"system message\", a directive in natural language given to GPT-4 in order to specify its tone of voice and task. For example, the system message can instruct the model to \"be a Shakespearean pirate\", in which case it will respond in rhyming, Shakespearean prose, or request it to \"always write the output of [its] response in JSON\", in which case the model will do so, adding keys and values as it sees fit to match the structure of its reply. In the examples provided by OpenAI, GPT-4 refused to deviate from its system message despite requests to do otherwise by the user during the conversation.\\nWhen instructed to do so, GPT-4 can interact with external interfaces. For example, the model could be instructed to enclose a query within <search></search> tags to perform a web search, the result of which would be inserted into the model\\'s prompt to allow it to form a response. This allows the model to perform tasks beyond its normal text-prediction capabilities, such as using APIs, generating images, and accessing and summarizing webpages.\\nA 2023 article in Nature stated programmers have found GPT-4 useful for assisting in coding tasks (despite its propensity for error), such as finding errors in existing'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "957b7fcc-bc86-4930-be09-dd3636b731cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generative artificial intelligence (generative AI, GenAI, or GAI) is a subset of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models often generate output in response to specific prompts. Generative AI systems learn the underlying patterns and structures of their training data, enabling them to create new data. \\nImprovements in transformer-based deep neural networks, particularly large language models (LLMs), enabled an AI boom of generative AI systems in the early 2020s. These include chatbots such as ChatGPT, Copilot, Gemini and LLaMA, text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney and DALL-E, and text-to-video AI generators such as Sora. Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.\\nGenerative AI has uses across a wide range of industries, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. However, concerns have been raised about the potential misuse of generative AI such as cybercrime, the use of fake news or deepfakes to deceive or manipulate people, and the mass replacement of human jobs. Intellectual property law concerns also exist around generative models that are trained on and emulate copyrighted works of art.\\n\\n\\n== History ==\\n\\n\\n=== Early history ===\\nSince its inception, researchers in the field have raised philosophical and ethical arguments about the nature of the human mind and the consequences of creating artificial beings with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity. The concept of automated art dates back at least to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of Alexandria were described as having designed machines capable of writing text, generating sounds, and playing music. The tradition of creative automations has flourished throughout history, exemplified by Maillardet\\'s automaton created in the early 1800s. Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906, and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains. Once a Markov chain is learned on a text corpus, it can then be used as a probabilistic text generator.\\n\\n\\n=== Academic artificial intelligence ===\\nThe academic discipline of artificial intelligence was established at a research workshop held at Dartmouth College in 1956 and has experienced several waves of advancement and optimism in the decades since. Artificial Intelligence research began in the 1950s with works like Computing Machinery and Intelligence (1950) and the 1956 Dartmouth Summer Research Project on AI. Since the 1950s, artists and researchers have used artificial intelligence to create artistic works. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by AARON, the computer program Cohen created to generate paintings.\\nThe terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft.\\n\\n\\n=== Generative neural nets (2014-2019) ===\\n\\nSince its inception, the field of machine learning used both discriminative models and generative models, to model a'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_data[-1].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808bbc42-028c-48c3-a2c4-e674d5255459",
   "metadata": {},
   "source": [
    "## Chunking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bff5ce2-552c-437b-92b0-561c57cc9bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ba9a068-63c0-46ca-b258-e45a56ecccdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = chunk_data(data, chunk_size=512)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5c67be4-937e-4f21-aa2f-e07893d1f888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'files/us_constitution.pdf', 'page': 0}, page_content='The United States Constitution \\n W e the People of the United States, in Order to form a more perfect \\n Union, establish Justice, insure domestic T ranquility , provide for the \\n common defence, promote the general W elfare, and secure the \\n Blessings of Liberty to ourselves and our Posterity , do ordain and \\n establish this Constitution for the United States of America. \\nThe Constitutional Con v ention \\n Article I \\n Section 1: Congress')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cd6490c-a6dc-418b-a6ba-7910c12d710b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'files/us_constitution.pdf', 'page': 0}, page_content='All legislative Powers herein granted shall be vested in a Congress of \\n the United States, which shall consist of a Senate and House of \\n Representatives. \\nSection 2: The House of Representatives')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46de537a-2f9d-43d3-8cf8-8f3d15a958a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'files/us_constitution.pdf', 'page': 40}, page_content='26th Amendment \\n Section 1 \\n The right of citizens of the United States, who are eighteen years of \\n age or older , to vote shall not be denied or abridged by the United \\n States or by any State on account of age. \\nSection 2 \\n The Congress shall have power to enforce this article by appropriate \\n legislation. \\n27th Amendment \\n No law , varying the compensation for the services of the Senators and \\n Representatives, shall take ef fect, until an election of representatives \\n shall have intervened.')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83882c66-5af6-4332-941e-bfea869ff487",
   "metadata": {},
   "source": [
    "## Calculating the Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d4b23b5-8399-48de-b7aa-660058b5c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-3-small')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.00002:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05be52a1-125b-42a9-baf0-1927cef2c477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 10032\n",
      "Embedding Cost in USD: 0.000201\n"
     ]
    }
   ],
   "source": [
    "data_embedding_cost = print_embedding_cost(data)\n",
    "data_embedding_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b5e223d-55a4-49ce-83da-48a9f3cce6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 9927\n",
      "Embedding Cost in USD: 0.000199\n"
     ]
    }
   ],
   "source": [
    "data_embedding_cost = print_embedding_cost(chunks)\n",
    "data_embedding_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f1928-2c60-4a6e-9e2b-e9e3e71e6692",
   "metadata": {},
   "source": [
    "## Embedding and Uploading to a Vector Database (Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fe809024-f739-48ef-9402-14f7fb90ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_fetch_embeddings(index_name, chunks):\n",
    "    # importing the necessary libraries and initializing the Pinecone client\n",
    "    import pinecone\n",
    "    from langchain_community.vectorstores import Pinecone\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    from pinecone import PodSpec\n",
    "\n",
    "    \n",
    "    pc = pinecone.Pinecone()\n",
    "        \n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)  # 512 works as well\n",
    "\n",
    "    # loading from existing index\n",
    "    if index_name in pc.list_indexes().names():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings ... ', end='')\n",
    "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        # creating the index and embedding the chunks into the index \n",
    "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
    "\n",
    "        # creating a new index\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1536,\n",
    "            metric='cosine',\n",
    "            spec=PodSpec(\n",
    "                environment='gcp-starter'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # processing the input documents, generating embeddings using the provided `OpenAIEmbeddings` instance,\n",
    "        # inserting the embeddings into the index and returning a new Pinecone vector store object. \n",
    "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "        print('Ok')\n",
    "        \n",
    "    return vector_store\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "91ad5395-60a7-49a6-b716-af99cf675b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name='all'):\n",
    "    import pinecone\n",
    "    pc = pinecone.Pinecone()\n",
    "    \n",
    "    if index_name == 'all':\n",
    "        indexes = pc.list_indexes().names()\n",
    "        print('Deleting all indexes ... ')\n",
    "        for index in indexes:\n",
    "            pc.delete_index(index)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Deleting index {index_name} ...', end='')\n",
    "        pc.delete_index(index_name)\n",
    "        print('Ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d3bc5627-6833-4ec1-8177-d38db8d687b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes ... \n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6a4b590e-985c-45d5-9653-f80360a1b84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index askadocument and embeddings ...Ok\n"
     ]
    }
   ],
   "source": [
    "index_name = 'askadocument'\n",
    "vector_store = insert_or_fetch_embeddings(index_name=index_name, chunks=chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dc3fe2-e406-4000-8028-ac4c4f20e1ea",
   "metadata": {},
   "source": [
    "## Asking and Getting Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a603ca3f-3491-4e41-a4b0-bf9b5d7dc768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_and_get_answer(vector_store, q, k=3):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-4o-mini', temperature=1)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k})\n",
    "\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "    \n",
    "    answer = chain.invoke(q)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a489d1ee-4142-4ec0-934e-a028203152c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the Bill of Rights?', 'result': 'The Bill of Rights refers to the first ten amendments to the United States Constitution. These amendments were ratified in 1791 and serve to protect individual liberties and rights against government interference. The Bill of Rights includes important provisions such as freedom of speech, religion, and assembly (First Amendment), the right to bear arms (Second Amendment), protection against unreasonable searches and seizures (Fourth Amendment), and rights related to criminal prosecutions (Fifth through Eighth Amendments), among others.'}\n"
     ]
    }
   ],
   "source": [
    "q = 'What is the Bill of Rights?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7211e89-5a17-46b5-8687-89cfec344279",
   "metadata": {},
   "source": [
    "### While loop for Asking questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "41a8094b-54e4-44b6-8939-fe099356a4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Quit or Exit to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #1:  What is the first amendment described in the document?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The First Amendment states: \"Congress shall make no law respecting an establishment of religion, or prohibiting the free exercise thereof; or abridging the freedom of speech, or of the press; or the right of the people peaceably to assemble, and to petition the Government for a redress of grievances.\"\n",
      "\n",
      " ---------------------------------------------------------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #2:  what about the second amendment?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The Second Amendment states: \"A well regulated Militia, being necessary to the security of a free State, the right of the people to keep and bear Arms, shall not be infringed.\" This amendment is often cited in discussions about gun rights and gun control in the United States.\n",
      "\n",
      " ---------------------------------------------------------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #3:  Explain the election laws in the states as described in the document.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The document mentions several key aspects of election laws in the states, primarily focused on voting rights and representation:\n",
      "\n",
      "1. **Due Process and Equal Protection**: States cannot deprive individuals of life, liberty, or property without due process of law. Additionally, they cannot deny any person within their jurisdiction equal protection under the laws. This establishes a legal foundation for ensuring fair treatment in the electoral process.\n",
      "\n",
      "2. **Apportionment of Representatives**: Representatives in Congress are to be apportioned among the states based on their respective populations, counting the total number of persons in each state while excluding non-taxed Indians. This ensures that representation in Congress is reflective of the population distribution across states.\n",
      "\n",
      "3. **Election of Senators**: The 17th Amendment specifies that Senators are to be elected directly by the people of each state, rather than appointed by state legislatures. Each state is represented by two Senators, and they serve a six-year term.\n",
      "\n",
      "4. **Voting Rights**: The 24th Amendment states that citizens have the right to vote in various elections without being subject to poll taxes or other restrictions. This amendment emphasizes the importance of access to the electoral process for all citizens.\n",
      "\n",
      "In summary, these elements establish a framework for fair representation and protection of voting rights within the states, mandating due process and equal protection, while also outlining specific methods for electing representatives and Senators.\n",
      "\n",
      " ---------------------------------------------------------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #4:  Bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Goodbye! If you have any more questions in the future, feel free to ask.\n",
      "\n",
      " ---------------------------------------------------------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #5:  Exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quitting ... bye bye!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "i = 1\n",
    "print('Write Quit or Exit to quit.')\n",
    "while True:\n",
    "    q = input(f'Question #{i}: ')\n",
    "    i = i + 1\n",
    "    if q.lower() in ['quit', 'exit']:\n",
    "        print('Quitting ... bye bye!')\n",
    "        time.sleep(2)\n",
    "        break\n",
    "    \n",
    "    answer = ask_and_get_answer(vector_store, q)\n",
    "    print(f'\\nAnswer: {answer[\"result\"]}')\n",
    "    print(f'\\n {\"-\" * 100} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc78b88c-f85c-4168-a193-64d428fada81",
   "metadata": {},
   "source": [
    "### Ask Wikipidea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ada903bb-f24a-4021-ac3d-2d2abb3d9ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes ... \n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "40d15e21-9512-47c9-84ed-3ea4128bb47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /opt/conda/lib/python3.11/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "data = load_from_wikipedia('Google Gemini', load_max_docs=100)\n",
    "chunks = chunk_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c97ba181-ab67-4aef-aa9c-326df15f0d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1991"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8dff5f03-0c17-4c88-b5aa-a3acd953d253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Gemini (chatbot)', 'summary': \"Gemini, formerly known as Bard, is a generative artificial intelligence chatbot developed by Google. Based on the large language model (LLM) of the same name, it was launched in 2023 after being developed as a direct response to the rise of OpenAI's ChatGPT. It was previously based on PaLM, and initially the LaMDA family of large language models.\\nLaMDA had been developed and announced in 2021, but it was not released to the public out of an abundance of caution. OpenAI's launch of ChatGPT in November 2022 and its subsequent popularity caught Google executives off-guard, prompting a sweeping response in the ensuing months. After mobilizing its workforce, the company launched Bard in a limited capacity in March 2023 before expanding to other countries in May. Bard took center stage during the 2023 Google I/O keynote in May and was upgraded to the Gemini LLM in December. In February 2024, Bard and Duet AI, another artificial intelligence product from Google, were unified under the Gemini brand, coinciding with the launch of an Android app.\", 'source': 'https://en.wikipedia.org/wiki/Gemini_(chatbot)'}, page_content=\"Gemini, formerly known as Bard, is a generative artificial intelligence chatbot developed by Google. Based on the large language model (LLM) of the same name, it was launched in 2023 after being developed as a direct response to the rise of OpenAI's\"),\n",
       " Document(metadata={'title': 'Gemini (chatbot)', 'summary': \"Gemini, formerly known as Bard, is a generative artificial intelligence chatbot developed by Google. Based on the large language model (LLM) of the same name, it was launched in 2023 after being developed as a direct response to the rise of OpenAI's ChatGPT. It was previously based on PaLM, and initially the LaMDA family of large language models.\\nLaMDA had been developed and announced in 2021, but it was not released to the public out of an abundance of caution. OpenAI's launch of ChatGPT in November 2022 and its subsequent popularity caught Google executives off-guard, prompting a sweeping response in the ensuing months. After mobilizing its workforce, the company launched Bard in a limited capacity in March 2023 before expanding to other countries in May. Bard took center stage during the 2023 Google I/O keynote in May and was upgraded to the Gemini LLM in December. In February 2024, Bard and Duet AI, another artificial intelligence product from Google, were unified under the Gemini brand, coinciding with the launch of an Android app.\", 'source': 'https://en.wikipedia.org/wiki/Gemini_(chatbot)'}, page_content='ChatGPT. It was previously based on PaLM, and initially the LaMDA family of large language models.'),\n",
       " Document(metadata={'title': 'Gemini (chatbot)', 'summary': \"Gemini, formerly known as Bard, is a generative artificial intelligence chatbot developed by Google. Based on the large language model (LLM) of the same name, it was launched in 2023 after being developed as a direct response to the rise of OpenAI's ChatGPT. It was previously based on PaLM, and initially the LaMDA family of large language models.\\nLaMDA had been developed and announced in 2021, but it was not released to the public out of an abundance of caution. OpenAI's launch of ChatGPT in November 2022 and its subsequent popularity caught Google executives off-guard, prompting a sweeping response in the ensuing months. After mobilizing its workforce, the company launched Bard in a limited capacity in March 2023 before expanding to other countries in May. Bard took center stage during the 2023 Google I/O keynote in May and was upgraded to the Gemini LLM in December. In February 2024, Bard and Duet AI, another artificial intelligence product from Google, were unified under the Gemini brand, coinciding with the launch of an Android app.\", 'source': 'https://en.wikipedia.org/wiki/Gemini_(chatbot)'}, page_content=\"LaMDA had been developed and announced in 2021, but it was not released to the public out of an abundance of caution. OpenAI's launch of ChatGPT in November 2022 and its subsequent popularity caught Google executives off-guard, prompting a sweeping\")]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "baa720bf-a240-42e7-93e8-a0d46ebc5bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index gemini and embeddings ...Ok\n"
     ]
    }
   ],
   "source": [
    "index_name = 'gemini'\n",
    "vector_store = insert_or_fetch_embeddings(index_name=index_name, chunks=chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a0182d35-41c7-4b76-9798-c8994d0e0553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Gemini is a large language model (LLM) developed by Google DeepMind, announced during the Google I/O keynote on May 10, 2023. It is touted as Google's \"largest and most capable AI model\" and is designed to emulate human behavior. Google intended to incorporate Gemini into various products, including Search, Ads, Chrome, Duet AI on Google Workspace, and AlphaCode 2. It serves as a more powerful successor to PaLM 2 and was made available only in English.\n"
     ]
    }
   ],
   "source": [
    "q = 'What is Google Gemini?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f1afc866-653f-45cd-a165-8e399b32992b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Gemini was announced on May 10, 2023, and the launch date was targeted for late 2023.\n"
     ]
    }
   ],
   "source": [
    "q = 'When was Google Gemini launched?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ab8ffb82-ac53-4f84-b2af-30b08a0ffdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Google Gemini family includes several models: Gemini Ultra, Gemini Pro, Gemini Flash, and Gemini Nano.\n"
     ]
    }
   ],
   "source": [
    "q = 'What models (llms) are available with Google Gemini?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bebac2e5-ead6-4810-9381-9378c1c7e19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes ... \n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41051035-d803-4d5f-a696-988f1552a7d7",
   "metadata": {},
   "source": [
    "## Using Chroma as a Vector DB\n",
    "\n",
    "- [Langchain - Chroma integration](https://docs.trychroma.com/integrations/langchain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e86d138-94ba-43bb-8bd5-bbdec7d6c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_chroma(chunks, persist_directory=\"./chroma_db\"):\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=1536)\n",
    "    vectore_store = Chroma.from_documents(chunks, embeddings, persist_directory=persist_directory)\n",
    "    return vectore_store\n",
    "\n",
    "def load_embeddings_chroma(persist_directory=\"./chroma_db\"):\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=1536)\n",
    "    vectore_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "    return vectore_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7800be32-b52c-4606-ae5a-5355d17d6e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files/rag_powered_by_google_search.pdf\n"
     ]
    }
   ],
   "source": [
    "data = load_documents('files/rag_powered_by_google_search.pdf')\n",
    "\n",
    "# Splitting the document into chunks\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "\n",
    "# Creating a Chroma vector store using the provided text chunks and embedding model (default is text-embedding-3-small)\n",
    "vector_store = create_embeddings_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9ed364e-4e8b-48de-acba-68c1ccfec3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is Vertex AI Search?',\n",
       " 'result': \"Vertex AI Search is a search feature within Google's Vertex AI platform that offers advanced capabilities including customizable answers, search tuning, vector search, grounding, and compliance updates tailored for enterprises. It integrates generative AI technologies to enhance the search experience and provide more relevant results.\"}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Asking questions\n",
    "q = 'What is Vertex AI Search?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba74ca92-f698-43d9-8369-4763721ace2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Vertex AI Search is a search feature within Google's Vertex AI platform that offers advanced capabilities including customizable answers, search tuning, vector search, grounding, and compliance updates tailored for enterprises. It integrates generative AI technologies to enhance the search experience and provide more relevant results.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9236c4a-1797-4634-a388-b4dc89a99304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The StackOverflow dataset had 8 million pairs of questions and answers.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a Chroma vector store from the specified directory (default ./chroma_db) \n",
    "vectore_store = load_embeddings_chroma()\n",
    "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "answer = ask_and_get_answer(vector_store, q, k=5)\n",
    "answer[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "76dc3f66-27fa-406e-b696-120f60b4db81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "# We can't ask follow-up questions. There is no memory (chat history) available.\n",
    "q = 'Multiply that number by 2.'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989a25b8-be9f-4878-8d92-3604744fbdf7",
   "metadata": {},
   "source": [
    "## Adding Memory (Chat History)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "48d1905c-bc26-4afe-9e0e-bf09d8e1657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "537c64af-a48b-4b13-b922-a69e139fb2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "retriever = vectore_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 15})\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    chain_type=\"stuff\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "accfd471-4a68-481f-94db-f64ac2d54bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to ask questions\n",
    "def ask_question(q, chain):\n",
    "    result = chain.invoke({'question': q})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9d23e088-cb25-4af2-8c56-b4d89844c656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files/rag_powered_by_google_search.pdf\n"
     ]
    }
   ],
   "source": [
    "data = load_documents('files/rag_powered_by_google_search.pdf')\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "vector_store = create_embeddings_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "46717d6d-8c2e-4152-9032-e16177bcaf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "candidate pairs in many real-world RAG scenarios. Therefore, it is vital\n",
      "for an AI model to learn and be able to predict the relationship between\n",
      "queries and their corresponding answers to deliver production-quality\n",
      "semantic search.\n",
      "Google Search sta\u0000ed incorporating semantic search in 2015, with the\n",
      "introduction of notewo\u0000hy AI search innovations like deep learning\n",
      "\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "candidate pairs in many real-world RAG scenarios. Therefore, it is vital\n",
      "for an AI model to learn and be able to predict the relationship between\n",
      "queries and their corresponding answers to deliver production-quality\n",
      "semantic search.\n",
      "Google Search sta\u0000ed incorporating semantic search in 2015, with the\n",
      "introduction of notewo\u0000hy AI search innovations like deep learning\n",
      "\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "candidate pairs in many real-world RAG scenarios. Therefore, it is vital\n",
      "for an AI model to learn and be able to predict the relationship between\n",
      "queries and their corresponding answers to deliver production-quality\n",
      "semantic search.\n",
      "Google Search sta\u0000ed incorporating semantic search in 2015, with the\n",
      "introduction of notewo\u0000hy AI search innovations like deep learning\n",
      "\n",
      "The primary reason for the lower search quality lies in the principle that\n",
      "\"the question is not the answer.\" A question like \"Why is the sky blue?\"\n",
      "and its answer, \"The sca\u0000ering of sunlight causes the blue color,\" have\n",
      "distinctly di\u0000erent meanings. Why, then, do you use similarity search to\n",
      "\u0000nd answers?\n",
      "Semantic search is not just similarity\n",
      "search\n",
      "In the Stack Ove\u0000low demo that we introduced in a previous post,\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "\n",
      "The primary reason for the lower search quality lies in the principle that\n",
      "\"the question is not the answer.\" A question like \"Why is the sky blue?\"\n",
      "and its answer, \"The sca\u0000ering of sunlight causes the blue color,\" have\n",
      "distinctly di\u0000erent meanings. Why, then, do you use similarity search to\n",
      "\u0000nd answers?\n",
      "Semantic search is not just similarity\n",
      "search\n",
      "In the Stack Ove\u0000low demo that we introduced in a previous post,\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "\n",
      "The primary reason for the lower search quality lies in the principle that\n",
      "\"the question is not the answer.\" A question like \"Why is the sky blue?\"\n",
      "and its answer, \"The sca\u0000ering of sunlight causes the blue color,\" have\n",
      "distinctly di\u0000erent meanings. Why, then, do you use similarity search to\n",
      "\u0000nd answers?\n",
      "Semantic search is not just similarity\n",
      "search\n",
      "In the Stack Ove\u0000low demo that we introduced in a previous post,\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "\n",
      "distinctly di\u0000erent meanings. Why, then, do you use similarity search to\n",
      "\u0000nd answers?\n",
      "Semantic search is not just similarity\n",
      "search\n",
      "In the Stack Ove\u0000low demo that we introduced in a previous post,\n",
      "\n",
      "distinctly di\u0000erent meanings. Why, then, do you use similarity search to\n",
      "\u0000nd answers?\n",
      "Semantic search is not just similarity\n",
      "search\n",
      "In the Stack Ove\u0000low demo that we introduced in a previous post,\n",
      "\n",
      "distinctly di\u0000erent meanings. Why, then, do you use similarity search to\n",
      "\u0000nd answers?\n",
      "Semantic search is not just similarity\n",
      "search\n",
      "In the Stack Ove\u0000low demo that we introduced in a previous post,\n",
      "\n",
      "candidate pairs in many real-world RAG scenarios. Therefore, it is vital\n",
      "for an AI model to learn and be able to predict the relationship between\n",
      "queries and their corresponding answers to deliver production-quality\n",
      "semantic search.\n",
      "\n",
      "candidate pairs in many real-world RAG scenarios. Therefore, it is vital\n",
      "for an AI model to learn and be able to predict the relationship between\n",
      "queries and their corresponding answers to deliver production-quality\n",
      "semantic search.\n",
      "\n",
      "candidate pairs in many real-world RAG scenarios. Therefore, it is vital\n",
      "for an AI model to learn and be able to predict the relationship between\n",
      "queries and their corresponding answers to deliver production-quality\n",
      "semantic search.\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'How many pairs of questions and answers had the StackOverflow dataset?',\n",
       " 'chat_history': [HumanMessage(content='How many pairs of questions and answers had the StackOverflow dataset?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The StackOverflow dataset had 8 million pairs of questions and answers.', additional_kwargs={}, response_metadata={})],\n",
       " 'answer': 'The StackOverflow dataset had 8 million pairs of questions and answers.'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "result = ask_question(q, retrieval_chain)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ffda3457-1ea5-44b9-b35f-eb1927105f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The StackOverflow dataset had 8 million pairs of questions and answers.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1bface65-b956-4d78-b93d-7c644690a22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\n",
      "Assistant: The StackOverflow dataset had 8 million pairs of questions and answers.\n",
      "Follow Up Input: Multiply that number by 10.\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "This substantial investment has helped to reduce costs and latency,\n",
      "enabling us to bring production-grade semantic search experience to\n",
      "billions of users.\n",
      "Google has spent years investing in and developing a powe\u0000ul set of\n",
      "\n",
      "This substantial investment has helped to reduce costs and latency,\n",
      "enabling us to bring production-grade semantic search experience to\n",
      "billions of users.\n",
      "Google has spent years investing in and developing a powe\u0000ul set of\n",
      "\n",
      "This substantial investment has helped to reduce costs and latency,\n",
      "enabling us to bring production-grade semantic search experience to\n",
      "billions of users.\n",
      "Google has spent years investing in and developing a powe\u0000ul set of\n",
      "\n",
      "Google has spent years investing in and developing a powe\u0000ul set of\n",
      "search technologies. For instance, Google Search processes semantic\n",
      "search with query and document embeddings with ScaNN, one of the\n",
      "world’s largest and fastest vector search infrastructures. ScaNN powers\n",
      "Google Search and many other Google services, quickly \u0000nding highly\n",
      "relevant documents and content to help users get the information they\n",
      "need in seconds. According to ANN benchmarking graphs, ScaNN is\n",
      "\n",
      "Google has spent years investing in and developing a powe\u0000ul set of\n",
      "search technologies. For instance, Google Search processes semantic\n",
      "search with query and document embeddings with ScaNN, one of the\n",
      "world’s largest and fastest vector search infrastructures. ScaNN powers\n",
      "Google Search and many other Google services, quickly \u0000nding highly\n",
      "relevant documents and content to help users get the information they\n",
      "need in seconds. According to ANN benchmarking graphs, ScaNN is\n",
      "\n",
      "Google has spent years investing in and developing a powe\u0000ul set of\n",
      "search technologies. For instance, Google Search processes semantic\n",
      "search with query and document embeddings with ScaNN, one of the\n",
      "world’s largest and fastest vector search infrastructures. ScaNN powers\n",
      "Google Search and many other Google services, quickly \u0000nding highly\n",
      "relevant documents and content to help users get the information they\n",
      "need in seconds. According to ANN benchmarking graphs, ScaNN is\n",
      "\n",
      "Overall, Google’s suite of breakthrough search technologies, including\n",
      "RankBrain, neural matching, ScaNN, and its family of TPUs, represents\n",
      "some of the most valuable technology assets built over the last decade.\n",
      "\n",
      "access the bene\u0000ts of the semantic search technology that has been\n",
      "tested by billions of users over many years.\n",
      "For example, with a popular keyword-based search engine loaded with\n",
      "all product catalogs from the Google Merchandise Store, a query \"warm\n",
      "\n",
      "access the bene\u0000ts of the semantic search technology that has been\n",
      "tested by billions of users over many years.\n",
      "For example, with a popular keyword-based search engine loaded with\n",
      "all product catalogs from the Google Merchandise Store, a query \"warm\n",
      "\n",
      "access the bene\u0000ts of the semantic search technology that has been\n",
      "tested by billions of users over many years.\n",
      "For example, with a popular keyword-based search engine loaded with\n",
      "all product catalogs from the Google Merchandise Store, a query \"warm\n",
      "\n",
      "e\u0000ciency of an advanced RAG system.\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "Google Search and many other Google services, quickly \u0000nding highly\n",
      "relevant documents and content to help users get the information they\n",
      "need in seconds. According to ANN benchmarking graphs, ScaNN is\n",
      "\n",
      "Google Search and many other Google services, quickly \u0000nding highly\n",
      "relevant documents and content to help users get the information they\n",
      "need in seconds. According to ANN benchmarking graphs, ScaNN is\n",
      "\n",
      "Google Search and many other Google services, quickly \u0000nding highly\n",
      "relevant documents and content to help users get the information they\n",
      "need in seconds. According to ANN benchmarking graphs, ScaNN is\n",
      "\n",
      "item, rendering the search useless. The models may not be familiar with\n",
      "newly added product names or trained to memorize millions of product\n",
      "model numbers in its embedding space.\n",
      "Contact sales Get started for freeCloud Blog\n",
      "Human: What is 8 million multiplied by 10?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'8 million multiplied by 10 is 80 million.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = 'Multiply that number by 10.'\n",
    "result = ask_question(q, retrieval_chain)\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "295b693e-529f-4e15-a873-4b86b2951bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\n",
      "Assistant: The StackOverflow dataset had 8 million pairs of questions and answers.\n",
      "Human: Multiply that number by 10.\n",
      "Assistant: 8 million multiplied by 10 is 80 million.\n",
      "Follow Up Input: Devide the result by 80.\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "e\u0000ciency of an advanced RAG system.\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "di\u0000erent requirements and a\u0000ributes, including type, activity, and style.\n",
      "Vector search and AI processor\n",
      "serving billions\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "di\u0000erent requirements and a\u0000ributes, including type, activity, and style.\n",
      "Vector search and AI processor\n",
      "serving billions\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "di\u0000erent requirements and a\u0000ributes, including type, activity, and style.\n",
      "Vector search and AI processor\n",
      "serving billions\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "This substantial investment has helped to reduce costs and latency,\n",
      "enabling us to bring production-grade semantic search experience to\n",
      "billions of users.\n",
      "Google has spent years investing in and developing a powe\u0000ul set of\n",
      "\n",
      "functionality is especially crucial in sectors like healthcare and \u0000nance,\n",
      "where keyword searches are fundamental for accessing precise and\n",
      "critical patient information, \u0000nancial data, or compliance-related\n",
      "documents.\n",
      "\n",
      "Despite these advancements, most RAG systems still use simple\n",
      "similarity search in vector databases to retrieve information. This\n",
      "approach can o\u0000en lead to the return of low-quality, irrelevant results.\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "Despite these advancements, most RAG systems still use simple\n",
      "similarity search in vector databases to retrieve information. This\n",
      "approach can o\u0000en lead to the return of low-quality, irrelevant results.\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "Despite these advancements, most RAG systems still use simple\n",
      "similarity search in vector databases to retrieve information. This\n",
      "approach can o\u0000en lead to the return of low-quality, irrelevant results.\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "Despite these advancements, most RAG systems still use simple\n",
      "similarity search in vector databases to retrieve information. This\n",
      "approach can o\u0000en lead to the return of low-quality, irrelevant results.\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "Despite these advancements, most RAG systems still use simple\n",
      "similarity search in vector databases to retrieve information. This\n",
      "approach can o\u0000en lead to the return of low-quality, irrelevant results.\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "Despite these advancements, most RAG systems still use simple\n",
      "similarity search in vector databases to retrieve information. This\n",
      "approach can o\u0000en lead to the return of low-quality, irrelevant results.\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "ranking system RankBrain. This innovation was quickly followed with\n",
      "neural matching to improve the accuracy of document retrieval in\n",
      "Search. Neural matching allows a retrieval engine to learn the\n",
      "relationships between a query’s intentions and highly relevant\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "ranking system RankBrain. This innovation was quickly followed with\n",
      "neural matching to improve the accuracy of document retrieval in\n",
      "Search. Neural matching allows a retrieval engine to learn the\n",
      "relationships between a query’s intentions and highly relevant\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "ranking system RankBrain. This innovation was quickly followed with\n",
      "neural matching to improve the accuracy of document retrieval in\n",
      "Search. Neural matching allows a retrieval engine to learn the\n",
      "relationships between a query’s intentions and highly relevant\n",
      "Contact sales Get started for freeCloud Blog\n",
      "Human: What is 80 million divided by 80?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'80 million divided by 80 is 1 million.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = 'Devide the result by 80.'\n",
    "result = ask_question(q, retrieval_chain)\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "554b2886-aa7b-4b9d-aeef-3b392207fc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='How many pairs of questions and answers had the StackOverflow dataset?' additional_kwargs={} response_metadata={}\n",
      "content='The StackOverflow dataset had 8 million pairs of questions and answers.' additional_kwargs={} response_metadata={}\n",
      "content='Multiply that number by 10.' additional_kwargs={} response_metadata={}\n",
      "content='8 million multiplied by 10 is 80 million.' additional_kwargs={} response_metadata={}\n",
      "content='Devide the result by 80.' additional_kwargs={} response_metadata={}\n",
      "content='80 million divided by 80 is 1 million.' additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "for item in result['chat_history']:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f82b10d-1c71-44b2-8847-748e1235e8e9",
   "metadata": {},
   "source": [
    "### Loop for asking questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a6e5d3a-6e86-43a1-9f68-367ba1d4cbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your question:  Tell me about Google Search technologies as described in the document.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\n",
      "Assistant: The StackOverflow dataset had 8 million pairs of questions and answers.\n",
      "Human: Multiply that number by 10.\n",
      "Assistant: 8 million multiplied by 10 is 80 million.\n",
      "Human: Devide the result by 80.\n",
      "Assistant: 80 million divided by 80 is 1 million.\n",
      "Human: Tell me about Google Search technologies as described in the document.\n",
      "Assistant: The Google Search technologies described in the document include RankBrain, neural matching, ScaNN, and a family of TPUs (Tensor Processing Units).\n",
      "Follow Up Input: Tell me about Google Search technologies as described in the document.\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Google has spent years investing in and developing a powe\u0000ul set of\n",
      "search technologies. For instance, Google Search processes semantic\n",
      "search with query and document embeddings with ScaNN, one of the\n",
      "world’s largest and fastest vector search infrastructures. ScaNN powers\n",
      "Google Search and many other Google services, quickly \u0000nding highly\n",
      "relevant documents and content to help users get the information they\n",
      "need in seconds. According to ANN benchmarking graphs, ScaNN is\n",
      "\n",
      "Google has spent years investing in and developing a powe\u0000ul set of\n",
      "search technologies. For instance, Google Search processes semantic\n",
      "search with query and document embeddings with ScaNN, one of the\n",
      "world’s largest and fastest vector search infrastructures. ScaNN powers\n",
      "Google Search and many other Google services, quickly \u0000nding highly\n",
      "relevant documents and content to help users get the information they\n",
      "need in seconds. According to ANN benchmarking graphs, ScaNN is\n",
      "\n",
      "Google has spent years investing in and developing a powe\u0000ul set of\n",
      "search technologies. For instance, Google Search processes semantic\n",
      "search with query and document embeddings with ScaNN, one of the\n",
      "world’s largest and fastest vector search infrastructures. ScaNN powers\n",
      "Google Search and many other Google services, quickly \u0000nding highly\n",
      "relevant documents and content to help users get the information they\n",
      "need in seconds. According to ANN benchmarking graphs, ScaNN is\n",
      "\n",
      "Overall, Google’s suite of breakthrough search technologies, including\n",
      "RankBrain, neural matching, ScaNN, and its family of TPUs, represents\n",
      "some of the most valuable technology assets built over the last decade.\n",
      "Inherited by Ve\u0000ex AI Search, these same technologies enable the\n",
      "delivery of Google-quality semantic search capabilities with\n",
      "millisecond-level latency and at a reasonable cost, all while le\u0000ing\n",
      "developers allocate more TPU resources to access the power of AI and\n",
      "\n",
      "Overall, Google’s suite of breakthrough search technologies, including\n",
      "RankBrain, neural matching, ScaNN, and its family of TPUs, represents\n",
      "some of the most valuable technology assets built over the last decade.\n",
      "Inherited by Ve\u0000ex AI Search, these same technologies enable the\n",
      "delivery of Google-quality semantic search capabilities with\n",
      "millisecond-level latency and at a reasonable cost, all while le\u0000ing\n",
      "developers allocate more TPU resources to access the power of AI and\n",
      "\n",
      "Overall, Google’s suite of breakthrough search technologies, including\n",
      "RankBrain, neural matching, ScaNN, and its family of TPUs, represents\n",
      "some of the most valuable technology assets built over the last decade.\n",
      "Inherited by Ve\u0000ex AI Search, these same technologies enable the\n",
      "delivery of Google-quality semantic search capabilities with\n",
      "millisecond-level latency and at a reasonable cost, all while le\u0000ing\n",
      "developers allocate more TPU resources to access the power of AI and\n",
      "\n",
      "Overall, Google’s suite of breakthrough search technologies, including\n",
      "RankBrain, neural matching, ScaNN, and its family of TPUs, represents\n",
      "some of the most valuable technology assets built over the last decade.\n",
      "\n",
      "Overall, Google’s suite of breakthrough search technologies, including\n",
      "RankBrain, neural matching, ScaNN, and its family of TPUs, represents\n",
      "some of the most valuable technology assets built over the last decade.\n",
      "\n",
      "Overall, Google’s suite of breakthrough search technologies, including\n",
      "RankBrain, neural matching, ScaNN, and its family of TPUs, represents\n",
      "some of the most valuable technology assets built over the last decade.\n",
      "\n",
      "search technologies. For instance, Google Search processes semantic\n",
      "search with query and document embeddings with ScaNN, one of the\n",
      "world’s largest and fastest vector search infrastructures. ScaNN powers\n",
      "\n",
      "search technologies. For instance, Google Search processes semantic\n",
      "search with query and document embeddings with ScaNN, one of the\n",
      "world’s largest and fastest vector search infrastructures. ScaNN powers\n",
      "\n",
      "search technologies. For instance, Google Search processes semantic\n",
      "search with query and document embeddings with ScaNN, one of the\n",
      "world’s largest and fastest vector search infrastructures. ScaNN powers\n",
      "\n",
      "right data so that it doesn't have to make things up, and you need the\n",
      "best search technology to get the best data.\n",
      "Google Search technologies, backed by over two decades of\n",
      "experience pioneering information retrieval at scale, o\u0000er powe\u0000ul\n",
      "information retrieval capabilities for RAG solutions. With Ve\u0000ex AI\n",
      "Search, for instance, developers can easily combine LLMs with Google\n",
      "Search technology, le\u0000ing them focus on building innovative AI apps\n",
      "instead of taking months to design and build their own advanced\n",
      "\n",
      "right data so that it doesn't have to make things up, and you need the\n",
      "best search technology to get the best data.\n",
      "Google Search technologies, backed by over two decades of\n",
      "experience pioneering information retrieval at scale, o\u0000er powe\u0000ul\n",
      "information retrieval capabilities for RAG solutions. With Ve\u0000ex AI\n",
      "Search, for instance, developers can easily combine LLMs with Google\n",
      "Search technology, le\u0000ing them focus on building innovative AI apps\n",
      "instead of taking months to design and build their own advanced\n",
      "\n",
      "right data so that it doesn't have to make things up, and you need the\n",
      "best search technology to get the best data.\n",
      "Google Search technologies, backed by over two decades of\n",
      "experience pioneering information retrieval at scale, o\u0000er powe\u0000ul\n",
      "information retrieval capabilities for RAG solutions. With Ve\u0000ex AI\n",
      "Search, for instance, developers can easily combine LLMs with Google\n",
      "Search technology, le\u0000ing them focus on building innovative AI apps\n",
      "instead of taking months to design and build their own advanced\n",
      "Human: What are the Google Search technologies described in the document?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The Google Search technologies described in the document include RankBrain, neural matching, ScaNN, and a family of TPUs (Tensor Processing Units).\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your question:  Is Vertex AI Search a fully-managed platform?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\n",
      "Assistant: The StackOverflow dataset had 8 million pairs of questions and answers.\n",
      "Human: Multiply that number by 10.\n",
      "Assistant: 8 million multiplied by 10 is 80 million.\n",
      "Human: Devide the result by 80.\n",
      "Assistant: 80 million divided by 80 is 1 million.\n",
      "Human: Tell me about Google Search technologies as described in the document.\n",
      "Assistant: The Google Search technologies described in the document include RankBrain, neural matching, ScaNN, and a family of TPUs (Tensor Processing Units).\n",
      "Human: Tell me about Google Search technologies as described in the document.\n",
      "Assistant: The Google Search technologies described in the document include RankBrain, neural matching, ScaNN, and a family of TPUs (Tensor Processing Units).\n",
      "Follow Up Input: Is Vertex AI Search a fully-managed platform?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Related articles\n",
      "AI & Machine Learning\n",
      "Vertex AI Search adds new\n",
      "generative AI capabilities and\n",
      "enterprise-ready features\n",
      "Vertex AI search offers customizable answers,\n",
      "search tuning, vector search, grounding and\n",
      "compliance updates for enterprises.\n",
      "\n",
      "Related articles\n",
      "AI & Machine Learning\n",
      "Vertex AI Search adds new\n",
      "generative AI capabilities and\n",
      "enterprise-ready features\n",
      "Vertex AI search offers customizable answers,\n",
      "search tuning, vector search, grounding and\n",
      "compliance updates for enterprises.\n",
      "\n",
      "Related articles\n",
      "AI & Machine Learning\n",
      "Vertex AI Search adds new\n",
      "generative AI capabilities and\n",
      "enterprise-ready features\n",
      "Vertex AI search offers customizable answers,\n",
      "search tuning, vector search, grounding and\n",
      "compliance updates for enterprises.\n",
      "\n",
      "Related articles\n",
      "AI & Machine Learning\n",
      "Vertex AI Search adds new\n",
      "generative AI capabilities and\n",
      "enterprise-ready features\n",
      "Vertex AI search offers customizable answers,\n",
      "search tuning, vector search, grounding and\n",
      "compliance updates for enterprises.\n",
      "By Lisa O'Malley • 6-minute read\n",
      "Posted inAI & Machine Learning\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "Related articles\n",
      "AI & Machine Learning\n",
      "Vertex AI Search adds new\n",
      "generative AI capabilities and\n",
      "enterprise-ready features\n",
      "Vertex AI search offers customizable answers,\n",
      "search tuning, vector search, grounding and\n",
      "compliance updates for enterprises.\n",
      "By Lisa O'Malley • 6-minute read\n",
      "Posted inAI & Machine Learning\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "Related articles\n",
      "AI & Machine Learning\n",
      "Vertex AI Search adds new\n",
      "generative AI capabilities and\n",
      "enterprise-ready features\n",
      "Vertex AI search offers customizable answers,\n",
      "search tuning, vector search, grounding and\n",
      "compliance updates for enterprises.\n",
      "By Lisa O'Malley • 6-minute read\n",
      "Posted inAI & Machine Learning\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "Hybrid search in Vertex AI Search\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "Hybrid search in Vertex AI Search\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "Hybrid search in Vertex AI Search\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "large language models as a commercial service.\n",
      "Vertex AI Search uses Tensor Processing Unit (TPU) to power its large-scale\n",
      "semantic search\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "large language models as a commercial service.\n",
      "Vertex AI Search uses Tensor Processing Unit (TPU) to power its large-scale\n",
      "semantic search\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "large language models as a commercial service.\n",
      "Vertex AI Search uses Tensor Processing Unit (TPU) to power its large-scale\n",
      "semantic search\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "developers allocate more TPU resources to access the power of AI and\n",
      "large language models as a commercial service.\n",
      "Vertex AI Search uses Tensor Processing Unit (TPU) to power its large-scale\n",
      "semantic search\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "developers allocate more TPU resources to access the power of AI and\n",
      "large language models as a commercial service.\n",
      "Vertex AI Search uses Tensor Processing Unit (TPU) to power its large-scale\n",
      "semantic search\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "developers allocate more TPU resources to access the power of AI and\n",
      "large language models as a commercial service.\n",
      "Vertex AI Search uses Tensor Processing Unit (TPU) to power its large-scale\n",
      "semantic search\n",
      "Contact sales Get started for freeCloud Blog\n",
      "Human: Is Vertex AI Search a fully-managed platform?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I don't know.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your question:  Is Vertex AI Search a fully-managed platform?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\n",
      "Assistant: The StackOverflow dataset had 8 million pairs of questions and answers.\n",
      "Human: Multiply that number by 10.\n",
      "Assistant: 8 million multiplied by 10 is 80 million.\n",
      "Human: Devide the result by 80.\n",
      "Assistant: 80 million divided by 80 is 1 million.\n",
      "Human: Tell me about Google Search technologies as described in the document.\n",
      "Assistant: The Google Search technologies described in the document include RankBrain, neural matching, ScaNN, and a family of TPUs (Tensor Processing Units).\n",
      "Human: Tell me about Google Search technologies as described in the document.\n",
      "Assistant: The Google Search technologies described in the document include RankBrain, neural matching, ScaNN, and a family of TPUs (Tensor Processing Units).\n",
      "Human: Is Vertex AI Search a fully-managed platform?\n",
      "Assistant: I don't know.\n",
      "Follow Up Input: Is Vertex AI Search a fully-managed platform?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Related articles\n",
      "AI & Machine Learning\n",
      "Vertex AI Search adds new\n",
      "generative AI capabilities and\n",
      "enterprise-ready features\n",
      "Vertex AI search offers customizable answers,\n",
      "search tuning, vector search, grounding and\n",
      "compliance updates for enterprises.\n",
      "\n",
      "Related articles\n",
      "AI & Machine Learning\n",
      "Vertex AI Search adds new\n",
      "generative AI capabilities and\n",
      "enterprise-ready features\n",
      "Vertex AI search offers customizable answers,\n",
      "search tuning, vector search, grounding and\n",
      "compliance updates for enterprises.\n",
      "\n",
      "Related articles\n",
      "AI & Machine Learning\n",
      "Vertex AI Search adds new\n",
      "generative AI capabilities and\n",
      "enterprise-ready features\n",
      "Vertex AI search offers customizable answers,\n",
      "search tuning, vector search, grounding and\n",
      "compliance updates for enterprises.\n",
      "\n",
      "Related articles\n",
      "AI & Machine Learning\n",
      "Vertex AI Search adds new\n",
      "generative AI capabilities and\n",
      "enterprise-ready features\n",
      "Vertex AI search offers customizable answers,\n",
      "search tuning, vector search, grounding and\n",
      "compliance updates for enterprises.\n",
      "By Lisa O'Malley • 6-minute read\n",
      "Posted inAI & Machine Learning\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "Related articles\n",
      "AI & Machine Learning\n",
      "Vertex AI Search adds new\n",
      "generative AI capabilities and\n",
      "enterprise-ready features\n",
      "Vertex AI search offers customizable answers,\n",
      "search tuning, vector search, grounding and\n",
      "compliance updates for enterprises.\n",
      "By Lisa O'Malley • 6-minute read\n",
      "Posted inAI & Machine Learning\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "Related articles\n",
      "AI & Machine Learning\n",
      "Vertex AI Search adds new\n",
      "generative AI capabilities and\n",
      "enterprise-ready features\n",
      "Vertex AI search offers customizable answers,\n",
      "search tuning, vector search, grounding and\n",
      "compliance updates for enterprises.\n",
      "By Lisa O'Malley • 6-minute read\n",
      "Posted inAI & Machine Learning\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "Hybrid search in Vertex AI Search\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "Hybrid search in Vertex AI Search\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "Hybrid search in Vertex AI Search\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "large language models as a commercial service.\n",
      "Vertex AI Search uses Tensor Processing Unit (TPU) to power its large-scale\n",
      "semantic search\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "large language models as a commercial service.\n",
      "Vertex AI Search uses Tensor Processing Unit (TPU) to power its large-scale\n",
      "semantic search\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "large language models as a commercial service.\n",
      "Vertex AI Search uses Tensor Processing Unit (TPU) to power its large-scale\n",
      "semantic search\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "developers allocate more TPU resources to access the power of AI and\n",
      "large language models as a commercial service.\n",
      "Vertex AI Search uses Tensor Processing Unit (TPU) to power its large-scale\n",
      "semantic search\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "developers allocate more TPU resources to access the power of AI and\n",
      "large language models as a commercial service.\n",
      "Vertex AI Search uses Tensor Processing Unit (TPU) to power its large-scale\n",
      "semantic search\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "developers allocate more TPU resources to access the power of AI and\n",
      "large language models as a commercial service.\n",
      "Vertex AI Search uses Tensor Processing Unit (TPU) to power its large-scale\n",
      "semantic search\n",
      "Contact sales Get started for freeCloud Blog\n",
      "Human: Is Vertex AI Search a fully-managed platform?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I don't know.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your question:  What advancements have been made in semantic search?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\n",
      "Assistant: The StackOverflow dataset had 8 million pairs of questions and answers.\n",
      "Human: Multiply that number by 10.\n",
      "Assistant: 8 million multiplied by 10 is 80 million.\n",
      "Human: Devide the result by 80.\n",
      "Assistant: 80 million divided by 80 is 1 million.\n",
      "Human: Tell me about Google Search technologies as described in the document.\n",
      "Assistant: The Google Search technologies described in the document include RankBrain, neural matching, ScaNN, and a family of TPUs (Tensor Processing Units).\n",
      "Human: Tell me about Google Search technologies as described in the document.\n",
      "Assistant: The Google Search technologies described in the document include RankBrain, neural matching, ScaNN, and a family of TPUs (Tensor Processing Units).\n",
      "Human: Is Vertex AI Search a fully-managed platform?\n",
      "Assistant: I don't know.\n",
      "Human: Is Vertex AI Search a fully-managed platform?\n",
      "Assistant: I don't know.\n",
      "Follow Up Input: What advancements have been made in semantic search?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Another common misconception is that semantic search is a relatively\n",
      "new innovation that has gained popularity with the rise of LLMs. While\n",
      "semantic search may be among the current hot topics, it’s actually the\n",
      "result of years of research and development. Google has been at the\n",
      "forefront of semantic search development for nearly a decade, sta\u0000ing\n",
      "with a strategic decision to invest in developing its family of custom, in-\n",
      "house AI processors back in 2013 — the Tensor Processing Unit (TPU).\n",
      "\n",
      "Another common misconception is that semantic search is a relatively\n",
      "new innovation that has gained popularity with the rise of LLMs. While\n",
      "semantic search may be among the current hot topics, it’s actually the\n",
      "result of years of research and development. Google has been at the\n",
      "forefront of semantic search development for nearly a decade, sta\u0000ing\n",
      "with a strategic decision to invest in developing its family of custom, in-\n",
      "house AI processors back in 2013 — the Tensor Processing Unit (TPU).\n",
      "\n",
      "Another common misconception is that semantic search is a relatively\n",
      "new innovation that has gained popularity with the rise of LLMs. While\n",
      "semantic search may be among the current hot topics, it’s actually the\n",
      "result of years of research and development. Google has been at the\n",
      "forefront of semantic search development for nearly a decade, sta\u0000ing\n",
      "with a strategic decision to invest in developing its family of custom, in-\n",
      "house AI processors back in 2013 — the Tensor Processing Unit (TPU).\n",
      "\n",
      "result of years of research and development. Google has been at the\n",
      "forefront of semantic search development for nearly a decade, sta\u0000ing\n",
      "with a strategic decision to invest in developing its family of custom, in-\n",
      "\n",
      "result of years of research and development. Google has been at the\n",
      "forefront of semantic search development for nearly a decade, sta\u0000ing\n",
      "with a strategic decision to invest in developing its family of custom, in-\n",
      "\n",
      "result of years of research and development. Google has been at the\n",
      "forefront of semantic search development for nearly a decade, sta\u0000ing\n",
      "with a strategic decision to invest in developing its family of custom, in-\n",
      "\n",
      "Google Search sta\u0000ed incorporating semantic search in 2015, with the\n",
      "introduction of notewo\u0000hy AI search innovations like deep learning\n",
      "ranking system RankBrain. This innovation was quickly followed with\n",
      "\n",
      "Google Search sta\u0000ed incorporating semantic search in 2015, with the\n",
      "introduction of notewo\u0000hy AI search innovations like deep learning\n",
      "ranking system RankBrain. This innovation was quickly followed with\n",
      "\n",
      "Google Search sta\u0000ed incorporating semantic search in 2015, with the\n",
      "introduction of notewo\u0000hy AI search innovations like deep learning\n",
      "ranking system RankBrain. This innovation was quickly followed with\n",
      "\n",
      "search technologies. For instance, Google Search processes semantic\n",
      "search with query and document embeddings with ScaNN, one of the\n",
      "world’s largest and fastest vector search infrastructures. ScaNN powers\n",
      "\n",
      "search technologies. For instance, Google Search processes semantic\n",
      "search with query and document embeddings with ScaNN, one of the\n",
      "world’s largest and fastest vector search infrastructures. ScaNN powers\n",
      "\n",
      "search technologies. For instance, Google Search processes semantic\n",
      "search with query and document embeddings with ScaNN, one of the\n",
      "world’s largest and fastest vector search infrastructures. ScaNN powers\n",
      "\n",
      "Beyond our expe\u0000ise in semantic search, we have also spent over 25\n",
      "years advancing keyword-based (or token-based) search technology.\n",
      "Ve\u0000ex AI Search builds on this by providing a hybrid search engine that\n",
      "\n",
      "Beyond our expe\u0000ise in semantic search, we have also spent over 25\n",
      "years advancing keyword-based (or token-based) search technology.\n",
      "Ve\u0000ex AI Search builds on this by providing a hybrid search engine that\n",
      "\n",
      "Beyond our expe\u0000ise in semantic search, we have also spent over 25\n",
      "years advancing keyword-based (or token-based) search technology.\n",
      "Ve\u0000ex AI Search builds on this by providing a hybrid search engine that\n",
      "Human: What advancements have been made in semantic search?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Advancements in semantic search include the development of deep learning ranking systems like Google's RankBrain, which was introduced in 2015. Additionally, Google has implemented query and document embeddings using ScaNN, one of the largest and fastest vector search infrastructures. These innovations have been the result of years of research and development, with Google being at the forefront of semantic search technology.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your question:  bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bye bye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    q = input('Your question: ')\n",
    "    if q.lower() in 'exit quit bye':\n",
    "        print('Bye bye!')\n",
    "        break\n",
    "    result = ask_question(q, retrieval_chain)\n",
    "    print(result['answer'])\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311813ae-06a6-4c1a-ae06-e1bac68d76ba",
   "metadata": {},
   "source": [
    "## Using a Custom Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dccc9ccf-364f-48bc-bbae-2520f55113fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4o', temperature=0)\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "\n",
    "system_template = r'''\n",
    "Use the following pieces of context to answer the user's question.\n",
    "Before answering translate your response to Jamaican Patois.\n",
    "If you don't find the answer in the provided context, just respond \"I don't know.\"\n",
    "---------------\n",
    "Context: ```{context}```\n",
    "'''\n",
    "\n",
    "user_template = '''\n",
    "Question: ```{question}```\n",
    "'''\n",
    "\n",
    "messages= [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(user_template)\n",
    "]\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    chain_type='stuff',\n",
    "    combine_docs_chain_kwargs={'prompt': qa_prompt },\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "75247b15-81a4-479b-a181-216943b44a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Use the following pieces of context to answer the user's question.\n",
      "Before answering translate your response to Jamaican Patois.\n",
      "If you don't find the answer in the provided context, just respond \"I don't know.\"\n",
      "---------------\n",
      "Context: ```simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "candidate pairs in many real-world RAG scenarios. Therefore, it is vital\n",
      "for an AI model to learn and be able to predict the relationship between\n",
      "queries and their corresponding answers to deliver production-quality\n",
      "semantic search.\n",
      "Google Search sta\u0000ed incorporating semantic search in 2015, with the\n",
      "introduction of notewo\u0000hy AI search innovations like deep learning\n",
      "\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "candidate pairs in many real-world RAG scenarios. Therefore, it is vital\n",
      "for an AI model to learn and be able to predict the relationship between\n",
      "queries and their corresponding answers to deliver production-quality\n",
      "semantic search.\n",
      "Google Search sta\u0000ed incorporating semantic search in 2015, with the\n",
      "introduction of notewo\u0000hy AI search innovations like deep learning```\n",
      "\n",
      "Human: \n",
      "Question: ```How many pairs of questions and answers had the StackOverflow dataset?```\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'How many pairs of questions and answers had the StackOverflow dataset?',\n",
       " 'chat_history': [HumanMessage(content='How many pairs of questions and answers had the StackOverflow dataset?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Di StackOverflow dataset did have 8 million pairs of questions and answers.', additional_kwargs={}, response_metadata={})],\n",
       " 'answer': 'Di StackOverflow dataset did have 8 million pairs of questions and answers.'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# db = load_embeddings_chroma()\n",
    "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "result = ask_question(q, crc)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f4b21714-b88d-4d56-b710-00c8ad6555b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Di StackOverflow dataset did have 8 million pairs of questions and answers.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3f1ff259-7b40-49b2-a1c9-a1bc347eac7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\n",
      "Assistant: Di StackOverflow dataset did have 8 million pairs of questions and answers.\n",
      "Follow Up Input: When was Elon Musk born?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Use the following pieces of context to answer the user's question.\n",
      "Before answering translate your response to Jamaican Patois.\n",
      "If you don't find the answer in the provided context, just respond \"I don't know.\"\n",
      "---------------\n",
      "Context: ```house AI processors back in 2013 — the Tensor Processing Unit (TPU).\n",
      "TPUs are speci\u0000cally tailored to provide the underlying power needed\n",
      "to suppo\u0000 machine learning and AI workloads, but their genesis is\n",
      "\n",
      "house AI processors back in 2013 — the Tensor Processing Unit (TPU).\n",
      "TPUs are speci\u0000cally tailored to provide the underlying power needed\n",
      "to suppo\u0000 machine learning and AI workloads, but their genesis is\n",
      "\n",
      "house AI processors back in 2013 — the Tensor Processing Unit (TPU).\n",
      "TPUs are speci\u0000cally tailored to provide the underlying power needed\n",
      "to suppo\u0000 machine learning and AI workloads, but their genesis is\n",
      "\n",
      "Follow us\n",
      "Telecommunications\n",
      "By Ankur Jain • 7-minute read\n",
      "Gaming\n",
      "By Patrick Smith • 8-minute read\n",
      "Application Development\n",
      "By Mete Atamel • 5-minute read\n",
      "AI & Machine Learning\n",
      "By Burak Gokturk • 3-minute read\n",
      "MWC’24: Unlocking the AI-enabled Telco with\n",
      "\n",
      "Follow us\n",
      "Telecommunications\n",
      "By Ankur Jain • 7-minute read\n",
      "Gaming\n",
      "By Patrick Smith • 8-minute read\n",
      "Application Development\n",
      "By Mete Atamel • 5-minute read\n",
      "AI & Machine Learning\n",
      "By Burak Gokturk • 3-minute read\n",
      "MWC’24: Unlocking the AI-enabled Telco with```\n",
      "\n",
      "Human: \n",
      "Question: ```When was Elon Musk born?```\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Mi nuh know.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = 'When was Elon Musk born?'\n",
    "result = ask_question(q, crc)\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e3cb2c08-1a2b-43d0-8246-03a8354ec6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\n",
      "Assistant: Di StackOverflow dataset did have 8 million pairs of questions and answers.\n",
      "Human: When was Elon Musk born?\n",
      "Assistant: Mi nuh know.\n",
      "Follow Up Input: When was Bill Gates born?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Use the following pieces of context to answer the user's question.\n",
      "Before answering translate your response to Jamaican Patois.\n",
      "If you don't find the answer in the provided context, just respond \"I don't know.\"\n",
      "---------------\n",
      "Context: ```This substantial investment has helped to reduce costs and latency,\n",
      "enabling us to bring production-grade semantic search experience to\n",
      "billions of users.\n",
      "Google has spent years investing in and developing a powe\u0000ul set of\n",
      "\n",
      "Google Cloud Google Cloud Products Privacy Terms\n",
      "Help  English \n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "Google Cloud Google Cloud Products Privacy Terms\n",
      "Help  English \n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "Google Cloud Google Cloud Products Privacy Terms\n",
      "Help  English \n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "Google Cloud Google Cloud Products Privacy Terms\n",
      "Help  English \n",
      "Contact sales Get started for freeCloud Blog```\n",
      "\n",
      "Human: \n",
      "Question: ```When was Bill Gates born?```\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Mi nuh know.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = 'When was Bill Gates born?'\n",
    "result = ask_question(q, crc)\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58947b2c-9b1c-4860-a90a-4aa750d6a180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
