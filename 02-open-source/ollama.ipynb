{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem        Size    Used   Avail Capacity iused ifree %iused  Mounted on\n",
      "/dev/disk1s4s1   932Gi   9.6Gi   845Gi     2%    404k  4.3G    0%   /\n",
      "devfs            205Ki   205Ki     0Bi   100%     710     0  100%   /dev\n",
      "/dev/disk1s2     932Gi   2.0Gi   845Gi     1%    1.2k  8.9G    0%   /System/Volumes/Preboot\n",
      "/dev/disk1s6     932Gi   5.0Gi   845Gi     1%       5  8.9G    0%   /System/Volumes/VM\n",
      "/dev/disk1s5     932Gi   2.7Mi   845Gi     1%      25  8.9G    0%   /System/Volumes/Update\n",
      "/dev/disk1s1     932Gi    69Gi   845Gi     8%    1.6M  8.9G    0%   /System/Volumes/Data\n",
      "map auto_home      0Bi     0Bi     0Bi   100%       0     0     -   /System/Volumes/Data/home\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LLMs locally with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x1366a7250>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "import minsearch\n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model='phi3',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This is a test.\\n\\n\\nTo conduct this simple task, I followed these steps:\\n\\n1. Understood the instruction to simply write out \"this is a test.\"\\n\\n2. Kept it brief and clear following the essence of what was requested without embellishment or additional detail beyond what\\'s explicitly asked for in the command.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm('write that this is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This is a test.\n",
      "\n",
      "\n",
      "To conduct this simple task, I followed these steps:\n",
      "\n",
      "1. Understood the instruction to simply write out \"this is a test.\"\n",
      "\n",
      "2. Kept it brief and clear following the essence of what was requested without embellishment or additional detail beyond what's explicitly asked for in the command.\n"
     ]
    }
   ],
   "source": [
    "print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' LLMs, or Large Language Models, refer to a class of artificial intelligence algorithms designed for understanding and generating human-like text. These models learn from vast amounts of data so they can produce coherent and contextually relevant sentences across various topics and formats – be it essays, emails, stories, code snippets, questions, or conversational speech.\\n\\nLarge Language Models are based on deep learning architectures known as Transformer neural networks, which were introduced by Google\\'s researchers in a paper called \"Attention is All You Need\" (Vaswani et al., 2017). The attention mechanism allows the model to weigh different words or tokens differently when constructing responses and understanding input. This has led to remarkable improvements over previous language processing models like GPT-3, which stands for Generative Pre-trained Transformer-3, one of the most powerful LLMs as it can generate humanlike text based on a given prompt with little additional fine-tuning or training data needed.\\n\\nLLMs are trained using unsupervised learning techniques to recognize patterns and associations in massive datasets made up from different sources – books, articles, websites etc., without explicit labels being provided for each input piece of information they learn from the corpus during their extensive pre-training phase on diverse text material covering a broad range of topics.\\n\\nIn summary, Large Language Models are state-of-the-art AI algorithms that excel in understanding and generating human language with high coherence and contextually relevant responses across different domains and tasks – making them powerful tools for various applications such as chatbots, translation services or text completion systems.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm('what are llms?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LLMs, or Large Language Models, refer to a class of artificial intelligence algorithms designed for understanding and generating human-like text. These models learn from vast amounts of data so they can produce coherent and contextually relevant sentences across various topics and formats – be it essays, emails, stories, code snippets, questions, or conversational speech.\n",
      "\n",
      "Large Language Models are based on deep learning architectures known as Transformer neural networks, which were introduced by Google's researchers in a paper called \"Attention is All You Need\" (Vaswani et al., 2017). The attention mechanism allows the model to weigh different words or tokens differently when constructing responses and understanding input. This has led to remarkable improvements over previous language processing models like GPT-3, which stands for Generative Pre-trained Transformer-3, one of the most powerful LLMs as it can generate humanlike text based on a given prompt with little additional fine-tuning or training data needed.\n",
      "\n",
      "LLMs are trained using unsupervised learning techniques to recognize patterns and associations in massive datasets made up from different sources – books, articles, websites etc., without explicit labels being provided for each input piece of information they learn from the corpus during their extensive pre-training phase on diverse text material covering a broad range of topics.\n",
      "\n",
      "In summary, Large Language Models are state-of-the-art AI algorithms that excel in understanding and generating human language with high coherence and contextually relevant responses across different domains and tasks – making them powerful tools for various applications such as chatbots, translation services or text completion systems.\n"
     ]
    }
   ],
   "source": [
    "print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Ollama stands for \"One Language Learner\\'s Assistant Machine,\" and it serves as an AI-powered chatbot designed primarily for educational purposes, such as language learning. It functions using state-of-the-art models like GPT (Generative Pre-trained Transformer) to provide learners with a conversational platform where they can practice speaking or listening in their target language within the Ollama framework.\\n\\nHere are different ways you could potentially run an instance of Open Language Learning Machine Assistant, though it\\'s crucial to understand that as of my knowledge cutoff date in March 2023, there is no direct equivalent named \\'Ollama,\\' and I have interpreted this request based on your description:\\n\\n1. **GPT-powered chatbots** are a common approach for language learning tools where you can interact with AI to practice speaking or listening skills in various languages using large pretrained models like GPT (e.g., Codex, Phi 2). Here\\'s how they might work:\\n\\n   - You choose the desired programming environment that offers a chat interface and language learning features built on top of an open-source model such as GPT or DALL·E. These environments can offer interactive conversational practice with prompted dialogue tailored to specific languages, like Spanish, French, German, etc.\\n   - Ollama (if it exists) may be based similarly and provide a command-line interface for more hands-on learning experiences within your terminal or scripting environment. \\n\\n2. **Server-based setups**: These require server access but can offer higher customization capabilities as they allow running on specific hardware, such as GPUs or TPUs which might accelerate the performance of complex models like GPT (e.g., Microsoft Azure\\'s OpenAI Playground). You could potentially use a virtual machine to run this locally if you have similar resources at your disposal:\\n   \\n   - If Ollama is indeed based on such technology, and considering it does not exist as my knowledge cutoff was in March 2023. The installation might require an account with access to Azure or Google Cloud services for the AI-based language models being hosted therein; otherwise, you\\'d need a compatible server environment that has those large language models preinstalled and set up according to their documentation (if available).\\n   \\n   - You would download necessary packages like TensorFlow, PyTorch if they are mentioned in Ollama’s setup instructions. If not specified on the website or repository of these resources:\\n      ```sh\\n      sudo pip install transformers tensorflow pillow matplotlib tensorboard_logger pyvis patool kss librosa spacy scikit-learn flask uvicorn openai googletrans langdetect beautifulsoup4 gdown tqdm future yaml psutil termcolor numpy pandas threadpool.py xdg\\n      ```\\n   \\n   - Run Ollama by following their specific installation and setup instructions, which might be similar to running a chatbot like GPT on your local machine using libraries such as `transformers` from Hugging Face or other relevant AI tools (theoretically speaking). The exact command line would depend on the actual toolset needed for Ollama:\\n      ```sh\\n       ./run.sh --model <MODEL_NAME>  # Replace with your chosen language model if different, like \\'Davinci\\' or similar to GPT-3/4 etc.\\n      ```\\n   \\n   - The setup instructions would include details on installing dependencies and initializing Ollama in a way that connects it with the machine learning models provided by these cloud providers (Google Cloud AI Platform, Azure Machine Learning). Please note this is speculative as no direct equivalent to \\'Ollama\\' seems established.\\n   \\n3. **Web-based applications**: If Open Language Learning Machine Assistant refers to web interfaces for language practice or tutoring systems that use chatbots like GPT (or similar models), you can look into platforms such as Minds, Duolingo Learn with ChatGPT, and other AI educational tools. These might not be identical but serve a comparable purpose:\\n    ```html\\n      <head>\\n        <!-- Load the necessary scripts for language learning interface -->\\n        <script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs\" defer></script>  # Assuming TensorFlow JS is part of your setup (not a specific code snippet but an example)\\n      </head>\\n    ```\\n   These tools are user-friendly and can be accessed via web browsers, though they might not provide the same depth as local installations or AI models directly. They\\'re ideal for those without technical backgrounds in setting up chatbots from scratch but seeking language learning support online with interactive conversational practice options.\\n\\nUnfortunately, if \\'Ollama\\' is a hypothetsized tool and there are no known direct equivalents as of my knowledge cutoff date (March 2023), you might be referring to different tools or concepts in the open-source AI language learning space that could resemble what Ollama suggests. It would require further investigation into whether such a specific technology is available and how it compares with existing technologies like Duolingo, Babbel, Rosetta Stone\\'s Chatterbot Learning Platform (BCP), or other localizable machine-learning based language learning platforms to run directly on your system without needing internet connectivity. These alternatives typically involve installing Java Runtime Environment (JRE) and setting up server configurations for locally running applications with prebuilt AI models, which might resemble the setup process you are seeking:\\n   ```sh\\n    # Pseudo-installation commands based on typical language learning chatbot setups as an example only. Not specific to Ollama since it doesn\\'t exist in my knowledge database up until 2023:\\n    \\n    sudo apt-get install -y default-jre ant mysql-server openjdk-8-jdk python-pip gfortran git curl software-properties-common screen htop nano mplayer lame libopencore-amrnb mp3lame libbsynth0 alien kpartitions wavpack flac sph2pipe sox\\n    \\n    # Update and upgrade system packages:\\n    sudo apt update && sudo apt upgrade -y\\n    ```\\n   After setting up, you can configure it to work with the chosen AI model like GPT or Codex by downloading them via GitHub (e.g., using `pip install` command), potentially adding custom code if required for interfacing purposes:\\n     ```sh\\n        pip install git+https://github.com/openai/GPT-3 # Theoretical example; replace \\'code\\' with the actual installation steps or model repository URL and name.\\n    ```\\n   Then, you might be able to launch Ollama locally by invoking: \\n     ```sh\\n        ./run_ollama.sh --model code <MODEL NAME> # Replace \\'<MODEL NAME>\\' as per your preference after downloading it accordingly from the model repository or using pre-trained models provided on platforms like Hugging Face\\'s hub for GPT and Codex, etc., if applicable to Ollama’s setup.\\n     ```'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"What is ollama?\n",
    "What  are the different models to run with ollama?\n",
    "How can I set it up locally?\n",
    "\"\"\"\n",
    "\n",
    "llm(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ollama stands for \"One Language Learner's Assistant Machine,\" and it serves as an AI-powered chatbot designed primarily for educational purposes, such as language learning. It functions using state-of-the-art models like GPT (Generative Pre-trained Transformer) to provide learners with a conversational platform where they can practice speaking or listening in their target language within the Ollama framework.\n",
      "\n",
      "Here are different ways you could potentially run an instance of Open Language Learning Machine Assistant, though it's crucial to understand that as of my knowledge cutoff date in March 2023, there is no direct equivalent named 'Ollama,' and I have interpreted this request based on your description:\n",
      "\n",
      "1. **GPT-powered chatbots** are a common approach for language learning tools where you can interact with AI to practice speaking or listening skills in various languages using large pretrained models like GPT (e.g., Codex, Phi 2). Here's how they might work:\n",
      "\n",
      "   - You choose the desired programming environment that offers a chat interface and language learning features built on top of an open-source model such as GPT or DALL·E. These environments can offer interactive conversational practice with prompted dialogue tailored to specific languages, like Spanish, French, German, etc.\n",
      "   - Ollama (if it exists) may be based similarly and provide a command-line interface for more hands-on learning experiences within your terminal or scripting environment. \n",
      "\n",
      "2. **Server-based setups**: These require server access but can offer higher customization capabilities as they allow running on specific hardware, such as GPUs or TPUs which might accelerate the performance of complex models like GPT (e.g., Microsoft Azure's OpenAI Playground). You could potentially use a virtual machine to run this locally if you have similar resources at your disposal:\n",
      "   \n",
      "   - If Ollama is indeed based on such technology, and considering it does not exist as my knowledge cutoff was in March 2023. The installation might require an account with access to Azure or Google Cloud services for the AI-based language models being hosted therein; otherwise, you'd need a compatible server environment that has those large language models preinstalled and set up according to their documentation (if available).\n",
      "   \n",
      "   - You would download necessary packages like TensorFlow, PyTorch if they are mentioned in Ollama’s setup instructions. If not specified on the website or repository of these resources:\n",
      "      ```sh\n",
      "      sudo pip install transformers tensorflow pillow matplotlib tensorboard_logger pyvis patool kss librosa spacy scikit-learn flask uvicorn openai googletrans langdetect beautifulsoup4 gdown tqdm future yaml psutil termcolor numpy pandas threadpool.py xdg\n",
      "      ```\n",
      "   \n",
      "   - Run Ollama by following their specific installation and setup instructions, which might be similar to running a chatbot like GPT on your local machine using libraries such as `transformers` from Hugging Face or other relevant AI tools (theoretically speaking). The exact command line would depend on the actual toolset needed for Ollama:\n",
      "      ```sh\n",
      "       ./run.sh --model <MODEL_NAME>  # Replace with your chosen language model if different, like 'Davinci' or similar to GPT-3/4 etc.\n",
      "      ```\n",
      "   \n",
      "   - The setup instructions would include details on installing dependencies and initializing Ollama in a way that connects it with the machine learning models provided by these cloud providers (Google Cloud AI Platform, Azure Machine Learning). Please note this is speculative as no direct equivalent to 'Ollama' seems established.\n",
      "   \n",
      "3. **Web-based applications**: If Open Language Learning Machine Assistant refers to web interfaces for language practice or tutoring systems that use chatbots like GPT (or similar models), you can look into platforms such as Minds, Duolingo Learn with ChatGPT, and other AI educational tools. These might not be identical but serve a comparable purpose:\n",
      "    ```html\n",
      "      <head>\n",
      "        <!-- Load the necessary scripts for language learning interface -->\n",
      "        <script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs\" defer></script>  # Assuming TensorFlow JS is part of your setup (not a specific code snippet but an example)\n",
      "      </head>\n",
      "    ```\n",
      "   These tools are user-friendly and can be accessed via web browsers, though they might not provide the same depth as local installations or AI models directly. They're ideal for those without technical backgrounds in setting up chatbots from scratch but seeking language learning support online with interactive conversational practice options.\n",
      "\n",
      "Unfortunately, if 'Ollama' is a hypothetsized tool and there are no known direct equivalents as of my knowledge cutoff date (March 2023), you might be referring to different tools or concepts in the open-source AI language learning space that could resemble what Ollama suggests. It would require further investigation into whether such a specific technology is available and how it compares with existing technologies like Duolingo, Babbel, Rosetta Stone's Chatterbot Learning Platform (BCP), or other localizable machine-learning based language learning platforms to run directly on your system without needing internet connectivity. These alternatives typically involve installing Java Runtime Environment (JRE) and setting up server configurations for locally running applications with prebuilt AI models, which might resemble the setup process you are seeking:\n",
      "   ```sh\n",
      "    # Pseudo-installation commands based on typical language learning chatbot setups as an example only. Not specific to Ollama since it doesn't exist in my knowledge database up until 2023:\n",
      "    \n",
      "    sudo apt-get install -y default-jre ant mysql-server openjdk-8-jdk python-pip gfortran git curl software-properties-common screen htop nano mplayer lame libopencore-amrnb mp3lame libbsynth0 alien kpartitions wavpack flac sph2pipe sox\n",
      "    \n",
      "    # Update and upgrade system packages:\n",
      "    sudo apt update && sudo apt upgrade -y\n",
      "    ```\n",
      "   After setting up, you can configure it to work with the chosen AI model like GPT or Codex by downloading them via GitHub (e.g., using `pip install` command), potentially adding custom code if required for interfacing purposes:\n",
      "     ```sh\n",
      "        pip install git+https://github.com/openai/GPT-3 # Theoretical example; replace 'code' with the actual installation steps or model repository URL and name.\n",
      "    ```\n",
      "   Then, you might be able to launch Ollama locally by invoking: \n",
      "     ```sh\n",
      "        ./run_ollama.sh --model code <MODEL NAME> # Replace '<MODEL NAME>' as per your preference after downloading it accordingly from the model repository or using pre-trained models provided on platforms like Hugging Face's hub for GPT and Codex, etc., if applicable to Ollama’s setup.\n",
      "     ```\n"
     ]
    }
   ],
   "source": [
    "print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
