{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GrWfdTsY302"
   },
   "source": [
    "# Building with LangGraph\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Youtube: Flow Engineering with LangChain/LangGraph and CodiumAI](https://www.youtube.com/watch?v=eBjxz7qrNBs)\n",
    "- [LangGraph - quickStart](https://langchain-ai.github.io/langgraph/#overview)\n",
    "- [LangGraph - Tutorials](https://langchain-ai.github.io/langgraph/tutorials/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n0rYnRb6YvWJ"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade -q openai langchain langchain-openai langchain-community langgraph langgraph-checkpoint-sqlite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRWSOLz6c7LY"
   },
   "source": [
    "## Building a ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kAZbx7cnZBuE"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DtjWt8EdlAM",
    "outputId": "960636a7-6b4e-4de7-e741-8b2a79341b67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your OPENAI_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OPENAI_API_KEY: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFnyXWiod3PI"
   },
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "  messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.5)\n",
    "\n",
    "# defining the chatbot node\n",
    "def chatbot(state: State) -> State:\n",
    "  return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "# adding the node to the graph\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# setting the entry and the finish points\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "\n",
    "# building the graph\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0yAfxmMgnYM"
   },
   "source": [
    "The code snippet defines a system for constructing and using a stateful graph of operations, leveraging components like `State`, a state machine graph builder, and a chatbot function. Below is a breakdown of each part:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. State Definition**\n",
    "\n",
    "```python\n",
    "class State(TypedDict):\n",
    "  messages: Annotated[list, add_messages]\n",
    "```\n",
    "\n",
    "- `State` is a type definition using Python's `TypedDict` from the `typing` module, which allows you to define dictionary-like structures with specific key-value types.\n",
    "- `messages`:\n",
    "  - Its type is `list`, constrained or further described by an `Annotated` type.\n",
    "  - The `Annotated` type allows adding metadata (like `add_messages`) to the field. While the specific functionality of `add_messages` isn't detailed here, it could be a validator, schema descriptor, or a helper function.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Graph Builder Initialization**\n",
    "\n",
    "```python\n",
    "graph_builder = StateGraph(State)\n",
    "```\n",
    "\n",
    "- `StateGraph` appears to be a utility class (not part of standard Python) for creating and managing a directed graph of states or operations.\n",
    "- The graph operates on data structures conforming to the `State` type, ensuring consistency throughout the graph nodes.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Language Model Setup**\n",
    "\n",
    "```python\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "```\n",
    "\n",
    "- `ChatOpenAI` is initialized, representing a large language model interface (likely OpenAI’s API).\n",
    "- Parameters:\n",
    "  - `model_name=\"gpt-4o-mini\"` specifies the model variant. `gpt-4o-mini` suggests a lightweight or specialized version of GPT-4o.\n",
    "  - `temperature=0` ensures deterministic outputs, meaning the same input will always produce the same response.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Chatbot Node Definition**\n",
    "\n",
    "```python\n",
    "def chatbot(state: State) -> State:\n",
    "  return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "```\n",
    "\n",
    "- This defines a function (`chatbot`) to be used as a node in the graph.\n",
    "- Inputs:\n",
    "  - `state`: A dictionary conforming to the `State` schema. Specifically, it will contain a `messages` key.\n",
    "- Outputs:\n",
    "  - A dictionary with a single key `messages`. The value is the result of invoking the `llm` object with the provided `state[\"messages\"]`.\n",
    "  - The `llm.invoke()` method is presumably used to pass the `state[\"messages\"]` list to the language model and receive its response.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Adding the Chatbot Node to the Graph**\n",
    "\n",
    "```python\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "```\n",
    "\n",
    "- The `chatbot` function is added as a node in the `graph_builder`, with the label `\"chatbot\"`.\n",
    "- This means the `chatbot` node will execute its logic whenever invoked by the graph.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Entry and Finish Points**\n",
    "\n",
    "```python\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "```\n",
    "\n",
    "- The `entry point` marks the node where the graph begins processing. Here, it’s set to the `\"chatbot\"` node.\n",
    "- The `finish point` marks the final node to complete graph execution, also set to the `\"chatbot\"` node in this case.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Compiling the Graph**\n",
    "\n",
    "```python\n",
    "graph = graph_builder.compile()\n",
    "```\n",
    "\n",
    "- The graph is finalized (compiled) for execution. This step prepares the defined nodes and logic into a runnable pipeline or workflow.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Concepts**\n",
    "\n",
    "1. **State Management:**\n",
    "   - The `State` object holds the inputs and outputs for graph operations. Each node (like `chatbot`) modifies or augments this state.\n",
    "\n",
    "2. **Graph-Based Workflow:**\n",
    "   - Using `StateGraph`, the operations (`chatbot` in this case) are structured as nodes in a graph, facilitating modular and reusable workflows.\n",
    "\n",
    "3. **Language Model Integration:**\n",
    "   - The chatbot node integrates a language model (`llm`) to process and respond to inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Execution**\n",
    "Suppose `state` is initialized as:\n",
    "\n",
    "```python\n",
    "state = {\"messages\": [\"Hello, how can I help you?\"]}\n",
    "```\n",
    "\n",
    "When the graph executes:\n",
    "1. The `state[\"messages\"]` list is passed to the `chatbot` node.\n",
    "2. The `llm.invoke()` method processes the messages and generates a response.\n",
    "3. The updated state is output as:\n",
    "\n",
    "```python\n",
    "{\"messages\": [\"<response from GPT-4o-mini>\"]}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Potential Use Case**\n",
    "This setup can be part of a larger system where:\n",
    "- The graph orchestrates complex workflows involving multiple nodes.\n",
    "- The chatbot node acts as a dynamic conversational agent integrated into the workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2osTg35fSo4"
   },
   "source": [
    "## Visualizing the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "id": "hqCYwDEffQbi",
    "outputId": "6ed8f1f7-df1f-4263-b114-4e3301fbf328"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LRzU7qQKfsr9",
    "outputId": "53490138-b1ce-4a5e-9c41-315e569ce3d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#@title Draw graph using ascii\n",
    "!pip install -q grandalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xlk-xcZrjsHu",
    "outputId": "54237b13-78c5-46ab-be89-7dbc08cb637f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+  \n",
      "| __start__ |  \n",
      "+-----------+  \n",
      "      *        \n",
      "      *        \n",
      "      *        \n",
      " +---------+   \n",
      " | chatbot |   \n",
      " +---------+   \n",
      "      *        \n",
      "      *        \n",
      "      *        \n",
      " +---------+   \n",
      " | __end__ |   \n",
      " +---------+   \n"
     ]
    }
   ],
   "source": [
    "print(graph.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAVdWE--j5q7"
   },
   "source": [
    "## Running the ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XhEQJhQDjyp7",
    "outputId": "1ca024df-53e1-4480-ea5a-465026f3f114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Paris is ...\n",
      "Assistant: Paris is the capital city of France, known for its rich history, art, culture, and architecture. It is often referred to as \"The City of Light\" (La Ville Lumière) and is famous for landmarks such as the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, and the Champs-Élysées. Paris is also renowned for its cuisine, fashion, and vibrant neighborhoods like Montmartre and Le Marais. The city has played a significant role in various movements, including the Enlightenment and the arts, making it a cultural hub in Europe and around the world.\n",
      "--------------------\n",
      "User: History of Notre-Dame\n",
      "Assistant: Notre-Dame de Paris, one of the most famous Gothic cathedrals in the world, has a rich and complex history that dates back to the early 12th century. Here’s an overview of its history:\n",
      "\n",
      "### Early History\n",
      "- **Foundation**: The site of Notre-Dame was originally occupied by a Gallo-Roman temple dedicated to Jupiter. In the early Christian period, a church was built on the site, which was later replaced by the current cathedral.\n",
      "- **Construction Begins**: The construction of Notre-Dame began in 1163 under Bishop Maurice de Sully. The cathedral was built in stages, with various architects contributing to its design.\n",
      "\n",
      "### Gothic Architecture\n",
      "- **Completion**: The cathedral was largely completed by 1345, although some modifications and additions continued for centuries. It is a prime example of French Gothic architecture, characterized by its flying buttresses, ribbed vaults, and pointed arches.\n",
      "- **Sculptural Decorations**: The façade features numerous sculptures, including gargoyles and biblical figures, which reflect the artistic styles of the period.\n",
      "\n",
      "### Historical Events\n",
      "- **Coronation of Napoleon**: One of the most significant events in Notre-Dame's history was the coronation of Napoleon Bonaparte as Emperor of the French in 1804, which took place in the cathedral.\n",
      "- **French Revolution**: During the French Revolution (1789-1799), Notre-Dame was desecrated; many religious symbols were destroyed, and it was repurposed as a warehouse. The cathedral suffered significant damage during this time.\n",
      "\n",
      "### Restoration and Renovation\n",
      "- **19th Century Restoration**: A major restoration project was undertaken in the 19th century, led by architect Eugène Viollet-le-Duc. This restoration included the addition of the iconic spire, which was completed in 1860.\n",
      "- **Cultural Significance**: Notre-Dame has been a symbol of French culture and identity, inspiring literature, art, and music, including Victor Hugo's novel \"The Hunchback of Notre-Dame,\" published in 1831.\n",
      "\n",
      "### Modern Era\n",
      "- **World War II**: During World War II, the cathedral survived the war relatively unscathed, although it was closed to the public during the German occupation of France.\n",
      "- **Recent Events**: In April 2019, a devastating fire broke out at Notre-Dame, leading to the collapse of the spire and significant damage to the roof and interior. The fire prompted an international outpouring of support and funding for restoration efforts.\n",
      "\n",
      "### Restoration Efforts\n",
      "- **Reconstruction**: Following the fire, extensive restoration efforts began, with a goal to restore the cathedral to its former glory. The French government set a target for reopening the cathedral to the public by 2024, in time for the Paris Olympics.\n",
      "\n",
      "Notre-Dame de Paris remains a significant religious, cultural, and historical landmark, representing not only the architectural achievements of the Gothic period but also the resilience of the French people through centuries of change and challenge.\n",
      "--------------------\n",
      "User: Arsenal is ..\n",
      "Assistant: Arsenal Football Club is a professional football club based in Islington, London, England. Founded in 1886, it is one of the oldest and most successful football clubs in English football history. Arsenal plays its home matches at the Emirates Stadium, which has a capacity of over 60,000 spectators.\n",
      "\n",
      "The club has a rich history, having won numerous domestic and international titles, including multiple Premier League championships and FA Cups. Arsenal is known for its distinctive red and white kit and has a passionate fan base. The club has also been recognized for its attractive style of play, particularly during the era under former manager Arsène Wenger, who led the team from 1996 to 2018.\n",
      "\n",
      "Arsenal's rivalry with clubs like Tottenham Hotspur (the North London Derby) and Manchester United are among the most notable in English football. The club's motto is \"Victoria Concordia Crescit,\" which translates to \"Victory Through Harmony.\"\n",
      "--------------------\n",
      "User: quit\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input('User: ')\n",
    "    if user_input.lower() in ['quit', 'exit', 'bye',  'q']:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "\n",
    "    for event in graph.stream({'messages': ('user', user_input)}):\n",
    "        for value in event.values():\n",
    "            print(f'Assistant: {value[\"messages\"][-1].content}')\n",
    "            print('-' * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8nKxxmoksay"
   },
   "source": [
    "## Tavily AI\n",
    "\n",
    "- https://tavily.com/\n",
    "- https://docs.tavily.com/docs/python-sdk/tavily-search/getting-started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jxTZGNXtkNCn"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4rDzQICk0-D",
    "outputId": "3be8411e-5cf0-4739-a437-4e33ec7e30e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your TAVILY_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your TAVILY_API_KEY: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "sY1SRcJ2lxpL",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "9e48f598-861b-4d12-dd8d-6fe89b835471"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'EUFA EURO 2024 FINA',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'title': 'UEFA Euro 2024 final - Wikipedia',\n",
       "   'url': 'https://en.wikipedia.org/wiki/UEFA_Euro_2024_Final',\n",
       "   'content': \"The UEFA Euro 2024 final was a football match that determined the winners of UEFA Euro 2024.The match was the seventeenth final of the European Championship, a quadrennial tournament contested by the men's national teams of the member associations of UEFA to decide the champions of Europe. The match was held at the Olympiastadion in Berlin, Germany, on 14 July 2024, and was contested by Spain\",\n",
       "   'score': 0.87779135,\n",
       "   'raw_content': None},\n",
       "  {'title': 'EURO 2024 - UEFA.com',\n",
       "   'url': 'https://www.uefa.com/euro2024/',\n",
       "   'content': \"Group A Live now 07:55  Live 17/07/2024 Highlights: Spain 2-1 England ----------------------------- 05:53  Live 17/07/2024 Highlights: Spain 2-1 France ---------------------------- 05:57  Live 17/07/2024 Highlights: Netherlands 1-2 England ----------------------------------- 05:59  Live 17/07/2024 Highlights: England 1-1 Switzerland (aet) ----------------------------------------- 05:57  Live 17/07/2024 Highlights: Netherlands 2-1 Türkiye ----------------------------------- 06:00  Live 17/07/2024 Highlights: England 2-1 Slovakia (aet) -------------------------------------- 06:01  Live 17/07/2024 Highlights: Türkiye 3-1 Georgia ------------------------------- 06:06  Live 17/07/2024 Highlights: France 1-1 Poland ----------------------------- 05:55  Live 17/07/2024 Highlights: Slovenia 1-1 Denmark -------------------------------- 05:54  Live 17/07/2024 Highlights: Belgium 0-1 Slovakia -------------------------------- 06:00  Live 17/07/2024 Highlights: Poland 1-2 Netherlands ---------------------------------- 01:51  Live 15/07/2024 EURO 2024 top scorers --------------------- 01:06  Live 18/07/2024 EURO 2024 through netcam ------------------------ 02:15  Live 17/07/2024 England's EURO 2024 goals -------------------------\",\n",
       "   'score': 0.7545535,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Euro 2024 final: Spain tops England to win dramatic, historic 4th title',\n",
       "   'url': 'https://sports.yahoo.com/live/euro-2024-final-spain-tops-england-to-win-dramatic-historic-4th-title-180013299.html',\n",
       "   'content': \"Spain, the best team of Euro 2024 by a relatively wide margin, beat England in Sunday's final on Mikel Oyarzabal's dramatic 86th-minute winner.\",\n",
       "   'score': 0.6940954,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Spain wins Euro 2024, defeating England 2-1 in a dramatic final to ...',\n",
       "   'url': 'https://www.cnn.com/2024/07/14/sport/spain-england-euro-2024-final-spt-intl/index.html',\n",
       "   'content': \"Spain won a record-breaking fourth European Championship, defeating England 2-1 following a drama-filled second half in the Euro 2024 final on Sunday in Berlin.. Nico Williams - one of Spain's\",\n",
       "   'score': 0.50203323,\n",
       "   'raw_content': None},\n",
       "  {'title': 'EURO 2024 final: Who was in it? When and where was it?',\n",
       "   'url': 'https://www.uefa.com/euro2024/news/0284-18bb952a9458-2a9e1ff202c4-1000--euro-2024-final-who-was-in-it-when-and-where-was-it/',\n",
       "   'content': 'The UEFA EURO 2024 final was played on Sunday 14 July, kicking off at 21:00 CET. The final: all the reaction. The match took place at Olympiastadion Berlin, the biggest stadium at the tournament',\n",
       "   'score': 0.4932827,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 1.54}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tavily import TavilyClient\n",
    "\n",
    "# initializing a Tavily client\n",
    "client = TavilyClient()\n",
    "response = client.search(query=\"EUFA EURO 2024 FINA\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cSBGC7pmGnn",
    "outputId": "e3dd45f6-a9b8-41fa-c3f9-854fe792e51f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: UEFA Euro 2024 final - Wikipedia, URL: https://en.wikipedia.org/wiki/UEFA_Euro_2024_Final\n",
      "Title: EURO 2024 - UEFA.com, URL: https://www.uefa.com/euro2024/\n",
      "Title: Euro 2024 final: Spain tops England to win dramatic, historic 4th title, URL: https://sports.yahoo.com/live/euro-2024-final-spain-tops-england-to-win-dramatic-historic-4th-title-180013299.html\n",
      "Title: Spain wins Euro 2024, defeating England 2-1 in a dramatic final to ..., URL: https://www.cnn.com/2024/07/14/sport/spain-england-euro-2024-final-spt-intl/index.html\n",
      "Title: EURO 2024 final: Who was in it? When and where was it?, URL: https://www.uefa.com/euro2024/news/0284-18bb952a9458-2a9e1ff202c4-1000--euro-2024-final-who-was-in-it-when-and-where-was-it/\n"
     ]
    }
   ],
   "source": [
    "for result in response['results']:\n",
    "    print(f\"Title: {result['title']}, URL: {result['url']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "nU21Kk_gmneN",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "91f87c4b-938f-4907-ac67-07fcbf7094a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What are LLM agents?',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': 'LLM agents, or Large Language Model agents, are entities that are based on LLMs and are capable of manipulating their environment in some way. They typically consist of components such as a core for processing, memory, planning skills, and tool use. LLM agents have the ability to handle a wide range of tasks and interactions, making them valuable in various applications where language models can not only provide information but also perform actions autonomously.',\n",
       " 'images': ['https://sp-ao.shortpixel.ai/client/to_webp,q_glossy,ret_img,w_1080,h_1080/https://contenteratechspace.com/wp-content/uploads/2024/07/llm-agents.png',\n",
       "  'https://promptengineering.org/content/images/2023/08/Prompt-engineering---Large-Language-Model-LLM--Autonomous-Agent-Structure---PromptEngineering.org.jpg',\n",
       "  'https://promptengineering.org/content/images/2023/07/Prompt-engineering---Large-Language-Model-LLM--Autonomous-Agent.jpg',\n",
       "  'https://floatbot.ai/images/agentm/agent-m-architecture.png?v=1',\n",
       "  'https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/agent-components.png'],\n",
       " 'results': [{'title': 'What is LLM Agents - Iguazio',\n",
       "   'url': 'https://www.iguazio.com/glossary/llm-agents/',\n",
       "   'content': 'What is the Structure of LLM Agents? The LLM Agent is made up of four components: Each of these components contributes to the LLM Agent\\'s ability to handle a wide range of tasks and interactions. The Core - This is the fundamental part of an LLM Agent, acting as the central processing unit, i.e the \"brian\". The core manages the overall',\n",
       "   'score': 0.999015,\n",
       "   'raw_content': None},\n",
       "  {'title': 'LLM Agents: When Large Language Models Do Stuff For You',\n",
       "   'url': 'https://deepgram.com/learn/llm-agents-when-language-models-do-stuff-for-you',\n",
       "   'content': \"Camel uses the following three agents:\\na “user agent” that gives instructions for an objective\\nan “assistant agent” that generates solutions for the user agent\\na “task specifier” agent that causes tasks for the user and assistant agents to complete\\nA human defines some idea (e.g., making a trading bot) and designates roles for the user and assistant agents (e.g., a Python programmer and a stock trader).\\n And if the term variability isn’t confusing enough, current definitions are also in flux but it’s tough to do better than OpenAI AI safety researcher Lilian Weng's definition of LLM agents as:\\n“Agent = LLM + memory + planning skills + tool use”\\nLilian Weng\\nWhile there’s not yet a strong consensus on terms or definitions, for this article, at least, let’s sidestep this minefield of nuance and, borrowing from Weng, roughly define an LLM agent as an LLM-based entity that can manipulate its environment in some way (e.g., a computer, web browser, other agents, etc.) So much so that Andrej Karpathy—former director of Artificial Intelligence (AI) at Tesla and now back with OpenAI, where he started—kicked off a recent LLM agent-focused hackathon at the AGI House with an intriguing forecast of agents’ potential, reckoning that Artificial General Intelligence (AGI) will likely involve some flavor of agent framework, possibly even “organizations or civilizations of digital entities.”\\nBold, perhaps, but Karpathy’s vision of LLM agents’ promise isn’t precisely an outlier; sharing similar sentiments, Silicon Valley and venture capital firms are throwing gobs of resources at LLM agents.\\n (an example of Camel’s role-playing framework)\\nYou can see what this looks like in the below demo of a computational chemist agent and a poor Ph.D. student interacting to generate a molecular dynamic simulation:\\nWhile Camel’s role-playing concept is interesting, their study didn’t hook up their agents to tools or the internet (Camel only spells out the steps one would take), which would make them more useful.\\n But this question-reply cycle can grow tedious, and not everyone has the desire or the time to learn meticulous prompt engineering techniques, which is why some folks are trying to get large language models (LLMs) to handle most of this grunt work for them via (somewhat) set-it-and-forget-it autonomous “agents” that not only return the information we want but can also actually do things too.\\n\",\n",
       "   'score': 0.9986223,\n",
       "   'raw_content': None},\n",
       "  {'title': 'What is an LLM Agent and how does it work? - Medium',\n",
       "   'url': 'https://medium.com/@aydinKerem/what-is-an-llm-agent-and-how-does-it-work-1d4d9e4381ca',\n",
       "   'content': 'Large Language Model[2] The core computational engine of an LLM agent is a large language model. LLM is trained on a massive dataset to understand and reason from text data.',\n",
       "   'score': 0.9971439,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Introduction to LLM Agents | NVIDIA Technical Blog',\n",
       "   'url': 'https://developer.nvidia.com/blog/introduction-to-llm-agents',\n",
       "   'content': 'Demystifying Retrieval-Augmented Generation Pipelines\\nBuild Enterprise Retrieval-Augmented Generation Apps with NVIDIA Retrieval QA Embedding Model\\nTraining a Text2Sparql Model with MK-SQuIT and NeMo\\nRelated posts\\nDevelop Custom Enterprise Generative AI with NVIDIA NeMo\\nStreamline Evaluation of LLMs for Accuracy with NVIDIA NeMo Evaluator\\nNVIDIA H200 Tensor Core GPUs and NVIDIA TensorRT-LLM Set MLPerf LLM Inference Records\\nAn Easy Introduction to Multimodal Retrieval Augmented Generation\\nHow to Take a RAG Application from Pilot to Production in Four Steps While there isn’t a widely accepted definition for LLM-powered agents, they can be described as a system that can use an LLM to reason through a problem, create a plan to solve the problem, and execute the plan with the help of a set of tools.\\n To answer this question, you essentially must answer three questions individually (i.e., we need a planning module):\\nIn this case, you would need an agent that has access to a \\xa0Planning Module that does question-decomposition (generates sub-questions and searches for answers till the larger problem is solved), a RAG pipeline (used as a tool) to retrieve specific information, and memory modules to accurately handle the subquestions. Agents for enterprise applications\\nWhile the applications of agents are practically boundless, the following are a few interesting cases that may have an outsized impact for many businesses:\\n“Talk to your data” agent\\n“Talk to your data” isn’t a simple problem. For instance, agents can use a RAG pipeline to generate context aware answers, a code interpreter to solve complex programmatically tasks, an API to search information over the internet, or even any simple API service like a weather API or an API for an Instant messaging application.\\n',\n",
       "   'score': 0.9960699,\n",
       "   'raw_content': None},\n",
       "  {'title': 'How to Build a General-Purpose LLM Agent',\n",
       "   'url': 'https://towardsdatascience.com/build-a-general-purpose-ai-agent-c40be49e7400',\n",
       "   'content': 'Single Agent Architecture. (Image by author) The main difference between a simple LLM and an agent comes down to the system prompt.. The system prompt, in the context of an LLM, is a set of instructions and contextual information provided to the model before it engages with user queries.. The agentic behavior expected of the LLM can be codified within the system prompt.',\n",
       "   'score': 0.9951703,\n",
       "   'raw_content': None},\n",
       "  {'title': 'LLM Agents | Prompt Engineering Guide',\n",
       "   'url': 'https://www.promptingguide.ai/research/llm-agents',\n",
       "   'content': \"These are the possible high-level components of the hypothetical LLM agent but there are still important considerations such as creating a plan to address the task and potential access to a memory module that helps the agent keep track of the state of the flow of operations, observations, and overall progress.\\n The figure below shows an example of ReAct and the different steps involved in performing question answering:\\nLearn more about ReAct here:\\nMemory\\nThe memory module helps to store the agent's internal logs including past thoughts, actions, and observations from the environment, including all interactions between agent and user. To better motivate the usefulness of an LLM agent, let's say that we were interested in building a system that can help answer the following question:\\nWhat's the average daily calorie intake for 2023 in the United States?\\n LLM Agent Framework\\nGenerally speaking, an LLM agent framework can consist of the following core components:\\nAgent\\nA large language model (LLM) with general-purpose capabilities serves as the main brain, agent module, or coordinator of the system. Now let's give the system a more complex question like the following:\\nHow has the trend in the average daily calorie intake among adults changed over the last decade in the United States, and what impact might this have on obesity rates?\",\n",
       "   'score': 0.994089,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Intro to LLM Agents with Langchain: When RAG is Not Enough',\n",
       "   'url': 'https://towardsdatascience.com/intro-to-llm-agents-with-langchain-when-rag-is-not-enough-7d8c08145834',\n",
       "   'content': 'Good luck with your AI projects and don’t hesitate to reach out if you need help at your company!\\n--\\n--\\nWritten by Alex Honchar\\nTowards Data Science\\nCo-founder @ Neurons Lab\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams Notice, how we can easily decompose and define separately:\\nThe final definition of the agent will look as simple as this:\\nAs you can see in the outputs of the script (or you can run it yourself), it solves the issue in the previous part related to tools. For example, while building the tree of thoughts prompts, I save my sub-prompts in the prompts repository and load them:\\nYou can see in this notebook the result of such reasoning, the point I want to make here is the right process for defining your reasoning steps and versioning them in such an LLMOps system like Langsmith. If you prefer a narrative walkthrough, you can find the YouTube video here:\\nAs always, you can find the code on GitHub, and here are separate Colab Notebooks:\\nIntroduction to the agents\\nLet’s begin the lecture by exploring various examples of LLM agents. Also, you can see other examples of popular reasoning techniques in public repositories like ReAct or Self-ask with search:\\nOther notable approaches are:\\nStep 2: Memory\\nStep 3: Tools\\nChatGPT Plugins and OpenAI API function calling are good examples of LLMs augmented with tool use capability working in practice.\\n',\n",
       "   'score': 0.9777139,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 2.64}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.search(\n",
    "    query='What are LLM agents?',\n",
    "    search_depth='advanced',\n",
    "    max_results=7,\n",
    "    include_images=True,\n",
    "    include_answer=True,\n",
    "    include_raw_content=False\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_N0sU7r7nKhv",
    "outputId": "1d2f16d9-a48c-406a-b9b0-7bdb51860cbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Madrid won the UEFA Champions League in 2024 by defeating Borussia Dortmund 2-0 in the final held at Wembley Stadium on June 1st.\n"
     ]
    }
   ],
   "source": [
    "answer = client.qna_search(query='Who won the UEFA Champions League in 2024?')\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jhwhkllUoAWR",
    "outputId": "b104bc60-f783-4302-af72-5ab313f0dc4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'EOF',\n",
       "  'url': 'http://www.google.com/search?hl=en&q=what+is+the+\"reflection+&+critique\"+pattern+used+in+agentic+applications+and+langgraph?',\n",
       "  'content': \"https://www.google.com/search?hl=en&q=what+is+the+%22reflection+&+critique%22+pattern+used+in+agentic+applications+and+langgraph? Our systems have detected unusual traffic from your computer network. This page checks to see if it's really you sending the requests, and not a robot. This page appears when Google automatically detects requests coming from your computer network which appear to be in violation of the Terms of Service. This traffic may have been sent by malicious software, a browser plug-in, or a script that sends automated requests. If you share your network connection, ask your administrator for help — a different computer using the same IP address may be responsible. Sometimes you may be asked to solve the CAPTCHA if you are using advanced terms that robots are known to use, or sending requests very quickly. URL: https://www.google.com/search?hl=en&q=what+is+the+%22reflection+&+critique%22+pattern+used+in+agentic+applications+and+langgraph?\",\n",
       "  'score': 0.99946636,\n",
       "  'raw_content': None},\n",
       " {'title': 'Reflection - GitHub Pages',\n",
       "  'url': 'https://langchain-ai.github.io/langgraph/tutorials/reflection/reflection/',\n",
       "  'content': 'Agentic RAG Corrective RAG (CRAG) Corrective RAG (CRAG) using local LLMs ... Reflection & Critique Reflection & Critique ... Offer practical applications and strategies for children to apply the lessons from The Little Prince in their daily lives. 4. Consider the age range and reading level of the intended audience and adjust the language and',\n",
       "  'score': 0.9976404,\n",
       "  'raw_content': None}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.adapters.openai import convert_openai_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "query = 'What is the \"Reflection & Critique\" pattern used in agentic applications and LangGraph?'\n",
    "\n",
    "response = client.search(query, max_results=5, search_depth='advanced')['results']\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hsZRyuYkogPr",
    "outputId": "12ee9c5e-b9d9-4d24-ca8c-6d646fcebaf8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are an AI critical thinker research assistant. \\n        Your sole purpose is to write well written, objective and structured reports on given text.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Information: \"\"\"[{\\'title\\': \\'EOF\\', \\'url\\': \\'http://www.google.com/search?hl=en&q=what+is+the+\"reflection+&+critique\"+pattern+used+in+agentic+applications+and+langgraph?\\', \\'content\\': \"https://www.google.com/search?hl=en&q=what+is+the+%22reflection+&+critique%22+pattern+used+in+agentic+applications+and+langgraph? Our systems have detected unusual traffic from your computer network. This page checks to see if it\\'s really you sending the requests, and not a robot. This page appears when Google automatically detects requests coming from your computer network which appear to be in violation of the Terms of Service. This traffic may have been sent by malicious software, a browser plug-in, or a script that sends automated requests. If you share your network connection, ask your administrator for help — a different computer using the same IP address may be responsible. Sometimes you may be asked to solve the CAPTCHA if you are using advanced terms that robots are known to use, or sending requests very quickly. URL: https://www.google.com/search?hl=en&q=what+is+the+%22reflection+&+critique%22+pattern+used+in+agentic+applications+and+langgraph?\", \\'score\\': 0.99946636, \\'raw_content\\': None}, {\\'title\\': \\'Reflection - GitHub Pages\\', \\'url\\': \\'https://langchain-ai.github.io/langgraph/tutorials/reflection/reflection/\\', \\'content\\': \\'Agentic RAG Corrective RAG (CRAG) Corrective RAG (CRAG) using local LLMs ... Reflection & Critique Reflection & Critique ... Offer practical applications and strategies for children to apply the lessons from The Little Prince in their daily lives. 4. Consider the age range and reading level of the intended audience and adjust the language and\\', \\'score\\': 0.9976404, \\'raw_content\\': None}]\"\"\"\\n        Using the above information, answer the following query: \"\"\"What is the \"Reflection & Critique\" pattern used in agentic applications and LangGraph?\"\"\" in a detailed report', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting up the OpenAI API prompt\n",
    "prompt = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': f'''You are an AI critical thinker research assistant.\n",
    "        Your sole purpose is to write well written, objective and structured reports on given text.'''\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': f'''Information: \"\"\"{response}\"\"\"\n",
    "        Using the above information, answer the following query: \"\"\"{query}\"\"\" in a detailed report'''\n",
    "    }\n",
    "]\n",
    "lc_messages = convert_openai_messages(prompt)\n",
    "lc_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "3nP9MhaOpJA6",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "6c8bb676-7408-4799-a18b-8b5ea1994e57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Report on the \"Reflection & Critique\" Pattern in Agentic Applications and LangGraph\n",
      "\n",
      "## Introduction\n",
      "The \"Reflection & Critique\" pattern is an important concept used in various agentic applications, particularly within the LangGraph framework. This report aims to elucidate the purpose, applications, and implications of this pattern based on the provided information.\n",
      "\n",
      "## Definition and Purpose\n",
      "The \"Reflection & Critique\" pattern involves a systematic approach for evaluation and improvement within learning and interactive environments. It encourages users—whether they are students, educators, or developers—to actively reflect on their experiences and critically assess their understanding and the applications they are engaged with. This process not only reinforces learning but also fosters deeper cognitive engagement and self-awareness.\n",
      "\n",
      "## Context of Use\n",
      "### Agentic Applications\n",
      "Agentic applications are designed to empower users by granting them agency in their learning processes. These applications often utilize AI and interactive technologies to create personalized learning experiences. The \"Reflection & Critique\" pattern is integral to these applications as it allows users to engage in metacognitive strategies, enabling them to assess their learning outcomes and refine their approaches.\n",
      "\n",
      "### LangGraph Framework\n",
      "LangGraph, which is a part of the LangChain ecosystem, aims to provide tools and methodologies for developing language-based applications. Within this framework, the \"Reflection & Critique\" component is applied to help users, particularly children, make connections between theoretical lessons—such as those derived from literature (e.g., \"The Little Prince\")—and their practical applications in real life. This encourages a holistic understanding and fosters the ability to apply learned concepts in varied contexts.\n",
      "\n",
      "## Practical Applications\n",
      "The practical applications of the \"Reflection & Critique\" pattern can be seen in several areas, including:\n",
      "\n",
      "1. **Educational Settings**: Educators can use this pattern to guide students in reflecting on their learning experiences, thus enhancing retention and comprehension.\n",
      "2. **Development of AI Systems**: Developers can implement this pattern to evaluate the performance of AI models, allowing for continuous improvement based on user feedback.\n",
      "3. **Personal Development**: Individuals can use reflective practices to assess their personal growth and learning, which is particularly beneficial in self-directed learning environments.\n",
      "\n",
      "## Implementation Strategies\n",
      "- **Structured Reflection Prompts**: Incorporating specific questions or prompts that encourage users to think critically about their experiences.\n",
      "- **Feedback Mechanisms**: Providing channels for users to offer feedback on their interactions, which can be analyzed for insights into improving both the applications and user experience.\n",
      "- **Workshops and Training**: Facilitating sessions aimed at teaching users how to effectively engage in reflection and critique within their learning processes.\n",
      "\n",
      "## Conclusion\n",
      "The \"Reflection & Critique\" pattern is a vital component of agentic applications and the LangGraph framework. By fostering an environment of reflection and critical thinking, this pattern enhances learning outcomes and empowers users to take charge of their educational journeys. The integration of this pattern into various applications not only helps in personal growth but also in the continual advancement of AI and interactive technologies. \n",
      "\n",
      "For further exploration of the \"Reflection & Critique\" pattern and its applications, resources such as LangGraph's official documentation can be consulted.\n"
     ]
    }
   ],
   "source": [
    "response = ChatOpenAI(model='gpt-4o-mini').invoke(lc_messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 976
    },
    "id": "f51ngrYFpiDI",
    "outputId": "1cd98516-134a-4c36-fc27-ac3b9abc6a34"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Report on the \"Reflection & Critique\" Pattern in Agentic Applications and LangGraph\n",
       "\n",
       "## Introduction\n",
       "The \"Reflection & Critique\" pattern is an important concept used in various agentic applications, particularly within the LangGraph framework. This report aims to elucidate the purpose, applications, and implications of this pattern based on the provided information.\n",
       "\n",
       "## Definition and Purpose\n",
       "The \"Reflection & Critique\" pattern involves a systematic approach for evaluation and improvement within learning and interactive environments. It encourages users—whether they are students, educators, or developers—to actively reflect on their experiences and critically assess their understanding and the applications they are engaged with. This process not only reinforces learning but also fosters deeper cognitive engagement and self-awareness.\n",
       "\n",
       "## Context of Use\n",
       "### Agentic Applications\n",
       "Agentic applications are designed to empower users by granting them agency in their learning processes. These applications often utilize AI and interactive technologies to create personalized learning experiences. The \"Reflection & Critique\" pattern is integral to these applications as it allows users to engage in metacognitive strategies, enabling them to assess their learning outcomes and refine their approaches.\n",
       "\n",
       "### LangGraph Framework\n",
       "LangGraph, which is a part of the LangChain ecosystem, aims to provide tools and methodologies for developing language-based applications. Within this framework, the \"Reflection & Critique\" component is applied to help users, particularly children, make connections between theoretical lessons—such as those derived from literature (e.g., \"The Little Prince\")—and their practical applications in real life. This encourages a holistic understanding and fosters the ability to apply learned concepts in varied contexts.\n",
       "\n",
       "## Practical Applications\n",
       "The practical applications of the \"Reflection & Critique\" pattern can be seen in several areas, including:\n",
       "\n",
       "1. **Educational Settings**: Educators can use this pattern to guide students in reflecting on their learning experiences, thus enhancing retention and comprehension.\n",
       "2. **Development of AI Systems**: Developers can implement this pattern to evaluate the performance of AI models, allowing for continuous improvement based on user feedback.\n",
       "3. **Personal Development**: Individuals can use reflective practices to assess their personal growth and learning, which is particularly beneficial in self-directed learning environments.\n",
       "\n",
       "## Implementation Strategies\n",
       "- **Structured Reflection Prompts**: Incorporating specific questions or prompts that encourage users to think critically about their experiences.\n",
       "- **Feedback Mechanisms**: Providing channels for users to offer feedback on their interactions, which can be analyzed for insights into improving both the applications and user experience.\n",
       "- **Workshops and Training**: Facilitating sessions aimed at teaching users how to effectively engage in reflection and critique within their learning processes.\n",
       "\n",
       "## Conclusion\n",
       "The \"Reflection & Critique\" pattern is a vital component of agentic applications and the LangGraph framework. By fostering an environment of reflection and critical thinking, this pattern enhances learning outcomes and empowers users to take charge of their educational journeys. The integration of this pattern into various applications not only helps in personal growth but also in the continual advancement of AI and interactive technologies. \n",
       "\n",
       "For further exploration of the \"Reflection & Critique\" pattern and its applications, resources such as LangGraph's official documentation can be consulted."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4-mcRZPsbI6"
   },
   "source": [
    "## Enhancing the ChatBot with Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Ym1mLzLop1nk"
   },
   "outputs": [],
   "source": [
    "#@title Defining the tools\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tool = TavilySearchResults(max_results=5)\n",
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "2DyEr4BItkAf",
    "outputId": "2d18a5ed-6f92-4643-d002-10739bf99cc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.geeksforgeeks.org/ml-machine-learning/',\n",
       "  'content': 'Machine learning is a branch of artificial intelligence that enables algorithms to learn from data and make predictions without explicit programming. Learn about the difference between machine learning and traditional programming, the types of machine learning, the applications of machine learning, and the limitations of machine learning.'},\n",
       " {'url': 'https://www.ibm.com/topics/machine-learning',\n",
       "  'content': 'Machine learning is a branch of\\xa0artificial intelligence (AI)\\xa0and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy.\\n The system\\xa0used reinforcement learning\\xa0to learn when to attempt an answer (or question, as it were), which square to select on the board, and how much to wager—especially on daily doubles.\\n Download \"Machine learning for Dummies\"\\nExplore Gen AI for developers\\nUC\\xa0Berkeley\\xa0(link resides outside ibm.com)\\xa0breaks out the learning system of a machine learning algorithm into three main parts.\\n IBM CEO Arvind Krishna wrote: “IBM firmly opposes and will not condone uses of any technology, including facial recognition technology offered by other vendors, for mass surveillance, racial profiling, violations of basic human rights and freedoms, or any purpose which is not consistent with our values and\\xa0Principles of Trust and Transparency.”\\n A neural network that consists of more than three layers—which would be inclusive of the input and the output—can be considered a deep learning algorithm or a deep neural network.'},\n",
       " {'url': 'https://www.coursera.org/articles/what-is-machine-learning',\n",
       "  'content': 'In IBM’s Machine Learning Professional Certificate, you’ll master the most up-to-date practical skills and knowledge machine learning experts use in their daily roles, including how to use supervised and unsupervised learning to build models for a wide range of real-world purposes.\\n For example, a machine learning algorithm may be “trained” on a data set consisting of thousands of images of flowers that are labeled with each of their different flower types so that it can then correctly identify a flower in a new photograph based on the differentiating characteristics it learned from other pictures.\\n At a glance, here are some of the major benefits and potential drawbacks of machine learning:\\nLearn more with Coursera\\nAI and machine learning are quickly changing how we live and work in the world today. Learners are advised to conduct additional research to ensure that courses and other credentials pursued meet their personal, professional, and financial goals.\\n$1 unlocks unlimited opportunities\\nCoursera Footer\\nPopular AI Content\\nPopular Programs\\nPopular Skills\\nPopular Career Resources\\nCoursera\\nCommunity\\nMore Machine learning definition\\nMachine learning is a subfield of artificial intelligence (AI) that uses algorithms trained on data sets to create self-learning models that are capable of predicting outcomes and classifying information without human intervention.'},\n",
       " {'url': 'https://www.geeksforgeeks.org/introduction-machine-learning/',\n",
       "  'content': 'Support Vector Machines\\nDecision Tree\\nEnsemble Learning\\nGenerative Model\\nTime Series Forecasting\\nClustering Algorithm\\nConvolutional Neural Networks\\nRecurrent Neural Networks\\nReinforcement Learning\\nModel Deployment and Productionization\\nAdvanced Topics\\nAn introduction to Machine Learning\\nArthur Samuel, an early American leader in the field of computer gaming and artificial intelligence, coined the term “Machine Learning ” in 1959 while at IBM. Support Vector Machines\\nDecision Tree\\nEnsemble Learning\\nGenerative Model\\nTime Series Forecasting\\nClustering Algorithm\\nConvolutional Neural Networks\\nRecurrent Neural Networks\\nReinforcement Learning\\nModel Deployment and Productionization\\nAdvanced Topics\\nGetting Started with Machine Learning\\nData Preprocessing\\nClassification & Regression\\nK-Nearest Neighbors (KNN)\\n Classification of Machine Learning\\nMachine learning implementations are classified into four major categories, depending on the nature of the learning “signal” or “response” available to a learning system which are as follows:\\nA. Supervised learning:\\nSupervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. Data Structures and Algorithms\\nML & Data Science\\nWeb Development\\nLanguages\\nInterview Corner\\nCS Subjects\\nJobs\\nPractice\\nContests\\nGetting Started with Machine Learning\\nData Preprocessing\\nClassification & Regression\\nK-Nearest Neighbors (KNN)\\n Definition of learning:\\nA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks T, as measured by P, improves with experience E.\\nExamples\\nDefinition: A computer program which learns from experience is called a machine learning program or simply a learning program .\\n'},\n",
       " {'url': 'https://en.wikipedia.org/wiki/Machine_learning',\n",
       "  'content': 'Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results.[94][95][96] Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.[97]\\nIn 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision.[98] Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested.[99][100]\\nMachine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples).[24]\\nGeneralization[edit]\\nThe difference between optimization and machine learning arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples. For example, the rule\\n{\\no\\nn\\ni\\no\\nn\\ns\\n,\\np\\no\\nt\\na\\nt\\no\\ne\\ns\\n}\\n⇒\\n{\\nb\\nu\\nr\\ng\\ne\\nr\\n}\\n{\\\\displaystyle \\\\{\\\\mathrm {onions,potatoes} \\\\}\\\\Rightarrow \\\\{\\\\mathrm {burger} \\\\}}\\nfound in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI.[134] OpenAI estimated the hardware computing used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.[135][136]\\nNeuromorphic/Physical Neural Networks[edit]\\nA physical neural network or Neuromorphic computer\\nis a type of artificial neural network in which an electrically adjustable material is used to emulate the function of a neural synapse. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access.[120][121][122]\\nModel assessments[edit]\\nClassification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = tool.invoke(\"What is Machine Learning?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dyuJl0Hntxwe"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# --- new import\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "tool = TavilySearchResults(max_results=5)\n",
    "tools = [tool]\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.5)\n",
    "\n",
    "# tell the LLM which tools it can call\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# change the chatbot() node function. Use llm_with_tools instead of llm.\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# --- add this:\n",
    "# run the tools if they are called by adding the tools to a new node.\n",
    "# This node runs the tools requested in the last AIMessage.\n",
    "tool_node = ToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "# ---\n",
    "\n",
    "# --- add this\n",
    "# define the conditional_edges.\n",
    "# we'll use the prebuilt tools_condition in the conditional_edge to route to the ToolNode if the last message has tool calls,\n",
    "# otherwise, route to the end.\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "# ---\n",
    "\n",
    "# any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "\n",
    "# we don't need to explicitly set a finish_point because our graph already has a way to finish!\n",
    "# graph_builder.set_finish_point(\"chatbot\")\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "s4DnnhUUvWG9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANYAAAD5CAIAAADUe1yaAAAAAXNSR0IArs4c6QAAIABJREFUeJzt3WlcE9feB/AzSchCFghhB9lkV9zApYqioqJULdi6W7XWXrVW2+va9trW3rbW2uql3ufea1ute92LinXBfaGiolIFlLIG2UMCCQnZM8+L+KEUAqLNzJkk5/vpi5plzh/9cWbmzJkzGI7jAEHgocEuAHF0KIIIZCiCCGQogghkKIIIZCiCCGQM2AW8CIVMr5DqWxRGVbPBoLONYSWGE0ZnYM58urOAIfJhsp3psCuiCsw2/gEBAABIqjQlv6nK8lVcAcNowJ0FdC6fweTQgC38BAwWpmw0tDQbWxQGldzIdaEH9+aG9ePxhE6wS4PMNiIol+p/PdlAd8KEnszgXlx3Pxbsiv6qqhJ1WZ5KVqt19WAOnSRiODnuEZENRPDWGWlhTvPQye6hfXmwa7G+3641/ZohHZ7q3nuoC+xa4KB6BI9+W9l7mCAyTgC7EGLdPidrlukTZ3rBLgQC6kYQx/HvPyidvMjXJ5gDuxYyFNxSlOerkhf4wC6EbNSN4P/WFM9dF8QV2OQ5+4t5fEeR96vitXf9YRdCKopG8Gha5bAUkU+QQ/R/bT3MkkurtSOnesIuhDxUPBHLPi2NGS5wwPwBAGKGuTjz6Y9uK2AXQh7KRbCxXlecq4yItfPzjy4MSBReOSKBXQV5KBfBXzOkQyeJYFcBE8OJFjtGeOuMFHYhJKFWBGvLNSwOLSTGDsf/nsugJLfaco1eZ4JdCBmoFcGSB0o3byZpzeXl5Wm1Wlhf7xqbSy/LUxG0cUqhVgTL8lXBvbjktJWRkTF//ny1Wg3l688U3JuLIki2xnqdwI0h9CKpF3zhDsw8jEVc/2cWEsOVS/WENkERFIqgvEGPYRgRWxaLxYsXL46Pj09OTt6wYYPJZMrIyNi4cSMAYMyYMXFxcRkZGQCA3Nzcd955Jz4+Pj4+ftGiRY8ePTJ/vampKS4ubu/evevWrYuPj3/rrbcsft26GE40ZZNBJTdYfctUQ6FrDy0Ko7OAkFl0n332WXl5+cqVK1UqVU5ODo1GGzZs2Jw5c/bt25eWlsbj8QICAgAA1dXVWq124cKFNBrtyJEjy5cvz8jIYLPZ5o3s2LFj6tSp27Zto9PpXl5eHb9udVwBQ6UwcF0o9G9EBAr9eCqFgaDLcdXV1ZGRkampqQCAOXPmAADc3Nz8/f0BAL1793Z1dTV/bMKECcnJyeb/j46OXrx4cW5u7pAhQ8yvxMTELF26tHWbHb9udVwXukpuBD0I2jxVUCiCAOAMFiE74uTk5F27dm3atGnhwoVubm6dfQzDsMuXL+/bt6+srMzZ2RkAIJX+MTg3aNAgImrrAotNx01UvHxqXRQ6FuRwGc0yQg59li5dumLFiszMzMmTJx8+fLizj23fvn316tXR0dFbtmx57733AAAm0x8jcxwO2RcMmxp0zg4wS4NCEXQW0FsURiK2jGHYrFmzTpw4kZCQsGnTptzc3Na3WmdpaLXanTt3pqSkrFy5sl+/fjExMd3ZMqGTPIg7OKYUCkWQ7+bkRMyO2DyAwuVyFy9eDAB4/Phxa68mkTy9GqtWq7VabVRUlPmPTU1N7XrBdtp9nQh8Nwbf1f57QQr9hB5+rKpitbLJwLP23/vatWt5PN6QIUNu3LgBADDnrG/fvnQ6/Ztvvpk8ebJWq3311VdDQ0MPHjwoEomUSuX3339Po9GKi4s722bHr1u35vIClROThtEI+Z2kFPr69eth1/CHJolerzF5BrCtu9nKysobN26cPXtWrVYvW7Zs5MiRAACBQODl5XX+/Pnr168rFIqJEycOGDAgKyvr8OHDYrF42bJlgYGBx44dmz17tl6v37NnT3x8fHR0dOs2O37dujXfv9zkF8rx7GHlvwoKotaU1YrHqtI81cjXHGjCZmcyvq8eNc2D52r/t3hSaEcMAAiI5N46I6sVa7wDLf/2NzU1paSkWHzL39+/srKy4+sJCQmffvqptSttb+HChRb32lFRUa1XWdqKjY3dvHlzZ1vL+1XOc2U4Qv4o1wsCAKqK1bfOSqe8Y/n+CaPRWFdXZ/EtDLP8s3A4HKFQaO0y25NIJHq9hUu6nVXFYrFEok6nRX7/Qem8jwNZHPs/HaZiBAEAlw/Xh/Xn+Yc5wy4EjodZcp3GFJtI+K8NRVBoUKbVqGmeZ3fXqpWEjBFSXEVhS+kDpePkj6IRBADMXBPw01cVsKsgW3Oj/vy+uleW+MEuhFRU3BGbadXG/RsrZr8f4CCHRHViTea+utkfBNAcYCywLepG0NwrHNj0ZPIiH297v6Gz8K7it2vyaX+391kxllA6gmYXD9SpVcZhk9xJm1BNpsqilqwMqX8oZ9hkd9i1wGEDEQQAlOWpsjIaQmK4XgHs4N5cO9hVaVTGsnxVTZlG3qAfNklk9QtCNsQ2ImhWdL+56L6yLE8VNVjAYGJcAYPrQmex6TbxA9DpmEphaFEYlHKDQmaoE2uCe3HDY/kBEQ469tTKliLYqvyRSl6vVykMKrnRYDCZrDp6o9frCwoK+vbta82NAsDh0XET7ixg8FwYIh+mb087P7rtPpuMIKGkUunMmTMzMzNhF+IoKDouiDgOFEEEMhTB9jAMCw8Ph12FA0ERbA/H8d9//x12FQ4ERbA9DMNcXBx08XsoUATbw3FcLpfDrsKBoAha4OXliA9fgAVF0ILOJmYjREARbA/DsLZ3yiFEQxFsD8fxgoIC2FU4EBTB9jAMI3/5GEeGItgejuPELd+LdIQiiECGItgeOh0hGYpge+h0hGQogghkKILtYRhGwgIgSCsUwfZwHG9sbIRdhQNBEWwPzRckGYpge2i+IMlQBBHIUATbQ1NWSYYi2B6askoyFEEEMhRBBDIUQQtaH4CDkABF0AKLa+QjBEERRCBDEUQgQxFsD40LkgxFsD00LkgyFEEEMhTB9jAMCwwMhF2FA0ERbA/HcbFYDLsKB4IiiECGItgehmF0ukM874kiUATbw3HcaHTEJzDCgiLYHrqPmGQogu2h+4hJhiLYHrp9iWTo0TdPvfnmm7W1tXQ63Wg0SiQSLy8vDMMMBsPp06dhl2bnUC/41LRp05qbm6urq+vq6kwmU01NTXV1NYbZ/PMWqQ9F8KmkpKSQkJC2r+A4HhsbC68iR4Ei+IeZM2c6O//xXExvb+9Zs2ZBrcghoAj+ISkpqfXqsLkLjIyMhF2U/UMR/JO5c+dyuVxzFzhz5kzY5TgEFME/GTt2bGBgII7j/fv3RzcxkYMBu4Bn0KiMDdU6ndZEWosp4xaBluPjR8wrzVOR1qgzl+7m68RkOeK1aeqOCxoNeOa+2srf1f7hXD2JEYRCrzXJ6jSh/fijpnrCroVsFI2gVm08trUqNsndN9i5Gx+3E49uN9WVqye95QO7EFJRNIJ7N4hHTfdxcWfCLoRsxbmK2rKWCfO9YRdCHiqejuRny4OieQ6YPwBAaD8BbgLVpQ704BMqRrC+QsvhU/08iThOLJq0Rge7CvJQMYI6jUng5gS7CmhcvVkquQF2FeShYgQ1LSZHnrZs1OEGPRUP0AlCxQgiDgVFEIEMRRCBDEUQgQxFEIEMRRCBDEUQgQxFEIEMRRCBDEUQgQxFEIHMniNYVFw4KjHu5s3rz/Uto9H48GFu21fWfbxy0eI5z9t6x+0gFtlzBF/M15s/25K2gTrbsXsogu3ptFpKbcfu2cnMUI1Gs3ff9suXMyUN9V5ePuPGvjx71hvmt8rKSw4e3lNYWODvH/DusrUxMf0AAPX1dTt2/vfWrSyVStmjR+CsmW+MSRwPANi4af3lK+cBAKMS4wAAP+0/6ePtCwBQtag+Wb/m3v3bTCYrcfT4Nxe8zWKxAAAGg2Hnrm3nMk/J5U2BgcHz5y2KHzay43aOHj4rErnD/kuiKHuIoNFo/PAf7z3My52SOiO0Z3i5uPRJpbh1sd59+3dMm/r6hPGTfzqw6x8frfhp30kej2cwGh4/zn9l8msuAtdrNy59sWGdn1+PqMhec2YtkNTX1dRUffD+PwEAIrenuamrq3lpyPClb6+8c+fmkaP7q6qffPHZFgDAN5s/v3DxzJzZC4KCel64eOajj1d9+68f+vTp3247Li6uUP+GKM0eInj12sX7uTmrV32UPOGVju++u2xtUtJEAEBgQPDb78y/e+9WwohEXx+/XT8eMS+cNWHCK6mvjsnKuhIV2cvfP8DFxVXWKDV3lq1CgkOXvr0CADA+aZK7u+fhI/t+++2eUOh2LvPU3NcXzp+3CACQMCJxztzUXbu/27J5W2fbQTqyhwjevvMri8VKGjfR4rsCwdOneQUF9QQASCR15j8Wl/y+a/d3hYUF5n5UJpN2s7nUlOmHj+y7n5tj3rfGx48yv45h2MC4IecvoPUIn489nI40yqTuIo9nLpNPo9HMaQMA3Lt/5+2l8/Q63ZrVn3z6ySaBwMWEd/dueXd3DwCASqVUqZQAAKGrW+tbAoFLS0uLSkXeMgx2wB56QR6PL2vsbh9mtnfvdl9f/w1fpDEYDAAAh81p+27X91Y3NTUCAIRCN3d3TwCAQiE3hxIAIJNJGQwGm83uznYQM3voBfv3H6hWqy9eOtf6isHwjDvQ5Iqm0J7h5vzpdLoWdYvJ9LQXZLM5Mpm09Y8dXb16AQAwYMCgqKjeGIZl37phfl2n02XfutGrVx9zf/zM7SBm9tALjh2TfPzE4Y1fffL4cX5oz/DSsuK79259v21/F1/p1y/u3LmM02dOCPguR47tb25WlJeV4DiOYVjfPgPOnD255V8bYnr34/MFQ4eOAACUlBb9579bevYMKywsyDj1c8KIxMiIaABA0riJu3Z/ZzQafX39f/klXSaTfvjBZ+Ym2m7H19cfnZd0xh4iyGKxNn+z7Ycf/n3+wulTv/zs7e07auS4rjvCBfOXyKQN//6/r/l8wcSXp0x7bc6WtA33c3MG9B84dmxy4e8Fmed/uZl9fXzSJHMEZ86Yl5f326lffuZyeVNfm/3G/MXm7bz37vtcLi/9+KHmZkVwUM8Nn/9rQP+B5rfabmfu62+hCHaGimvKnNhWHR7n6h/mQAsatZX/a5NBZ4h/xVGGsu3hWBCxaSiCCGQogghkKIIIZCiCCGQogghkKIIIZCiCCGQogghkKIIIZCiCCGQogghkKIIIZFScrCUQOdFolJu/Qxo6A3Oo5yFSsRfkcGmSSse9D7y2vEUgcqDHrlAxgoFRzooGB3r8UDtqpTEgnNOND9oJKkbQJ5gj8mX+erIediEQnN9XFZvoyuQ40I6YirOmze5daqwu1fiFcT382AwmFX9VrEijNEhrtQ9vNI6e7hkQ4VjTxakbQQBARaGqMEfZ0mxsrPvTftloNOr1+tZ7Ja0Lx3GNRsPhkLQrVKvVLBZL4Mby8Gf2H+nqUEeBT+E2aNmyZcRtPC0tLT4+/uTJk8Q10VZ9ff3HH39MTlvUROlesKNLly6NHj2auO3X1NQsW7asvLw8Kipq7969xDXU0Z49exITE/38/MhslAps6Rhr+vTpRP8LHTlypLy8HABQUVFx6tQpQttqJzk5ecmSJVrHW5XQNnrB2tpaFxeXqqqq0NBQ4lqpqqpavny5WCw2/5H8jtB8aPjgwYPo6Gg+n09y07DYQC945MiR7OxsDodDaP4AAOnp6a35AwCIxeITJ04Q2mJHHA4nLCxs0qRJSqWS5KZhsYEIisXilJQUoluprq6+fPly21dUKtX+/V2tCkIQNze3K1euaDSa+nqHGBmldARv3rwJAFi1ahUJbR08eNDcBbYuRIRh2JMnT0ho2iJ3d3cejzdkyJDi4mJYNZAE9im5ZRqNZuDAgc3NzeQ3LZVKp0+fTn67Ful0ul27dsGuglhU7AVlMplYLL558yaPxyO/dRzHZTIZ+e1a5OTkNG/ePADAmjVrJBIJ7HIIQbkIbt++XSaThYeHP3PVVIeyYsWKzz//HHYVhKBWBIuKivR6PdFnvl3DMMz8QAdK8fb2/vbbbwEAp0/b21rWFIpgbW2tUChcsmQJ3DJwHKfy+HBwcPD48ePNK2bbB6pEMDk5WSgUurvDX1QPw7Do6GjYVXTKPGDe3NxcV1cHuxbrgB9Bo9F45syZnTt3UmT3ZzQaKT4g5+Hh4erqqlAovvzyS9i1WAHkCJaXl9fV1U2YMMHLywtuJa10Op1NXJkICwsLCwt78OAB7EL+KpgRbG5uXrlypa+vL8QaOtLpdBEREbCr6JbXXnstJCRELBZXVlbCruXFwYxgUVHRsWPHIBZgUV1dHUGTYYnA4/ECAwOXLl1K8YOHLsCJYG1tbXp6+oABA6C03rWioiKRSAS7iudz4sSJJ0+eaDQa2IW8CAgRLCgoWL16dWpqKvlNd4dUKu3Tpw/sKp5bbGys0Wj87rvvYBfy3CBEMCIigvx5eN2Xnp4+aNAg2FW8CC6Xi2FYTk4O7EKeD6kRNBgMe/bsofKVt5ycnOHDh0O5Nm0Vf/vb31xcXGBX8XxIjeC0adPGjRtHZovP6+DBg4mJibCr+EvCwsKuXbsGZabji7GNifvkqKmpWbt27Z49e2AXYgVZWVlqtXrMmDGwC3k2kiJYWVmpVCojIyNJaOuFffjhhwkJCUlJSbALcSxk7IiNRuOUKVMonr/Hjx9rNBo7y98XX3zR9m4YiiJhWuz9+/fLy8tJaOivSElJEYvFsKuwMqVSOW3aNNhVPAM6FgQAgAMHDgAAZs6cCbsQR0T4jvjQoUMUP8C/c+fO1atX7Th/x44dq6mpgV1FpwiP4KlTp+Li4ohu5YWZTKZPP/1027ZtsAshUFBQ0Pr162FX0Slid8Q4jqtUKiqP9M6YMeOzzz4LCwuDXQixHj582KNHD1dXV9iFWODQx4JoFIYKiN0R37p1a/ny5YQ28cIOHjzYu3dvB8mfwWCYOnUq7CosIzaCNBpNp6PiqtHHjx8vKiqaNWsW7EJIwmAw3NzcqDmDgdgdsU6nUygUVLgpqa2srKxDhw5t3boVdiGkMhqNOI4zGJR7zIfDHQvm5+dv3rz5xx9/hF0I8hThgzIpKSlSqZToVrqprKzsk08+ccz85efnL1iwAHYVFhAewQEDBpSUlBDdSnfU19dv3br16NGjsAuBQygUNjY2wq7CAkfZETc0NMyePfvcuXOwC0Hag38rOwkqKipmzJiB8kfNZUAIj6BUKp00aRLRrXRBIpGsW7fuwoULEGugAq1WS80p64SfootEIm9v78bGRqFQSHRbHUkkkjlz5qD+z7xWTktLC+wqLCDpWPCVV15RqVQKhcLT05O0hylUVFSkpaVt2bKFnOaoT61Wk/ZUqe4jsBccMWKE+dcOx3EMw8z/Q9qiVSUlJatWrUpPTyenOZtAwfwReyw4evRoGo1m3gWYX6HT6YMHDyauxVZ5eXk//PADyl9ber2empeJCYzg+vXro6Oj2+7oPT09+/btS1yLZrm5uV9//fXGjRuJbsi24DhOzdWPiD0j/uqrr4KCgsz/j+M4n88nehHf69evnzp1avfu3YS2YouYTCbJjzTrJmIj6OXl9fe//908TQHDMKK7wHPnzh07dmzdunWEtmK7qLlcE+HjgvHx8VOmTOFyuTwej9ADwePHj1+9ejUtLY24JmyaXq+fOHEi7Cos6NYZsUFvUitNL9zGzKkLxCX1RUVFIQG9mhsNL7ydLly+fDn/YemGDRuI2Lh9MD/VB3YVFjxjXPDRbcWD63JZrY7D+0trEbWOyxBEp9N5+vGqS1pC+vAGjhWKfCmxbDUVrF69+uLFi62DYuYjIhzH7927B7u0p7rqBW9nyhqq9cOnePPdbONp9SYj3iTRnd5VO2aWl0+QzayUSqglS5YUFBSYl+dv7QVazxGpoNNjwVtnZXKJYXiql63kDwBAo2Nu3qyUpYEXD9TXVdjkkqNWFxISEhsb23Zfh2HYiBEjoBb1J5Yj2Fiva6jSDpnoSXo91jF6pk9OJhXnxkExd+7ctg808Pf3nzFjBtSK/sRyBBuqtDhO4KEb0fhCpydFLTrti59C2ZPQ0NDWdWNxHB8+fDh1HrHRaQSVcqNHD9s+lgqM5spqqPscL5K9/vrrnp6eAAA/P7/Zs2fDLudPLEdQrzXpNbbdhSikBgBsuCO3rp49ew4ePBjH8YSEBEp1gWTMF0RegMmEVzxuUTYaVAqDQY+rVVaY7dzXd46mf1iE27ALB6zw8Do2h87k0JwFdIHQKSDS+a9sCkWQWh7dVhTeVVYWtfiGCww6nO5EpzkxAGaNQQkae9BLL+tNQG+NeavNStyoNxgNeicn7cnvqgOjueH9eRFx/BfYFIogVRTcUtw40eARwGdw+b3HUmtf2TVhoFtzfUv+XU1WhnR4iiis//MFEUUQPrXSeHpnnd5ICxnsz2BS94kYncEwTODFBYDL8xDkXJI9uqN8+U1vOr27B+IOcQcdlVUUqvZ8Ieb5uXlHeNhi/tpichg+0Z5Moeu2NSX1T7p7aQBFEKa6J5qrP8siRgSyODZzCeqZ2DxmrzHBp3fWKaTdWtEKRRCasnxl5j5Jj37UehautQQN9P/5v7W14mf3hSiCcCibDBcP2G3+zILi/H7+d5VB/4wBZhRBOM7uqQsa5Ae7CsL1HOL7y4/PGIZEEYQg53yjETAZTrZ98tEdLC5TpcLyb8q7+AyKIATZp6WeoRDWloDCM8QtK0PWxQesGcGCR3la7V+aGXDl6oVRiXEVFeXWK4py7l6Q+UW7ETqH/IX9c9PEoyesfPMrg0UXBfDzfu20I7RaBM+ey1j6znyNRm2tDdqrR3eUbBfbnoX0vFg89uMcZWfvWi2Cf7H/cxAKmV6jMnH4jnVrC0/EkTzR6DuZvmmdC3Rnz2WkfbsRAJAyZQwAYO2aT8YnTQIAZGb+sv/AzurqSpHI/eXk1Nmz3jAv8WEwGHbu2nYu85Rc3hQYGDx/3qL4YSM7bjY7+8b32/9dXV3p7e07edJrU1KnW6VaiJ4Utgj9iXoQUHHp3dPn/1td+zuf5xYaHDdh7BIB3x0AsO6LxFcnrc17dKWgMIvD5g0ZmDpu1ELzV4xG44UrO7Jzjut06p4hsXo9UXc7uAfxxY9aQvtZ+Nmt0wsOHjRs2tQ5AIAvv0jbmrZ98KBhAIBz5059+dUnYWGRH63bMDJh7I87/7f/p53mz3+z+fNDh/dOfDn1Hx9+7u3t+9HHqx48uN9umy0tLev/uZbpxFy5Yt3Ql0ZIpRKrlApXQ40exwk5BSwqufPDnuVensHTUv4xYuis0vL723Yu1emeRurgz5/6eoe//ea2AX0nZF76oaAwy/x6+qmvz1/ZERk+NHXiKqYTW61pJqI2AIDRiDVKLF8ssU4vKBS6+fr6AwCionq7uLiaJ4hv//E/MTH91n34OQBgxPDRzc2Kg4d2vzplZkND/bnMU3NfXzh/3iIAQMKIxDlzU3ft/m7L5j89CK6xSabVaocPHz12zASrFEkFKrmBwSJkeavjv2weEpeaOnGV+Y/hoYO/3jq9sDg7JnokAGDQgMmJCfMBAL7e4bfvnvi9ODs6Ylhl9ePsnPTEhDcmjFkMAIjr/3JJGVF3djqxGMpObiEnaqZMZWVFQ4Nk+rTXW18ZOPCl02dOVFZVFBYWAADi40eZX8cwbGDckPMXTrfbgq+PX69effbt38FmcyZNnMJkMgkqlUxqpZEltP5woKyxpk5S1iB7kp1zvO3rTfKnw8JM5tPc0+l0F4GnXCEBADwsuAIAGDH0j0eQYhhRg3QMFq1FQW4ElSolAMDV1a31FT5fAABokNSrVEoAgLDNWwKBS0tLi0qlarsFDMM2bti6fcf/bfsu7cjRfR+s/WffvgMIqpY0BK0n2qyUAgDGjlrYJ3pU29f5fAsPHaLRGCaTEQDQ1FTLZvO4zi6E1NQOjpk6+dmtnPrW+1U9PbwAAHJ5U+tbjY0ycxDd3T0BAArFHwNFMpmUwWCw2e2HKng83nvvvr971zEul7fuoxXUXKf2uXBd6Aat9dcc57D5AAC9XuvpEdT2Pw67q1MfLleo0Sj1BjKe0GbQGvhCy/2d1SLIYXMAAA0NT08aRCJ3by+f27ezWj9w9eoFNpsdGhoRFdUbw7DsWzfMr+t0uuxbN3r16kOn05lOzLbpNA/0+Pr4TUmdoVQpa2urrVUtLHwXhkFn/Qh6uAe4unjfuZeh1T0dlzUaDQaDvutv+ftFAgDuPyBjIW6Dzsh3tRxBusWHJVeVqI0G4B30HAfObI7ziZNHysWlGMAKHj2MiIjm8wSHjuyTSOr0ev3P6QcvXDwze9aCgXFDBHxBbW1N+vFDAGANDZL//e9fZeUlq1d97OPjx3BySj9+6HFhfkBAkLvIY+78KQ0NEqm0If34IZ1W++aCt7v/CLWi+4qgKGdeJz82LEq5Xlpr4Lha+YwEwzChq8/tuycLHl/HAS5+8jD91GajURfYIwYAcOn6Hn/fyIjQp8uaZd85zmZz+/cZ5+ke/CD/4t37p9UapVLVePNOeklZjr9vVHRkvHXLAwBo5KrgaLabl4UDeqtFUMAXeHh4Xbly/ubN683NiqSkiaGh4UKh26XLmWfOnmxqlM2a9cac2QvMF6YGxr2kUinPnD1x6dI5rjN31cp1Awe+BADg8/g+3r737t+hYbSo6JjKyoobWZev37gkEnm8v2a9n59/9+uhZgSdBYzbvzSIAq1/+OXlEeTvF11anns393RFZb6PT2hsvwnmccHOIkij0aLC4yUN4gf5F0vLc709Q2SN1V4ewUREsOxu3ZjZXjSahcuSllfWun1OptOAviPdOr5lK07vqEyY4u5NvcWNftr0xDVA5OziQBdImhtaDIrm1KWWJ0dSq5NwBNFDeMX56i4i+Hvx7T2HPuj4OofN72zoeGLSsiFxKdaq8FFh1v6jH3d8HcdxAHCLAzeL3/iPv29kZxvUKrW9BnE7exfm3Yn6AAACkElEQVRFkGz9RghvnioR+gvoDMvngkEBfVa8vbfj6zgOOpte48yx5p69Z3CsxQJMJhOO43S6hXFNAd+js63p1HpFrTJqYKfLyaEIQjBskqjgrsw7wvKTwplMthsT5oR+6xbQUNo4PKWrNa7RlFUI+gx35bCNWvUzBk3sgKZZ6yrCur65HUUQjglveJdmV8GuglgmE156uzr5De+uP4YiCAeTRUtZ4lt2255TWJpdOXNNwDM/hiIIjU8wZ8o73mW3qfhEpL/IaDAVZVXMWusv9Hz25BIUQZhcRMxJC73zMsvUCvtZGVvVqCm6UTF9hb8zr1snuyiCkLn7sZZu6WlSKqry6rQqMmYMEEet0D75rcbJpFz8VU9Bt1fJR4My8GEY9vKbPmV5qmvp9c6ubIYzS+DhTLedu4wNWqNCojJqdXqVduQU9x7hz7fiJYogVQT35gb35pY8VBbdVxVnydz8nfVaE53JYLAYFFyxGMdxo9Zg1BucmLTGWnVwb27YMF5Q9Issi4giSC09Y3g9Y3gAgJoytUpuVMkNOq1JY42Ffq2L5UxjOzOdBc58Id0r4BnDLl1DEaQon2AqPkGdCJYjyGRjJup1/s/FxcOJsBshEGuy/K/EFzpJxLa9LkLZA6XIxx7ueLJ7liPo2YNFyTVPuqtJogvq5cxwQt2gDei0F/QLZV87Vkt6PdZxcX/1kGQqPoEc6air5xHn35QX5Sr7JoiEXszOJrdRilppkDforx2tfXWZn2s3Lg0hVPCMR2KX5atyrzbVlmnoDKrvmN18WHKJLqS386AJIq4AnenbjGdEsJVWTfVH0uE4YDvbQFeNtNPdCCIIQVC3gUCGIohAhiKIQIYiiECGIohAhiKIQPb/1qVsc/Lv2q0AAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's visualize the graph we've built. We'll use the same code as in the previous video.\n",
    "from IPython.display import Image, Markdown, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  NVIDIA A100 vs. NVIDIA RTX A6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Assistant:  [{\"url\": \"https://www.bhphotovideo.com/c/product/1607840-REG/pny_technologies_vcnrtxa6000_pb_nvidia_rtx_a6000_graphic.html\", \"content\": \"Buy PNY NVIDIA RTX A6000 Graphics Card featuring 10752 CUDA Cores, Ampere Architecture, 48GB of ECC GDDR6 VRAM, 384-Bit Memory Interface, DisplayPort 1.4a, PCI Express 4.0 x16 Interface, Blower-Style Fan Cooler. ... PNY RTX A6000 Specs. Key Specs. GPU Model: NVIDIA RTX 6000. Stream Processors: 10,752 CUDA Cores. Dedicated AI Cores: Yes\"}, {\"url\": \"https://www.techpowerup.com/gpu-specs/rtx-a6000.c3686\", \"content\": \"The RTX A6000 is an enthusiast-class professional graphics card by NVIDIA, launched on October 5th, 2020. Built on the 8 nm process, and based on the GA102 graphics processor, the card supports DirectX 12 Ultimate. The GA102 graphics processor is a large chip with a die area of 628 mm² and 28,300 million transistors.\"}, {\"url\": \"https://blog.spheron.network/ultimate-guide-to-the-nvidia-rtx-a6000-key-features-and-specs\", \"content\": \"The NVIDIA RTX A6000 is a powerful GPU based on the Ampere architecture, offering impressive capabilities for handling memory-intensive tasks in various applications. As a cost-effective solution for high-performance computing (HPC) enthusiasts, this GPU distinguishes itself from other models like the NVIDIA RTX A5000.\"}, {\"url\": \"https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/quadro-product-literature/proviz-print-nvidia-rtx-a6000-datasheet-us-nvidia-1454980-r9-web+(1).pdf\", \"content\": \"The NVIDIA RTX ™ A6000, built on the NVIDIA Ampere architecture, delivers everything designers, engineers, scientists, and artists need to meet the most graphics and compute-intensive workflows. The RTX A6000 is equipped with the latest generation RT Cores, Tensor Cores, and CUDA® cores for unprecedented rendering, AI, graphics,\"}, {\"url\": \"https://www.tomshardware.com/pc-components/gpus/150-nvidia-rtx-a6000-gpus-power-the-vegas-sphere\", \"content\": \"150 Nvidia RTX A6000 GPUs power the Vegas Sphere — Nvidia fuels 1.2 million LED exterior pucks and multi-layered 16K interior screens. ... producing specs that make the eyes pop.\"}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Assistant:  ### Comparison: NVIDIA A100 vs. NVIDIA RTX A6000\n",
      "\n",
      "#### Overview\n",
      "Both the NVIDIA A100 and the NVIDIA RTX A6000 are high-performance GPUs designed for different applications, including AI, data science, and high-performance computing (HPC). They are built on NVIDIA's Ampere architecture but cater to different use cases.\n",
      "\n",
      "#### Key Specifications\n",
      "\n",
      "1. **NVIDIA A100**\n",
      "   - **CUDA Cores**: 6,912 (for the A100 PCIe version)\n",
      "   - **Memory**: Available in 40 GB and 80 GB configurations of HBM2 memory\n",
      "   - **Memory Bandwidth**: Up to 1,555 GB/s\n",
      "   - **Power Consumption**: 300 W\n",
      "   - **Architecture**: Ampere\n",
      "   - **Process Technology**: 7 nm\n",
      "   - **Performance**: Optimized for AI training and inference, offering features like Multi-Instance GPU (MiG) support for partitioning the GPU into multiple instances.\n",
      "   - **Use Case**: Primarily designed for data centers and AI workloads.\n",
      "\n",
      "   [More about A100 specifications](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/PB-10577-001_v02.pdf)\n",
      "\n",
      "2. **NVIDIA RTX A6000**\n",
      "   - **CUDA Cores**: 10,752\n",
      "   - **Memory**: 48 GB of GDDR6 ECC memory\n",
      "   - **Memory Bandwidth**: 768 GB/s\n",
      "   - **Power Consumption**: 300 W\n",
      "   - **Architecture**: Ampere\n",
      "   - **Process Technology**: 8 nm\n",
      "   - **Performance**: Designed for rendering, AI, and visualization tasks, the A6000 supports real-time ray tracing and AI-enhanced workflows.\n",
      "   - **Use Case**: Targeted towards designers, engineers, and artists for high-performance visual computing.\n",
      "\n",
      "   [More about RTX A6000 specifications](https://www.techpowerup.com/gpu-specs/rtx-a6000.c3686)\n",
      "\n",
      "#### Performance and Use Cases\n",
      "- **NVIDIA A100** is more suited for deep learning and AI workloads, offering superior performance in training large models and handling massive datasets.\n",
      "- **NVIDIA RTX A6000** excels in graphics-intensive tasks, real-time ray tracing, and applications requiring high memory bandwidth, making it ideal for creative professionals and visual computing.\n",
      "\n",
      "#### Summary\n",
      "- If your primary focus is on AI and data science, the **NVIDIA A100** is the better choice due to its specialized architecture and capabilities.\n",
      "- For tasks involving graphics rendering, visualization, and design, the **NVIDIA RTX A6000** is more appropriate, offering a higher number of CUDA cores and optimized memory for those applications.\n",
      "\n",
      "For further details and comparisons, you can explore the following resources:\n",
      "- [Comparative Analysis of A6000 vs A100 GPUs](https://www.cudocompute.com/blog/a-comparative-analysis-of-a6000-vs-a100-gpus-pytorch-performance)\n",
      "- [NVIDIA A100 Datasheet](https://resources.nvidia.com/en-us-gpu/nvidia-a100-datashee-1)\n",
      "- [NVIDIA RTX A6000 Datasheet](https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/quadro-product-literature/proviz-print-nvidia-rtx-a6000-datasheet-us-nvidia-1454980-r9-web+(1).pdf)\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  What are LLM agents?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Assistant:  [{\"url\": \"https://www.rapidinnovation.io/post/llm-agents-the-complete-guide\", \"content\": \"1.1 Definition and Concept of LLM Agents. LLM agents refer to systems or applications that utilize large language models to perform tasks involving natural language understanding and generation. These agents are built on complex algorithms trained on vast amounts of text data, enabling them to comprehend and produce language in a way that\"}, {\"url\": \"https://www.sapien.io/blog/what-are-llm-agents\", \"content\": \"LLM agents are AI-powered autonomous systems that leverage large language models for enhanced decision-making. Types of LLM agents include rule-based, learning-based, and hybrid models. Key features include natural language understanding, contextual awareness, adaptability, and tool integration.\"}, {\"url\": \"https://www.castordoc.com/ai-strategy/understanding-llm-agents-what-they-are-and-how-they-work\", \"content\": \"At its core, an LLM agent is an AI system that utilizes language models to generate human-like text. By analyzing patterns within this data, LLM agents gain insight into linguistic structures, forming a foundation for generating high-quality text outputs. These algorithms incorporate neural networks and natural language processing mechanisms, allowing the LLM agent to analyze, learn, and generate text. Through exposure to large amounts of training data, the LLM agent learns patterns and structures, refining its language modeling capabilities over time. In conclusion, LLM agents are powerful AI systems that leverage language models to generate human-like text, transforming various industries.\"}, {\"url\": \"https://blog.promptlayer.com/types-of-llm-agent/\", \"content\": \"type of LLM agent What are LLM Agents? LLM Agents are advanced AI systems that leverage large language models to autonomously perform tasks by interpreting inputs, planning actions, and executing them using integrated tools. Different Types of LLM Agents Clinical Decision Support: LLM agents assist healthcare professionals by analyzing patient data and medical literature to provide evidence-based treatment recommendations. Investment Analysis: LLM agents analyze vast financial datasets to provide insights into market trends, aiding investors in making informed decisions. For example, in legal advisory roles, an LLM agent could provide not only recommendations but also the rationale behind them, allowing users to make informed decisions. From healthcare and finance to education and customer service, these agents are proving their worth by streamlining processes, enhancing decision-making, and personalizing user experiences.\"}, {\"url\": \"https://medium.com/@aydinKerem/what-is-an-llm-agent-and-how-does-it-work-1d4d9e4381ca\", \"content\": \"Large Language Model[2] The core computational engine of an LLM agent is a large language model. LLM is trained on a massive dataset to understand and reason from text data.\"}]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Assistant:  LLM agents, or Large Language Model agents, are advanced AI systems that utilize large language models to perform various tasks related to natural language understanding and generation. Here are some key points about LLM agents:\n",
      "\n",
      "1. **Definition**: LLM agents are systems or applications that leverage large language models to interpret inputs, plan actions, and execute tasks autonomously. They are built on complex algorithms trained on vast amounts of text data, enabling them to comprehend and produce language in a human-like manner.\n",
      "\n",
      "2. **Capabilities**: These agents possess features such as natural language understanding, contextual awareness, adaptability, and the ability to integrate with various tools. They analyze patterns in language data, allowing them to generate high-quality text outputs and make informed decisions.\n",
      "\n",
      "3. **Types**: LLM agents can vary in their design and application, including rule-based, learning-based, and hybrid models. They can be used in various fields, such as healthcare (for clinical decision support), finance (for investment analysis), and customer service.\n",
      "\n",
      "4. **Applications**: LLM agents are transforming industries by streamlining processes, enhancing decision-making, and personalizing user experiences. They can assist professionals by analyzing data and providing evidence-based recommendations.\n",
      "\n",
      "Overall, LLM agents represent a significant advancement in AI technology, enabling machines to interact with human language more effectively.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input('User: ')\n",
    "    if user_input.lower() in ['quit', 'exit', 'bye',  'q']:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "        \n",
    "    for event in graph.stream({'messages': ('user', user_input)}):\n",
    "        for value in event.values():\n",
    "            print('Assistant: ', value['messages'][-1].content)\n",
    "        print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Memory to the ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7f032d8f9b90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "tool = TavilySearchResults(max_results=3)\n",
    "tools = [tool]\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.5)\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {'messages': [llm_with_tools.invoke(state['messages'])]}\n",
    "\n",
    "graph_builder.add_node('chatbot', chatbot)\n",
    "tool_node = ToolNode(tools=[tool])\n",
    "graph_builder.add_node('tools', tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    'chatbot',\n",
    "    tools_condition,\n",
    ")\n",
    "\n",
    "graph_builder.add_edge('tools', 'chatbot')\n",
    "graph_builder.set_entry_point('chatbot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.checkpoint.memory import MemorySaver ### fix\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(':memory:')\n",
    "graph = graph_builder.compile(checkpointer=MemorySaver()) ### fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'configurable': {'thread_id': '1'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the lastest AI model developed by OpenAI?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_3NpS3bjjXv5zJtMvIiAqXn0g)\n",
      " Call ID: call_3NpS3bjjXv5zJtMvIiAqXn0g\n",
      "  Args:\n",
      "    query: latest AI model developed by OpenAI\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"url\": \"https://openai.com/o1/\", \"content\": \"Latest advancements. OpenAI o1; OpenAI o1-mini; GPT-4; GPT-4o mini; DALL·E 3; Sora; ChatGPT. For Everyone; For Teams; For Enterprises; ChatGPT login (opens in a new window) Download; API. ... We've developed a new series of AI models designed to spend more time thinking before they respond. Here is the latest news on o1 research, product and\"}, {\"url\": \"https://www.technologyreview.com/2024/09/17/1104004/why-openais-new-model-is-such-a-big-deal/\", \"content\": \"Why OpenAI’s new model is such a big deal | MIT Technology Review This week we're going to talk about OpenAI’s impressive new reasoning model, called o1. However, last week OpenAI released a new model called o1 (previously referred to under the code name “Strawberry” and, before that, Q*) that blows GPT-4o out of the water for this type of purpose. Unlike previous models that are well suited for language tasks like writing and editing, OpenAI o1 is focused on multistep “reasoning,” the type of process required for advanced mathematics, coding, or other STEM-based questions. The new model also won’t be most users’ first pick for more language-heavy tasks, where GPT-4o continues to be the better option, according to OpenAI’s user surveys.\"}, {\"url\": \"https://www.wired.com/story/openai-o1-strawberry-problem-reasoning/\", \"content\": \"“This is what we consider the new paradigm in these models,” Mira Murati, OpenAI’s chief technology officer, tells WIRED. Murati says OpenAI o1 uses reinforcement learning, which involves giving a model positive feedback when it gets answers right and negative feedback when it does not, in order to improve its reasoning process. Mark Chen, vice president of research at OpenAI, demonstrated the new model to WIRED, using it to solve several problems that its prior model, GPT-4o, cannot. “The [new] model is learning to think for itself, rather than kind of trying to imitate the way humans would think,” as a conventional LLM does, Chen says. OpenAI says its new model performs markedly better on a number of problem sets, including ones focused on coding, math, physics, biology, and chemistry.\"}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The latest AI model developed by OpenAI is called **o1**. This model, previously referred to by the code names \"Strawberry\" and \"Q*\", is designed to excel in multistep reasoning tasks, making it particularly suitable for advanced mathematics, coding, and other STEM-related questions. Unlike its predecessor, GPT-4o, which is better for language-heavy tasks, o1 focuses on reasoning processes and has shown significant improvements in solving complex problems.\n",
      "\n",
      "For more detailed information, you can check out the following sources:\n",
      "- [OpenAI's Official Announcement](https://openai.com/o1/)\n",
      "- [MIT Technology Review Article](https://www.technologyreview.com/2024/09/17/1104004/why-openais-new-model-is-such-a-big-deal/)\n",
      "- [Wired Article on OpenAI o1](https://www.wired.com/story/openai-o1-strawberry-problem-reasoning/)\n"
     ]
    }
   ],
   "source": [
    "prompt = 'What is the lastest AI model developed by OpenAI?'\n",
    "\n",
    "# streaming the events. \n",
    "events = graph.stream(\n",
    "    {'messages': [('user', prompt)]}, config, stream_mode='values'\n",
    ")\n",
    "\n",
    "for event in events:\n",
    "    event['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What about Google?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_eLIWnjhXegk5tVybkmbLQPJ7)\n",
      " Call ID: call_eLIWnjhXegk5tVybkmbLQPJ7\n",
      "  Args:\n",
      "    query: latest AI model developed by Google\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"url\": \"https://techcrunch.com/2024/12/04/deepminds-genie-2-can-generate-interactive-worlds-that-look-like-video-games/\", \"content\": \"DeepMind, Google's AI research org, has unveiled a model that can generate an \\\"endless\\\" variety of playable 3D worlds.. Called Genie 2, the model — the successor to DeepMind's Genie\"}, {\"url\": \"https://blog.google/technology/ai/google-gemini-ai/\", \"content\": \"This is a long-term commitment that requires building collaboratively, so we’re partnering with the industry and broader ecosystem on defining best practices and setting safety and security benchmarks through organizations like MLCommons, the Frontier Model Forum and its AI Safety Fund, and our Secure AI Framework (SAIF), which was designed to help mitigate security risks specific to AI systems across the public and private sectors. This is a significant milestone in the development of AI, and the start of a new era for us at Google as we continue to rapidly innovate and responsibly advance the capabilities of our models.\\n This helps Gemini seamlessly understand and reason about all kinds of inputs from the ground up, far better than existing multimodal models — and its capabilities are state of the art in nearly every domain.\\nLearn more about Gemini’s capabilities and see how it works.\\n Gemini unlocks new scientific insights\\nUnderstanding text, images, audio and more\\nGemini 1.0 was trained to recognize and understand text, images, audio and more at the same time, so it better understands nuanced information and can answer questions relating to complicated topics. AI\\nIntroducing Gemini: our largest and most capable AI model\\nDec 06, 2023\\nmin read\\nMaking AI more helpful for everyone\\nA note from Google and Alphabet CEO Sundar Pichai:\\n\"}, {\"url\": \"https://cloud.google.com/blog/products/ai-machine-learning/introducing-veo-and-imagen-3-on-vertex-ai\", \"content\": \"Introducing Veo and Imagen 3 on Vertex AI | Google Cloud Blog Veo and Imagen 3: Announcing new video and image generation models on Vertex AI That’s why Google is investing in its AI technology with new models like Veo, our most advanced video generation model, and Imagen 3, our highest quality image generation model. Its AI-powered operating system for marketing transformation, WPP Open, already utilizes Imagen 3 for image generation and will soon incorporate Veo for video generation, streamlining the ideation and production of content. They’re now testing Imagen and Veo on Vertex AI to create visuals, allowing Agoda teams to generate unique images of travel destinations which would then be used to generate videos.\"}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The latest AI model developed by Google is called **Gemini**. Gemini 1.0 is a multimodal model capable of understanding and reasoning about text, images, audio, and more simultaneously, making it one of the most advanced AI models in various domains. It is designed to provide nuanced understanding and insights across a wide range of topics.\n",
      "\n",
      "Additionally, Google has also introduced **Veo** and **Imagen 3**, which are advanced models for video and image generation, respectively. These models are part of Google's ongoing efforts to innovate in AI technology and improve content creation capabilities.\n",
      "\n",
      "For more detailed information, you can refer to the following sources:\n",
      "- [Google Gemini Announcement](https://blog.google/technology/ai/google-gemini-ai/)\n",
      "- [DeepMind's Genie 2 Announcement](https://techcrunch.com/2024/12/04/deepminds-genie-2-can-generate-interactive-worlds-that-look-like-video-games/)\n",
      "- [Veo and Imagen 3 on Vertex AI](https://cloud.google.com/blog/products/ai-machine-learning/introducing-veo-and-imagen-3-on-vertex-ai)\n"
     ]
    }
   ],
   "source": [
    "# It has memory!\n",
    "prompt = 'What about Google?'\n",
    "events = graph.stream(\n",
    "    {'messages': [('user', prompt)]}, config, stream_mode='values'\n",
    ")\n",
    "for event in events:\n",
    "    event['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "And for AWS?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_ZPHWuIByJTTkuppINlssNoc9)\n",
      " Call ID: call_ZPHWuIByJTTkuppINlssNoc9\n",
      "  Args:\n",
      "    query: latest AI model developed by AWS\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"url\": \"https://www.techrepublic.com/article/amazon-nova-latest-ai-models/\", \"content\": \"Amazon Nova: the Latest AI Models Revolutionizing Business Amazon Nova: Inside the Latest AI Models Revolutionizing Business Amazon Nova: Inside the Latest AI Models Revolutionizing Business “Our new Amazon Nova models are intended to help with these challenges for internal and external builders and provide compelling intelligence and content generation while also delivering meaningful progress on latency, cost-effectiveness, customization, Retrieval Augmented Generation (RAG), and agentic capabilities.” Amazon Nova is a line of generative AI foundation models available on AWS’s Amazon Bedrock AI hosting service. Amazon expects Nova Premier to bring multimodal (video, image, or text-to-text) interpretation and a hefty data library that organizations can use to train other models. Amazon Nova: Inside the Latest AI Models Revolutionizing Business\"}, {\"url\": \"https://techcrunch.com/2024/12/03/amazon-announces-nova-a-new-family-of-multimodal-ai-models/\", \"content\": \"Amazon announces Nova, a new family of multimodal AI models | TechCrunch Amazon announces Nova, a new family of multimodal AI models | TechCrunch Amazon announces Nova, a new family of multimodal AI models At its re:Invent conference on Tuesday, Amazon Web Services (AWS), Amazon’s cloud computing division, announced a new family of multimodal generative AI models it calls Nova. In addition to those, there’s an image-generation model, Nova Canvas, and a video-generating model, Nova Reel. They’re available in AWS Bedrock, Amazon’s AI development platform, where they can be fine-tuned on text, images, and video and distilled for improved speed and higher efficiency. AI, AI, Amazon, Amazon Web Services, AWS, aws re:Invent, aws reinvent 2024, Enterprise, Generative AI, Media & Entertainment, Nova, reinvent 2024\"}, {\"url\": \"https://www.crn.com/news/cloud/2024/amazon-nova-ai-models-and-new-killer-feature-in-bedrock-are-huge-aws-partner-opportunities\", \"content\": \"Amazon Nova AI Models And New ‘Killer Feature’ In Bedrock Are Huge AWS Partner Opportunities Amazon Nova AI Models And New ‘Killer Feature’ In Bedrock Are Huge AWS Partner Opportunities Amazon’s new Nova foundational models and Amazon Bedrock latency-optimized inference feature represent massive opportunities for channel partners. Amazon’s new Nova family of foundational models and new latency-optimized inference on Amazon Bedrock are home runs for Amazon Web Services partners. Launched at AWS re:Invent 2024 in Las Vegas this week, Amazon Nova foundational models (FMs) are a new generation of AI models aimed at delivering intelligence and industry leading price performance on Amazon Bedrock. One new feature that will help drive AI and GenAI adoption for Amazon Bedrock and new Nova models is AWS’ new latency-optimized inference for FMs in Amazon Bedrock.\"}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The latest AI model developed by AWS is called **Amazon Nova**. This new family of multimodal generative AI models was announced at the AWS re:Invent 2024 conference. The Nova models include capabilities for text, image, and video generation, with specific models such as **Nova Canvas** for image generation and **Nova Reel** for video generation. These models are designed to be available on Amazon Bedrock, AWS's AI development platform, allowing for fine-tuning and improved efficiency.\n",
      "\n",
      "Amazon Nova aims to enhance intelligence and content generation while addressing challenges related to latency, cost-effectiveness, and customization.\n",
      "\n",
      "For more detailed information, you can refer to the following sources:\n",
      "- [TechRepublic on Amazon Nova](https://www.techrepublic.com/article/amazon-nova-latest-ai-models/)\n",
      "- [TechCrunch Announcement](https://techcrunch.com/2024/12/03/amazon-announces-nova-a-new-family-of-multimodal-ai-models/)\n",
      "- [CRN on Nova AI Models](https://www.crn.com/news/cloud/2024/amazon-nova-ai-models-and-new-killer-feature-in-bedrock-are-huge-aws-partner-opportunities)\n"
     ]
    }
   ],
   "source": [
    "# It has memory!\n",
    "prompt = 'And for AWS?'\n",
    "events = graph.stream(\n",
    "    {'messages': [('user', prompt)]}, config, stream_mode='values'\n",
    ")\n",
    "for event in events:\n",
    "    event['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What did I ask you so far?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "So far, you have asked about the latest AI models developed by the following companies:\n",
      "\n",
      "1. OpenAI\n",
      "2. Google\n",
      "3. AWS (Amazon Web Services)\n"
     ]
    }
   ],
   "source": [
    "prompt = 'What did I ask you so far?'\n",
    "\n",
    "events = graph.stream(\n",
    "    {'messages': [('user', prompt)]}, config, stream_mode='values'\n",
    ")\n",
    "for event in events:\n",
    "    event['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Now provide detailed explanations how I can use each of these models from each service provider.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_x1t1aINgHQQFj1NaeUc4s3Hf)\n",
      " Call ID: call_x1t1aINgHQQFj1NaeUc4s3Hf\n",
      "  Args:\n",
      "    query: how to use OpenAI o1 model\n",
      "  tavily_search_results_json (call_An1M2WaGLOu2O0NP2bDn6X4P)\n",
      " Call ID: call_An1M2WaGLOu2O0NP2bDn6X4P\n",
      "  Args:\n",
      "    query: how to use Google Gemini model\n",
      "  tavily_search_results_json (call_4Dk3zbUNnM9kcZFvi6N9TTXv)\n",
      " Call ID: call_4Dk3zbUNnM9kcZFvi6N9TTXv\n",
      "  Args:\n",
      "    query: how to use AWS Nova model\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"url\": \"https://ndurner.github.io/amazon-nova\", \"content\": \"Amazon Nova foundation model release Since there's community interest in how to set up AWS to use the new Amazon Nova models, here's a step-by-step guide to get everyone started: Ensure you have model access: open Bedrock in us-west-2 region, scroll down in the menu on the left, and hit Model Access:\"}, {\"url\": \"https://aws.amazon.com/ai/generative-ai/nova/\", \"content\": \"Generative Foundation Model - Amazon Nova - AWS Amazon Nova Canvas and Amazon Nova Reel are creative content generation models that accept text and image inputs and produce image or video outputs. Amazon Nova Pro’s capabilities, coupled with its industry-leading speed and cost efficiency, makes it a compelling model for almost any task, including video summarization, Q&A, mathematical reasoning, software development, and AI agents that can execute multi-step workflows. Amazon Nova Reel is a state-of-the-art video generation model that allows customers to easily create high quality video from text and images. Amazon Nova Canvas represents a significant jump forward in image quality from AWS’s already impressive line-up of models, which makes us really excited to include it in the Shutterstock AI Image Generator.\"}, {\"url\": \"https://press.aboutamazon.com/2024/12/introducing-amazon-nova-a-new-generation-of-foundation-models\", \"content\": \"The Amazon Nova model learns what matters most to the customer from their own data (including text, images, and videos), and then Amazon Bedrock trains a private fine-tuned model that will provide tailored responses. Amazon Nova Reel is a state-of-the-art video generation model that allows customers to easily create high-quality video from text and images. SAP, a strategic partner of AWS, is integrating Amazon Nova models into its SAP AI Core generative AI hub’s family of supported LLMs. This enables developers to create new skills for Joule, SAP’s AI copilot, and securely build AI-driven solutions that harness the full business context captured in SAP data, enabling automation, personalization, and advanced solutions like supply chain planning.\"}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here are detailed explanations on how to use the latest AI models from OpenAI, Google, and AWS:\n",
      "\n",
      "### 1. OpenAI o1 Model\n",
      "To use the OpenAI o1 model, follow these steps:\n",
      "\n",
      "- **Get Access**: First, you need to obtain access to the OpenAI o1 API. This typically involves signing up on the OpenAI platform and getting your API key.\n",
      "\n",
      "- **Set Up Your Environment**:\n",
      "  - Install the OpenAI Python library, which allows you to interact with the API.\n",
      "  - Use the following code snippet to set up the API connection:\n",
      "    ```python\n",
      "    import openai\n",
      "\n",
      "    openai.api_key = 'YOUR_API_KEY'\n",
      "    ```\n",
      "\n",
      "- **Make API Requests**:\n",
      "  - You can send requests to the o1 model using the following structure:\n",
      "    ```python\n",
      "    response = openai.ChatCompletion.create(\n",
      "        model=\"o1\",\n",
      "        messages=[{\"role\": \"user\", \"content\": \"Your prompt here\"}]\n",
      "    )\n",
      "    print(response['choices'][0]['message']['content'])\n",
      "    ```\n",
      "\n",
      "- **Manage Costs**: Be aware that using o1 may incur higher costs than previous models, so you should monitor your usage and manage the number of tokens used in requests.\n",
      "\n",
      "For more detailed guidance, you can refer to [this tutorial](https://dev.to/apilover/how-to-use-the-openai-o1-api-4bjb).\n",
      "\n",
      "### 2. Google Gemini Model\n",
      "To use the Google Gemini model, follow these steps:\n",
      "\n",
      "- **Access the API**: Start by obtaining an API key from the Google Cloud Console and enable the Gemini API.\n",
      "\n",
      "- **Install the Google Generative AI Library**:\n",
      "  - You can install the library using pip:\n",
      "    ```bash\n",
      "    pip install google-generative-ai\n",
      "    ```\n",
      "\n",
      "- **Configure the API**:\n",
      "  - Use the following code to configure your API key:\n",
      "    ```python\n",
      "    import google.generativeai as genai\n",
      "\n",
      "    genai.configure(api_key='YOUR_API_KEY')\n",
      "    ```\n",
      "\n",
      "- **Make API Calls**:\n",
      "  - After setting up, you can call the Gemini model like this:\n",
      "    ```python\n",
      "    response = genai.generate_text(\"Your prompt here\")\n",
      "    print(response)\n",
      "    ```\n",
      "\n",
      "- **Explore Capabilities**: Gemini can handle various input types, including text, images, and audio. Check the [official documentation](https://ai.google.dev/gemini-api/docs) for more features and examples.\n",
      "\n",
      "### 3. AWS Nova Model\n",
      "To use the Amazon Nova model, follow these steps:\n",
      "\n",
      "- **Access AWS Bedrock**: Ensure you have access to AWS Bedrock, where the Nova models are hosted. You may need to set up an account and request access if you do not have it yet.\n",
      "\n",
      "- **Navigate to the Model Access**:\n",
      "  - Open AWS Bedrock in the US West (Oregon) region and locate the Model Access section in the left menu.\n",
      "\n",
      "- **Choose the Model**: You can select from different Nova models, such as Nova Canvas for image generation or Nova Reel for video generation.\n",
      "\n",
      "- **Make API Calls**:\n",
      "  - Use the AWS SDK or the API directly to integrate the Nova models into your applications. For example, you can send requests to generate images or videos based on your input text.\n",
      "\n",
      "- **Explore Use Cases**: Nova models are suitable for various tasks, including video summarization, Q&A, and content generation. Check the [AWS Nova page](https://aws.amazon.com/ai/generative-ai/nova/) for more information on capabilities and examples.\n",
      "\n",
      "For detailed instructions, you can refer to [this guide](https://ndurner.github.io/amazon-nova).\n",
      "\n",
      "By following these steps, you can effectively utilize the latest AI models from OpenAI, Google, and AWS in your projects.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Now provide detailed explanations how I can use each of these models from each service provider.'\n",
    "\n",
    "events = graph.stream(\n",
    "    {'messages': [('user', prompt)]}, config, stream_mode='values'\n",
    ")\n",
    "for event in events:\n",
    "    event['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What did I ask you so far?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You haven't asked me anything yet in this conversation. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# NEW THREAD => no memory\n",
    "config1 = {\"configurable\": {\"thread_id\": \"10\"}}\n",
    "\n",
    "prompt = 'What did I ask you so far?'\n",
    "\n",
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", prompt)]}, config1, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'messages': [HumanMessage(content='What is the lastest AI model developed by OpenAI?', additional_kwargs={}, response_metadata={}, id='0c018b21-d224-41a1-8c4a-9d3783c7ce31'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_3NpS3bjjXv5zJtMvIiAqXn0g', 'function': {'arguments': '{\"query\":\"latest AI model developed by OpenAI\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 91, 'total_tokens': 115, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-a38cb64c-b5fc-4454-9d52-6a0b4ee5badc-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'latest AI model developed by OpenAI'}, 'id': 'call_3NpS3bjjXv5zJtMvIiAqXn0g', 'type': 'tool_call'}], usage_metadata={'input_tokens': 91, 'output_tokens': 24, 'total_tokens': 115, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='[{\"url\": \"https://openai.com/o1/\", \"content\": \"Latest advancements. OpenAI o1; OpenAI o1-mini; GPT-4; GPT-4o mini; DALL·E 3; Sora; ChatGPT. For Everyone; For Teams; For Enterprises; ChatGPT login (opens in a new window) Download; API. ... We\\'ve developed a new series of AI models designed to spend more time thinking before they respond. Here is the latest news on o1 research, product and\"}, {\"url\": \"https://www.technologyreview.com/2024/09/17/1104004/why-openais-new-model-is-such-a-big-deal/\", \"content\": \"Why OpenAI’s new model is such a big deal | MIT Technology Review This week we\\'re going to talk about OpenAI’s impressive new reasoning model, called o1. However, last week OpenAI released a new model called o1 (previously referred to under the code name “Strawberry” and, before that, Q*) that blows GPT-4o out of the water for this type of purpose. Unlike previous models that are well suited for language tasks like writing and editing, OpenAI o1 is focused on multistep “reasoning,” the type of process required for advanced mathematics, coding, or other STEM-based questions. The new model also won’t be most users’ first pick for more language-heavy tasks, where GPT-4o continues to be the better option, according to OpenAI’s user surveys.\"}, {\"url\": \"https://www.wired.com/story/openai-o1-strawberry-problem-reasoning/\", \"content\": \"“This is what we consider the new paradigm in these models,” Mira Murati, OpenAI’s chief technology officer, tells WIRED. Murati says OpenAI o1 uses reinforcement learning, which involves giving a model positive feedback when it gets answers right and negative feedback when it does not, in order to improve its reasoning process. Mark Chen, vice president of research at OpenAI, demonstrated the new model to WIRED, using it to solve several problems that its prior model, GPT-4o, cannot. “The [new] model is learning to think for itself, rather than kind of trying to imitate the way humans would think,” as a conventional LLM does, Chen says. OpenAI says its new model performs markedly better on a number of problem sets, including ones focused on coding, math, physics, biology, and chemistry.\"}]', name='tavily_search_results_json', id='2a15c89d-0b00-44d9-ab91-3c23ac902d50', tool_call_id='call_3NpS3bjjXv5zJtMvIiAqXn0g', artifact={'query': 'latest AI model developed by OpenAI', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'OpenAI o1 Hub | OpenAI', 'url': 'https://openai.com/o1/', 'content': \"Latest advancements. OpenAI o1; OpenAI o1-mini; GPT-4; GPT-4o mini; DALL·E 3; Sora; ChatGPT. For Everyone; For Teams; For Enterprises; ChatGPT login (opens in a new window) Download; API. ... We've developed a new series of AI models designed to spend more time thinking before they respond. Here is the latest news on o1 research, product and\", 'score': 0.9979966, 'raw_content': None}, {'title': \"Why OpenAI's new model is such a big deal - MIT Technology Review\", 'url': 'https://www.technologyreview.com/2024/09/17/1104004/why-openais-new-model-is-such-a-big-deal/', 'content': \"Why OpenAI’s new model is such a big deal | MIT Technology Review This week we're going to talk about OpenAI’s impressive new reasoning model, called o1. However, last week OpenAI released a new model called o1 (previously referred to under the code name “Strawberry” and, before that, Q*) that blows GPT-4o out of the water for this type of purpose. Unlike previous models that are well suited for language tasks like writing and editing, OpenAI o1 is focused on multistep “reasoning,” the type of process required for advanced mathematics, coding, or other STEM-based questions. The new model also won’t be most users’ first pick for more language-heavy tasks, where GPT-4o continues to be the better option, according to OpenAI’s user surveys.\", 'score': 0.98955137, 'raw_content': None}, {'title': 'OpenAI Announces a New AI Model, Code-Named Strawberry, That Solves ...', 'url': 'https://www.wired.com/story/openai-o1-strawberry-problem-reasoning/', 'content': '“This is what we consider the new paradigm in these models,” Mira Murati, OpenAI’s chief technology officer, tells WIRED. Murati says OpenAI o1 uses reinforcement learning, which involves giving a model positive feedback when it gets answers right and negative feedback when it does not, in order to improve its reasoning process. Mark Chen, vice president of research at OpenAI, demonstrated the new model to WIRED, using it to solve several problems that its prior model, GPT-4o, cannot. “The [new] model is learning to think for itself, rather than kind of trying to imitate the way humans would think,” as a conventional LLM does, Chen says. OpenAI says its new model performs markedly better on a number of problem sets, including ones focused on coding, math, physics, biology, and chemistry.', 'score': 0.98445636, 'raw_content': None}], 'response_time': 2.02}), AIMessage(content='The latest AI model developed by OpenAI is called **o1**. This model, previously referred to by the code names \"Strawberry\" and \"Q*\", is designed to excel in multistep reasoning tasks, making it particularly suitable for advanced mathematics, coding, and other STEM-related questions. Unlike its predecessor, GPT-4o, which is better for language-heavy tasks, o1 focuses on reasoning processes and has shown significant improvements in solving complex problems.\\n\\nFor more detailed information, you can check out the following sources:\\n- [OpenAI\\'s Official Announcement](https://openai.com/o1/)\\n- [MIT Technology Review Article](https://www.technologyreview.com/2024/09/17/1104004/why-openais-new-model-is-such-a-big-deal/)\\n- [Wired Article on OpenAI o1](https://www.wired.com/story/openai-o1-strawberry-problem-reasoning/)', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 194, 'prompt_tokens': 645, 'total_tokens': 839, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-a6ea6b88-65b5-4d42-bcd2-6cf57f2afd9f-0', usage_metadata={'input_tokens': 645, 'output_tokens': 194, 'total_tokens': 839, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What about Google?', additional_kwargs={}, response_metadata={}, id='f4a32f4a-860d-42db-bf30-7064dea4e1bc'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_eLIWnjhXegk5tVybkmbLQPJ7', 'function': {'arguments': '{\"query\":\"latest AI model developed by Google\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 850, 'total_tokens': 873, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f56c2a9e-b04a-4a0b-ac79-7e87653325b5-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'latest AI model developed by Google'}, 'id': 'call_eLIWnjhXegk5tVybkmbLQPJ7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 850, 'output_tokens': 23, 'total_tokens': 873, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='[{\"url\": \"https://techcrunch.com/2024/12/04/deepminds-genie-2-can-generate-interactive-worlds-that-look-like-video-games/\", \"content\": \"DeepMind, Google\\'s AI research org, has unveiled a model that can generate an \\\\\"endless\\\\\" variety of playable 3D worlds.. Called Genie 2, the model — the successor to DeepMind\\'s Genie\"}, {\"url\": \"https://blog.google/technology/ai/google-gemini-ai/\", \"content\": \"This is a long-term commitment that requires building collaboratively, so we’re partnering with the industry and broader ecosystem on defining best practices and setting safety and security benchmarks through organizations like MLCommons, the Frontier Model Forum and its AI Safety Fund, and our Secure AI Framework (SAIF), which was designed to help mitigate security risks specific to AI systems across the public and private sectors. This is a significant milestone in the development of AI, and the start of a new era for us at Google as we continue to rapidly innovate and responsibly advance the capabilities of our models.\\\\n This helps Gemini seamlessly understand and reason about all kinds of inputs from the ground up, far better than existing multimodal models — and its capabilities are state of the art in nearly every domain.\\\\nLearn more about Gemini’s capabilities and see how it works.\\\\n Gemini unlocks new scientific insights\\\\nUnderstanding text, images, audio and more\\\\nGemini 1.0 was trained to recognize and understand text, images, audio and more at the same time, so it better understands nuanced information and can answer questions relating to complicated topics. AI\\\\nIntroducing Gemini: our largest and most capable AI model\\\\nDec 06, 2023\\\\nmin read\\\\nMaking AI more helpful for everyone\\\\nA note from Google and Alphabet CEO Sundar Pichai:\\\\n\"}, {\"url\": \"https://cloud.google.com/blog/products/ai-machine-learning/introducing-veo-and-imagen-3-on-vertex-ai\", \"content\": \"Introducing Veo and Imagen 3 on Vertex AI | Google Cloud Blog Veo and Imagen 3: Announcing new video and image generation models on Vertex AI That’s why Google is investing in its AI technology with new models like Veo, our most advanced video generation model, and Imagen 3, our highest quality image generation model. Its AI-powered operating system for marketing transformation, WPP Open, already utilizes Imagen 3 for image generation and will soon incorporate Veo for video generation, streamlining the ideation and production of content. They’re now testing Imagen and Veo on Vertex AI to create visuals, allowing Agoda teams to generate unique images of travel destinations which would then be used to generate videos.\"}]', name='tavily_search_results_json', id='52757606-0ba7-4214-a174-08cfb32cc231', tool_call_id='call_eLIWnjhXegk5tVybkmbLQPJ7', artifact={'query': 'latest AI model developed by Google', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': \"DeepMind's Genie 2 can generate interactive worlds that look like video ...\", 'url': 'https://techcrunch.com/2024/12/04/deepminds-genie-2-can-generate-interactive-worlds-that-look-like-video-games/', 'content': 'DeepMind, Google\\'s AI research org, has unveiled a model that can generate an \"endless\" variety of playable 3D worlds.. Called Genie 2, the model — the successor to DeepMind\\'s Genie', 'score': 0.99128854, 'raw_content': None}, {'title': \"Introducing Gemini: Google's most capable AI model yet\", 'url': 'https://blog.google/technology/ai/google-gemini-ai/', 'content': 'This is a long-term commitment that requires building collaboratively, so we’re partnering with the industry and broader ecosystem on defining best practices and setting safety and security benchmarks through organizations like MLCommons, the Frontier Model Forum and its AI Safety Fund, and our Secure AI Framework (SAIF), which was designed to help mitigate security risks specific to AI systems across the public and private sectors. This is a significant milestone in the development of AI, and the start of a new era for us at Google as we continue to rapidly innovate and responsibly advance the capabilities of our models.\\n This helps Gemini seamlessly understand and reason about all kinds of inputs from the ground up, far better than existing multimodal models — and its capabilities are state of the art in nearly every domain.\\nLearn more about Gemini’s capabilities and see how it works.\\n Gemini unlocks new scientific insights\\nUnderstanding text, images, audio and more\\nGemini 1.0 was trained to recognize and understand text, images, audio and more at the same time, so it better understands nuanced information and can answer questions relating to complicated topics. AI\\nIntroducing Gemini: our largest and most capable AI model\\nDec 06, 2023\\nmin read\\nMaking AI more helpful for everyone\\nA note from Google and Alphabet CEO Sundar Pichai:\\n', 'score': 0.9754836, 'raw_content': None}, {'title': 'Introducing Veo and Imagen 3 on Vertex AI | Google Cloud Blog', 'url': 'https://cloud.google.com/blog/products/ai-machine-learning/introducing-veo-and-imagen-3-on-vertex-ai', 'content': 'Introducing Veo and Imagen 3 on Vertex AI | Google Cloud Blog Veo and Imagen 3: Announcing new video and image generation models on Vertex AI That’s why Google is investing in its AI technology with new models like Veo, our most advanced video generation model, and Imagen 3, our highest quality image generation model. Its AI-powered operating system for marketing transformation, WPP Open, already utilizes Imagen 3 for image generation and will soon incorporate Veo for video generation, streamlining the ideation and production of content. They’re now testing Imagen and Veo on Vertex AI to create visuals, allowing Agoda teams to generate unique images of travel destinations which would then be used to generate videos.', 'score': 0.9662198, 'raw_content': None}], 'response_time': 1.51}), AIMessage(content=\"The latest AI model developed by Google is called **Gemini**. Gemini 1.0 is a multimodal model capable of understanding and reasoning about text, images, audio, and more simultaneously, making it one of the most advanced AI models in various domains. It is designed to provide nuanced understanding and insights across a wide range of topics.\\n\\nAdditionally, Google has also introduced **Veo** and **Imagen 3**, which are advanced models for video and image generation, respectively. These models are part of Google's ongoing efforts to innovate in AI technology and improve content creation capabilities.\\n\\nFor more detailed information, you can refer to the following sources:\\n- [Google Gemini Announcement](https://blog.google/technology/ai/google-gemini-ai/)\\n- [DeepMind's Genie 2 Announcement](https://techcrunch.com/2024/12/04/deepminds-genie-2-can-generate-interactive-worlds-that-look-like-video-games/)\\n- [Veo and Imagen 3 on Vertex AI](https://cloud.google.com/blog/products/ai-machine-learning/introducing-veo-and-imagen-3-on-vertex-ai)\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 232, 'prompt_tokens': 1438, 'total_tokens': 1670, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-4a69c968-e7bf-4912-bb52-f5fc0a6b634a-0', usage_metadata={'input_tokens': 1438, 'output_tokens': 232, 'total_tokens': 1670, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='And for AWS?', additional_kwargs={}, response_metadata={}, id='566cd3f7-87ff-47b3-96bb-2e2220f4250c'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ZPHWuIByJTTkuppINlssNoc9', 'function': {'arguments': '{\"query\":\"latest AI model developed by AWS\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 1681, 'total_tokens': 1704, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1536}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-ced51782-c306-42b3-add4-32dfb60fdff8-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'latest AI model developed by AWS'}, 'id': 'call_ZPHWuIByJTTkuppINlssNoc9', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1681, 'output_tokens': 23, 'total_tokens': 1704, 'input_token_details': {'audio': 0, 'cache_read': 1536}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='[{\"url\": \"https://www.techrepublic.com/article/amazon-nova-latest-ai-models/\", \"content\": \"Amazon Nova: the Latest AI Models Revolutionizing Business Amazon Nova: Inside the Latest AI Models Revolutionizing Business Amazon Nova: Inside the Latest AI Models Revolutionizing Business “Our new Amazon Nova models are intended to help with these challenges for internal and external builders and provide compelling intelligence and content generation while also delivering meaningful progress on latency, cost-effectiveness, customization, Retrieval Augmented Generation (RAG), and agentic capabilities.” Amazon Nova is a line of generative AI foundation models available on AWS’s Amazon Bedrock AI hosting service. Amazon expects Nova Premier to bring multimodal (video, image, or text-to-text) interpretation and a hefty data library that organizations can use to train other models. Amazon Nova: Inside the Latest AI Models Revolutionizing Business\"}, {\"url\": \"https://techcrunch.com/2024/12/03/amazon-announces-nova-a-new-family-of-multimodal-ai-models/\", \"content\": \"Amazon announces Nova, a new family of multimodal AI models | TechCrunch Amazon announces Nova, a new family of multimodal AI models | TechCrunch Amazon announces Nova, a new family of multimodal AI models At its re:Invent conference on Tuesday, Amazon Web Services (AWS), Amazon’s cloud computing division, announced a new family of multimodal generative AI models it calls Nova. In addition to those, there’s an image-generation model, Nova Canvas, and a video-generating model, Nova Reel. They’re available in AWS Bedrock, Amazon’s AI development platform, where they can be fine-tuned on text, images, and video\\xa0and distilled for improved speed and higher efficiency. AI, AI, Amazon, Amazon Web Services, AWS, aws re:Invent, aws reinvent 2024, Enterprise, Generative AI, Media & Entertainment, Nova, reinvent 2024\"}, {\"url\": \"https://www.crn.com/news/cloud/2024/amazon-nova-ai-models-and-new-killer-feature-in-bedrock-are-huge-aws-partner-opportunities\", \"content\": \"Amazon Nova AI Models And New ‘Killer Feature’ In Bedrock Are Huge AWS Partner Opportunities Amazon Nova AI Models And New ‘Killer Feature’ In Bedrock Are Huge AWS Partner Opportunities Amazon’s new Nova foundational models and Amazon Bedrock latency-optimized inference feature represent massive opportunities for channel partners. Amazon’s new Nova family of foundational models and new latency-optimized inference on Amazon Bedrock are home runs for Amazon Web Services partners. Launched at AWS re:Invent 2024 in Las Vegas this week, Amazon Nova foundational models (FMs) are a new generation of AI models aimed at delivering intelligence and industry leading price performance on Amazon Bedrock. One new feature that will help drive AI and GenAI adoption for Amazon Bedrock and new Nova models is AWS’ new latency-optimized inference for FMs in Amazon Bedrock.\"}]', name='tavily_search_results_json', id='d3af655d-0382-4976-9705-afb924a2fd29', tool_call_id='call_ZPHWuIByJTTkuppINlssNoc9', artifact={'query': 'latest AI model developed by AWS', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Amazon Nova: the Latest AI Models Revolutionizing Business - TechRepublic', 'url': 'https://www.techrepublic.com/article/amazon-nova-latest-ai-models/', 'content': 'Amazon Nova: the Latest AI Models Revolutionizing Business Amazon Nova: Inside the Latest AI Models Revolutionizing Business Amazon Nova: Inside the Latest AI Models Revolutionizing Business “Our new Amazon Nova models are intended to help with these challenges for internal and external builders and provide compelling intelligence and content generation while also delivering meaningful progress on latency, cost-effectiveness, customization, Retrieval Augmented Generation (RAG), and agentic capabilities.” Amazon Nova is a line of generative AI foundation models available on AWS’s Amazon Bedrock AI hosting service. Amazon expects Nova Premier to bring multimodal (video, image, or text-to-text) interpretation and a hefty data library that organizations can use to train other models. Amazon Nova: Inside the Latest AI Models Revolutionizing Business', 'score': 0.9958175, 'raw_content': None}, {'title': 'Amazon announces Nova, a new family of multimodal AI models', 'url': 'https://techcrunch.com/2024/12/03/amazon-announces-nova-a-new-family-of-multimodal-ai-models/', 'content': 'Amazon announces Nova, a new family of multimodal AI models | TechCrunch Amazon announces Nova, a new family of multimodal AI models | TechCrunch Amazon announces Nova, a new family of multimodal AI models At its re:Invent conference on Tuesday, Amazon Web Services (AWS), Amazon’s cloud computing division, announced a new family of multimodal generative AI models it calls Nova. In addition to those, there’s an image-generation model, Nova Canvas, and a video-generating model, Nova Reel. They’re available in AWS Bedrock, Amazon’s AI development platform, where they can be fine-tuned on text, images, and video\\xa0and distilled for improved speed and higher efficiency. AI, AI, Amazon, Amazon Web Services, AWS, aws re:Invent, aws reinvent 2024, Enterprise, Generative AI, Media & Entertainment, Nova, reinvent 2024', 'score': 0.97779876, 'raw_content': None}, {'title': \"Amazon Nova AI Models And New 'Killer Feature' In Bedrock Are Huge AWS ...\", 'url': 'https://www.crn.com/news/cloud/2024/amazon-nova-ai-models-and-new-killer-feature-in-bedrock-are-huge-aws-partner-opportunities', 'content': 'Amazon Nova AI Models And New ‘Killer Feature’ In Bedrock Are Huge AWS Partner Opportunities Amazon Nova AI Models And New ‘Killer Feature’ In Bedrock Are Huge AWS Partner Opportunities Amazon’s new Nova foundational models and Amazon Bedrock latency-optimized inference feature represent massive opportunities for channel partners. Amazon’s new Nova family of foundational models and new latency-optimized inference on Amazon Bedrock are home runs for Amazon Web Services partners. Launched at AWS re:Invent 2024 in Las Vegas this week, Amazon Nova foundational models (FMs) are a new generation of AI models aimed at delivering intelligence and industry leading price performance on Amazon Bedrock. One new feature that will help drive AI and GenAI adoption for Amazon Bedrock and new Nova models is AWS’ new latency-optimized inference for FMs in Amazon Bedrock.', 'score': 0.97068775, 'raw_content': None}], 'response_time': 1.87}), AIMessage(content=\"The latest AI model developed by AWS is called **Amazon Nova**. This new family of multimodal generative AI models was announced at the AWS re:Invent 2024 conference. The Nova models include capabilities for text, image, and video generation, with specific models such as **Nova Canvas** for image generation and **Nova Reel** for video generation. These models are designed to be available on Amazon Bedrock, AWS's AI development platform, allowing for fine-tuning and improved efficiency.\\n\\nAmazon Nova aims to enhance intelligence and content generation while addressing challenges related to latency, cost-effectiveness, and customization.\\n\\nFor more detailed information, you can refer to the following sources:\\n- [TechRepublic on Amazon Nova](https://www.techrepublic.com/article/amazon-nova-latest-ai-models/)\\n- [TechCrunch Announcement](https://techcrunch.com/2024/12/03/amazon-announces-nova-a-new-family-of-multimodal-ai-models/)\\n- [CRN on Nova AI Models](https://www.crn.com/news/cloud/2024/amazon-nova-ai-models-and-new-killer-feature-in-bedrock-are-huge-aws-partner-opportunities)\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 245, 'prompt_tokens': 2328, 'total_tokens': 2573, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1536}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-d2ba53b0-1b4c-4b43-a07a-76a7304ddc38-0', usage_metadata={'input_tokens': 2328, 'output_tokens': 245, 'total_tokens': 2573, 'input_token_details': {'audio': 0, 'cache_read': 1536}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What did I ask you so far?', additional_kwargs={}, response_metadata={}, id='c29f88aa-7739-4fb9-867c-cac138014010'), AIMessage(content='So far, you have asked about the latest AI models developed by the following companies:\\n\\n1. OpenAI\\n2. Google\\n3. AWS (Amazon Web Services)', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 2588, 'total_tokens': 2623, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2432}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-a7e008f0-8853-4948-9449-c608e79c5cae-0', usage_metadata={'input_tokens': 2588, 'output_tokens': 35, 'total_tokens': 2623, 'input_token_details': {'audio': 0, 'cache_read': 2432}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='Now provide detailed explanations how I can use each of these models from each service provider.', additional_kwargs={}, response_metadata={}, id='76362294-f6ed-46f3-b4fe-439993068a59'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_x1t1aINgHQQFj1NaeUc4s3Hf', 'function': {'arguments': '{\"query\": \"how to use OpenAI o1 model\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_An1M2WaGLOu2O0NP2bDn6X4P', 'function': {'arguments': '{\"query\": \"how to use Google Gemini model\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_4Dk3zbUNnM9kcZFvi6N9TTXv', 'function': {'arguments': '{\"query\": \"how to use AWS Nova model\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 87, 'prompt_tokens': 2647, 'total_tokens': 2734, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2560}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-abe20465-2b0d-4c32-a62d-32081cf0113b-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'how to use OpenAI o1 model'}, 'id': 'call_x1t1aINgHQQFj1NaeUc4s3Hf', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'how to use Google Gemini model'}, 'id': 'call_An1M2WaGLOu2O0NP2bDn6X4P', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'how to use AWS Nova model'}, 'id': 'call_4Dk3zbUNnM9kcZFvi6N9TTXv', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2647, 'output_tokens': 87, 'total_tokens': 2734, 'input_token_details': {'audio': 0, 'cache_read': 2560}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='[{\"url\": \"https://dev.to/apilover/how-to-use-the-openai-o1-api-4bjb\", \"content\": \"How to Use the OpenAI o1 API - DEV Community How to Use the OpenAI o1 API #openai #api #development #ai The release of o1 marks a milestone in OpenAI’s pursuit of AI models with human-like reasoning capabilities. How to Use OpenAI o1? If you’re eager to tap into OpenAI’s latest model, o1, for its enhanced reasoning abilities, follow this quick guide to get started: 1. Get Access to the OpenAI o1 API 4. Integrate the OpenAI o1 API into Your project This code sets up an API route /ask where users can send a prompt, and the application will return the response generated by OpenAI. An Easier Way to Test OpenAI o1 API — Using Apidog You can send cURL requests to OpenAI’s API using Apidog.\"}, {\"url\": \"https://www.datacamp.com/tutorial/openai-o1-api\", \"content\": \"Learn how to connect to the OpenAI O1 models through the API and manage API costs by understanding reasoning tokens and how to control them. We’ll learn shortly how to control the number of tokens and manage costs, but for now, let’s focus on connecting to OpenAI’s API to use the o1 models. To connect to the o1-mini model through the API, follow all the steps we covered for the o1-preview model—except for the model parameter, where you need to use the string ”o1-mini”. What are reasoning tokens in OpenAI o1 models? To connect through the GPT-4o API, obtain your API key from OpenAI, install the OpenAI Python library, and use it to send requests and receive responses from the GPT-4o models.\"}, {\"url\": \"https://www.techtarget.com/WhatIs/feature/OpenAI-o1-explained-Everything-you-need-to-know\", \"content\": \"OpenAI\\'s o1 models, launched in September 2024, enhance reasoning in AI and excel in complex tasks, such as generating and debugging code. The o1 models are initially intended to be preview models, designed to provide users -- as well as OpenAI -- with a different type of LLM experience than the GPT-4o model. OpenAI o1 can perform many tasks like any of OpenAI\\'s other GPT models -- such as answering questions, summarizing content and generating new content. The o1 models are optimized for complex reasoning tasks, especially in STEM (science, technology, engineering and mathematics). For API users OpenAI o1 is more expensive than previous models -- including GPT-4o. ##### What businesses should know about OpenAI\\'s GPT-4o model  By: Will Kelly\"}]', name='tavily_search_results_json', id='ed522506-ab99-4b2a-826b-b3e998fa38d8', tool_call_id='call_x1t1aINgHQQFj1NaeUc4s3Hf', artifact={'query': 'how to use OpenAI o1 model', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'How to Use the OpenAI o1 API - DEV Community', 'url': 'https://dev.to/apilover/how-to-use-the-openai-o1-api-4bjb', 'content': 'How to Use the OpenAI o1 API - DEV Community How to Use the OpenAI o1 API #openai #api #development #ai The release of o1 marks a milestone in OpenAI’s pursuit of AI models with human-like reasoning capabilities. How to Use OpenAI o1? If you’re eager to tap into OpenAI’s latest model, o1, for its enhanced reasoning abilities, follow this quick guide to get started: 1. Get Access to the OpenAI o1 API 4. Integrate the OpenAI o1 API into Your project This code sets up an API route /ask where users can send a prompt, and the application will return the response generated by OpenAI. An Easier Way to Test OpenAI o1 API — Using Apidog You can send cURL requests to OpenAI’s API using Apidog.', 'score': 0.9985335, 'raw_content': None}, {'title': \"OpenAI O1 API Tutorial: How to Connect to OpenAI's API\", 'url': 'https://www.datacamp.com/tutorial/openai-o1-api', 'content': 'Learn how to connect to the OpenAI O1 models through the API and manage API costs by understanding reasoning tokens and how to control them. We’ll learn shortly how to control the number of tokens and manage costs, but for now, let’s focus on connecting to OpenAI’s API to use the o1 models. To connect to the o1-mini model through the API, follow all the steps we covered for the o1-preview model—except for the model parameter, where you need to use the string ”o1-mini”. What are reasoning tokens in OpenAI o1 models? To connect through the GPT-4o API, obtain your API key from OpenAI, install the OpenAI Python library, and use it to send requests and receive responses from the GPT-4o models.', 'score': 0.9982317, 'raw_content': None}, {'title': 'OpenAI o1 explained: Everything you need to know - TechTarget', 'url': 'https://www.techtarget.com/WhatIs/feature/OpenAI-o1-explained-Everything-you-need-to-know', 'content': \"OpenAI's o1 models, launched in September 2024, enhance reasoning in AI and excel in complex tasks, such as generating and debugging code. The o1 models are initially intended to be preview models, designed to provide users -- as well as OpenAI -- with a different type of LLM experience than the GPT-4o model. OpenAI o1 can perform many tasks like any of OpenAI's other GPT models -- such as answering questions, summarizing content and generating new content. The o1 models are optimized for complex reasoning tasks, especially in STEM (science, technology, engineering and mathematics). For API users OpenAI o1 is more expensive than previous models -- including GPT-4o. ##### What businesses should know about OpenAI's GPT-4o model  By: Will Kelly\", 'score': 0.99760324, 'raw_content': None}], 'response_time': 1.49}), ToolMessage(content='[{\"url\": \"https://ai.google.dev/gemini-api/docs\", \"content\": \"The Gemini API and Google AI Studio help you start working with Google\\'s latest models and turn your ideas into applications that scale. Python import google.generativeai as genai genai . configure ( api_key = \\\\\" YOUR_API_KEY \\\\\" ) model = genai .\"}, {\"url\": \"https://www.geeksforgeeks.org/getting-started-with-google-gemini-with-python-api-integration-and-model-capabilities/\", \"content\": \"Google released Gemini, their first truly multimodal device, in three sizes: Ultra, Pro, and Nano, in December. Since each Gemini model is designed for a specific set of use cases, the family of models is adaptable and functions well on a variety of platforms, including devices and data centers.. Gemini models combine and comprehend text, code, graphics, audio, and video with ease since they\"}, {\"url\": \"https://developers.google.com/learn/pathways/solution-ai-gemini-getting-started-web\", \"content\": \"Optionally, you can also try out the Gemini API using a simple NodeJS web application. Feel free to skip this step and return back to client-side web development in this pathway. Note that calling the Gemini API directly from your web app using the Google AI JavaScript SDK is only for prototyping and exploring the Gemini generative AI models.\"}]', name='tavily_search_results_json', id='df26f1be-3258-4cbc-ae07-3197dd40672b', tool_call_id='call_An1M2WaGLOu2O0NP2bDn6X4P', artifact={'query': 'how to use Google Gemini model', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Gemini API | Google AI for Developers', 'url': 'https://ai.google.dev/gemini-api/docs', 'content': 'The Gemini API and Google AI Studio help you start working with Google\\'s latest models and turn your ideas into applications that scale. Python import google.generativeai as genai genai . configure ( api_key = \" YOUR_API_KEY \" ) model = genai .', 'score': 0.97800976, 'raw_content': None}, {'title': 'Getting Started with Google Gemini with Python: API Integration and ...', 'url': 'https://www.geeksforgeeks.org/getting-started-with-google-gemini-with-python-api-integration-and-model-capabilities/', 'content': 'Google released Gemini, their first truly multimodal device, in three sizes: Ultra, Pro, and Nano, in December. Since each Gemini model is designed for a specific set of use cases, the family of models is adaptable and functions well on a variety of platforms, including devices and data centers.. Gemini models combine and comprehend text, code, graphics, audio, and video with ease since they', 'score': 0.9697281, 'raw_content': None}, {'title': 'Getting started with the Gemini API and Web apps', 'url': 'https://developers.google.com/learn/pathways/solution-ai-gemini-getting-started-web', 'content': 'Optionally, you can also try out the Gemini API using a simple NodeJS web application. Feel free to skip this step and return back to client-side web development in this pathway. Note that calling the Gemini API directly from your web app using the Google AI JavaScript SDK is only for prototyping and exploring the Gemini generative AI models.', 'score': 0.9639839, 'raw_content': None}], 'response_time': 1.67}), ToolMessage(content='[{\"url\": \"https://ndurner.github.io/amazon-nova\", \"content\": \"Amazon Nova foundation model release Since there\\'s community interest in how to set up AWS to use the new Amazon Nova models, here\\'s a step-by-step guide to get everyone started: Ensure you have model access: open Bedrock in us-west-2 region, scroll down in the menu on the left, and hit Model Access:\"}, {\"url\": \"https://aws.amazon.com/ai/generative-ai/nova/\", \"content\": \"Generative Foundation Model - Amazon Nova - AWS Amazon Nova Canvas and Amazon Nova Reel are creative content generation models that accept text and image inputs and produce image or video outputs. Amazon Nova Pro’s capabilities, coupled with its industry-leading speed and cost efficiency, makes it a compelling model for almost any task, including video summarization, Q&A, mathematical reasoning, software development, and AI agents that can execute multi-step workflows. Amazon Nova Reel is a state-of-the-art video generation model that allows customers to easily create high quality video from text and images. Amazon Nova Canvas represents a significant jump forward in image quality from AWS’s already impressive line-up of models, which makes us really excited to include it in the Shutterstock AI Image Generator.\"}, {\"url\": \"https://press.aboutamazon.com/2024/12/introducing-amazon-nova-a-new-generation-of-foundation-models\", \"content\": \"The Amazon Nova model learns what matters most to the customer from their own data (including text, images, and videos), and then Amazon Bedrock trains a private fine-tuned model that will provide tailored responses. Amazon Nova Reel is a state-of-the-art video generation model that allows customers to easily create high-quality video from text and images. SAP, a strategic partner of AWS, is integrating Amazon Nova models into its SAP AI Core generative AI hub’s family of supported LLMs. This enables developers to create new skills for Joule, SAP’s AI copilot, and securely build AI-driven solutions that harness the full business context captured in SAP data, enabling automation, personalization, and advanced solutions like supply chain planning.\"}]', name='tavily_search_results_json', id='482bad9a-5979-4c0e-bbfd-efbf19c76f6b', tool_call_id='call_4Dk3zbUNnM9kcZFvi6N9TTXv', artifact={'query': 'how to use AWS Nova model', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': \"Amazon Nova foundation model release | Nils Durner's Blog\", 'url': 'https://ndurner.github.io/amazon-nova', 'content': \"Amazon Nova foundation model release Since there's community interest in how to set up AWS to use the new Amazon Nova models, here's a step-by-step guide to get everyone started: Ensure you have model access: open Bedrock in us-west-2 region, scroll down in the menu on the left, and hit Model Access:\", 'score': 0.9961004, 'raw_content': None}, {'title': 'Generative Foundation Model - Amazon Nova - AWS', 'url': 'https://aws.amazon.com/ai/generative-ai/nova/', 'content': 'Generative Foundation Model - Amazon Nova - AWS Amazon Nova Canvas and Amazon Nova Reel are creative content generation models that accept text and image inputs and produce image or video outputs. Amazon Nova Pro’s capabilities, coupled with its industry-leading speed and cost efficiency, makes it a compelling model for almost any task, including video summarization, Q&A, mathematical reasoning, software development, and AI agents that can execute multi-step workflows. Amazon Nova Reel is a state-of-the-art video generation model that allows customers to easily create high quality video from text and images. Amazon Nova Canvas represents a significant jump forward in image quality from AWS’s already impressive line-up of models, which makes us really excited to include it in the Shutterstock AI Image Generator.', 'score': 0.9954261, 'raw_content': None}, {'title': 'Introducing Amazon Nova: A New Generation of Foundation Models', 'url': 'https://press.aboutamazon.com/2024/12/introducing-amazon-nova-a-new-generation-of-foundation-models', 'content': 'The Amazon Nova model learns what matters most to the customer from their own data (including text, images, and videos), and then Amazon Bedrock trains a private fine-tuned model that will provide tailored responses. Amazon Nova Reel is a state-of-the-art video generation model that allows customers to easily create high-quality video from text and images. SAP, a strategic partner of AWS, is integrating Amazon Nova models into its SAP AI Core generative AI hub’s family of supported LLMs. This enables developers to create new skills for Joule, SAP’s AI copilot, and securely build AI-driven solutions that harness the full business context captured in SAP data, enabling automation, personalization, and advanced solutions like supply chain planning.', 'score': 0.9951514, 'raw_content': None}], 'response_time': 1.14}), AIMessage(content='Here are detailed explanations on how to use the latest AI models from OpenAI, Google, and AWS:\\n\\n### 1. OpenAI o1 Model\\nTo use the OpenAI o1 model, follow these steps:\\n\\n- **Get Access**: First, you need to obtain access to the OpenAI o1 API. This typically involves signing up on the OpenAI platform and getting your API key.\\n\\n- **Set Up Your Environment**:\\n  - Install the OpenAI Python library, which allows you to interact with the API.\\n  - Use the following code snippet to set up the API connection:\\n    ```python\\n    import openai\\n\\n    openai.api_key = \\'YOUR_API_KEY\\'\\n    ```\\n\\n- **Make API Requests**:\\n  - You can send requests to the o1 model using the following structure:\\n    ```python\\n    response = openai.ChatCompletion.create(\\n        model=\"o1\",\\n        messages=[{\"role\": \"user\", \"content\": \"Your prompt here\"}]\\n    )\\n    print(response[\\'choices\\'][0][\\'message\\'][\\'content\\'])\\n    ```\\n\\n- **Manage Costs**: Be aware that using o1 may incur higher costs than previous models, so you should monitor your usage and manage the number of tokens used in requests.\\n\\nFor more detailed guidance, you can refer to [this tutorial](https://dev.to/apilover/how-to-use-the-openai-o1-api-4bjb).\\n\\n### 2. Google Gemini Model\\nTo use the Google Gemini model, follow these steps:\\n\\n- **Access the API**: Start by obtaining an API key from the Google Cloud Console and enable the Gemini API.\\n\\n- **Install the Google Generative AI Library**:\\n  - You can install the library using pip:\\n    ```bash\\n    pip install google-generative-ai\\n    ```\\n\\n- **Configure the API**:\\n  - Use the following code to configure your API key:\\n    ```python\\n    import google.generativeai as genai\\n\\n    genai.configure(api_key=\\'YOUR_API_KEY\\')\\n    ```\\n\\n- **Make API Calls**:\\n  - After setting up, you can call the Gemini model like this:\\n    ```python\\n    response = genai.generate_text(\"Your prompt here\")\\n    print(response)\\n    ```\\n\\n- **Explore Capabilities**: Gemini can handle various input types, including text, images, and audio. Check the [official documentation](https://ai.google.dev/gemini-api/docs) for more features and examples.\\n\\n### 3. AWS Nova Model\\nTo use the Amazon Nova model, follow these steps:\\n\\n- **Access AWS Bedrock**: Ensure you have access to AWS Bedrock, where the Nova models are hosted. You may need to set up an account and request access if you do not have it yet.\\n\\n- **Navigate to the Model Access**:\\n  - Open AWS Bedrock in the US West (Oregon) region and locate the Model Access section in the left menu.\\n\\n- **Choose the Model**: You can select from different Nova models, such as Nova Canvas for image generation or Nova Reel for video generation.\\n\\n- **Make API Calls**:\\n  - Use the AWS SDK or the API directly to integrate the Nova models into your applications. For example, you can send requests to generate images or videos based on your input text.\\n\\n- **Explore Use Cases**: Nova models are suitable for various tasks, including video summarization, Q&A, and content generation. Check the [AWS Nova page](https://aws.amazon.com/ai/generative-ai/nova/) for more information on capabilities and examples.\\n\\nFor detailed instructions, you can refer to [this guide](https://ndurner.github.io/amazon-nova).\\n\\nBy following these steps, you can effectively utilize the latest AI models from OpenAI, Google, and AWS in your projects.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 782, 'prompt_tokens': 4098, 'total_tokens': 4880, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2560}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-c576854e-e042-4807-bf21-ce8a05a1149e-0', usage_metadata={'input_tokens': 4098, 'output_tokens': 782, 'total_tokens': 4880, 'input_token_details': {'audio': 0, 'cache_read': 2560}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1efb30c2-610e-60fb-8015-61e8fdd3a46c'}}, metadata={'source': 'loop', 'writes': {'chatbot': {'messages': [AIMessage(content='Here are detailed explanations on how to use the latest AI models from OpenAI, Google, and AWS:\\n\\n### 1. OpenAI o1 Model\\nTo use the OpenAI o1 model, follow these steps:\\n\\n- **Get Access**: First, you need to obtain access to the OpenAI o1 API. This typically involves signing up on the OpenAI platform and getting your API key.\\n\\n- **Set Up Your Environment**:\\n  - Install the OpenAI Python library, which allows you to interact with the API.\\n  - Use the following code snippet to set up the API connection:\\n    ```python\\n    import openai\\n\\n    openai.api_key = \\'YOUR_API_KEY\\'\\n    ```\\n\\n- **Make API Requests**:\\n  - You can send requests to the o1 model using the following structure:\\n    ```python\\n    response = openai.ChatCompletion.create(\\n        model=\"o1\",\\n        messages=[{\"role\": \"user\", \"content\": \"Your prompt here\"}]\\n    )\\n    print(response[\\'choices\\'][0][\\'message\\'][\\'content\\'])\\n    ```\\n\\n- **Manage Costs**: Be aware that using o1 may incur higher costs than previous models, so you should monitor your usage and manage the number of tokens used in requests.\\n\\nFor more detailed guidance, you can refer to [this tutorial](https://dev.to/apilover/how-to-use-the-openai-o1-api-4bjb).\\n\\n### 2. Google Gemini Model\\nTo use the Google Gemini model, follow these steps:\\n\\n- **Access the API**: Start by obtaining an API key from the Google Cloud Console and enable the Gemini API.\\n\\n- **Install the Google Generative AI Library**:\\n  - You can install the library using pip:\\n    ```bash\\n    pip install google-generative-ai\\n    ```\\n\\n- **Configure the API**:\\n  - Use the following code to configure your API key:\\n    ```python\\n    import google.generativeai as genai\\n\\n    genai.configure(api_key=\\'YOUR_API_KEY\\')\\n    ```\\n\\n- **Make API Calls**:\\n  - After setting up, you can call the Gemini model like this:\\n    ```python\\n    response = genai.generate_text(\"Your prompt here\")\\n    print(response)\\n    ```\\n\\n- **Explore Capabilities**: Gemini can handle various input types, including text, images, and audio. Check the [official documentation](https://ai.google.dev/gemini-api/docs) for more features and examples.\\n\\n### 3. AWS Nova Model\\nTo use the Amazon Nova model, follow these steps:\\n\\n- **Access AWS Bedrock**: Ensure you have access to AWS Bedrock, where the Nova models are hosted. You may need to set up an account and request access if you do not have it yet.\\n\\n- **Navigate to the Model Access**:\\n  - Open AWS Bedrock in the US West (Oregon) region and locate the Model Access section in the left menu.\\n\\n- **Choose the Model**: You can select from different Nova models, such as Nova Canvas for image generation or Nova Reel for video generation.\\n\\n- **Make API Calls**:\\n  - Use the AWS SDK or the API directly to integrate the Nova models into your applications. For example, you can send requests to generate images or videos based on your input text.\\n\\n- **Explore Use Cases**: Nova models are suitable for various tasks, including video summarization, Q&A, and content generation. Check the [AWS Nova page](https://aws.amazon.com/ai/generative-ai/nova/) for more information on capabilities and examples.\\n\\nFor detailed instructions, you can refer to [this guide](https://ndurner.github.io/amazon-nova).\\n\\nBy following these steps, you can effectively utilize the latest AI models from OpenAI, Google, and AWS in your projects.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 782, 'prompt_tokens': 4098, 'total_tokens': 4880, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2560}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-c576854e-e042-4807-bf21-ce8a05a1149e-0', usage_metadata={'input_tokens': 4098, 'output_tokens': 782, 'total_tokens': 4880, 'input_token_details': {'audio': 0, 'cache_read': 2560}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}, 'thread_id': '1', 'step': 21, 'parents': {}}, created_at='2024-12-05T13:23:43.139426+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1efb30c1-db0f-6c25-8014-5f9be1012914'}}, tasks=())"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
