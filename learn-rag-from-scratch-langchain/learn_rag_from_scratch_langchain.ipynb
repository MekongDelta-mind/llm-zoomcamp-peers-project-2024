{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG From Scratch\n",
        "\n",
        "## Resources\n",
        "- [Youtube Course](https://www.youtube.com/watch?v=sVcwVQRHIc8)\n",
        "- [Github](https://github.com/langchain-ai/rag-from-scratch)\n",
        "- [LangChain -Freecodecamp](https://www.freecodecamp.org/news/beginners-guide-to-langchain/)\n"
      ],
      "metadata": {
        "id": "7XTGc5vjDM2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to Use LangChain to Build With LLMs – A Beginner's Guide\n",
        "\n",
        "- [LangChain - Python Library](https://python.langchain.com/v0.2/docs/introduction/)"
      ],
      "metadata": {
        "id": "bcUIE2M9JiuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project Setup"
      ],
      "metadata": {
        "id": "yOapP8ZiL31b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPCgi9XsBbBG"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain_core langchain_anthropic"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Export ANTHROPIC_API_KEY"
      ],
      "metadata": {
        "id": "5vqkyRzENvpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!export ANTHROPIC_API_KEY=sk-ant-api03-.............."
      ],
      "metadata": {
        "id": "U_Mg-bK5DJbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get(\"ANTHROPIC_API_KEY\")\n",
        "\n",
        "# print(api_key)\n"
      ],
      "metadata": {
        "id": "xTMsQYRgQB_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_anthropic import ChatAnthropic"
      ],
      "metadata": {
        "id": "sfZXokGhN9cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model = ChatAnthropic(\n",
        "    model=\"claude-3-sonnet-20240229\",\n",
        "    temperature=0,\n",
        "    api_key=api_key\n",
        ")"
      ],
      "metadata": {
        "id": "OYxgbjpgPJ-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `model` parameter is a string that matches one of [Anthropic’s supported models](https://docs.anthropic.com/claude/docs/models-overview#model-comparison). At the time of writing, Claude 3 Sonnet strikes a good balance between speed, cost, and reasoning capability.\n",
        "\n",
        "`temperature` is a measure of the amount of randomness the model uses to generate responses. For consistency, in this tutorial, we set it to `0` but you can experiment with higher values for creative use cases.\n",
        "\n",
        "Now, let’s try running it:"
      ],
      "metadata": {
        "id": "cPmVm2sZPngp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.invoke(\"Tell me a joke about bears!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7CpssbuPJ76",
        "outputId": "f4c29240-68a7-421d-e6fb-91181466b739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Here's a bear joke for you:\\n\\nWhy did the bear dissolve in water?\\nBecause it was a polar bear!\", response_metadata={'id': 'msg_01LZCLdUs6i6v2PBZHsffYAW', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 14, 'output_tokens': 30}}, id='run-1db9df40-af4a-45dc-ad79-4c9197d1a9c1-0', usage_metadata={'input_tokens': 14, 'output_tokens': 30, 'total_tokens': 44})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage"
      ],
      "metadata": {
        "id": "c_jNft5uPJ5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.invoke([\n",
        "    HumanMessage(\"Tell me a joke about bears!\")\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXc815nUPJ2c",
        "outputId": "c4e9e949-3bbe-46f6-de25-888ca8236528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Here's a bear joke for you:\\n\\nWhy did the bear dissolve in water?\\nBecause it was a polar bear!\", response_metadata={'id': 'msg_01Pbwx3fQ8oFse5PHxjbSirq', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 14, 'output_tokens': 30}}, id='run-2a62538b-c9a6-4d83-974d-4e6bc9599179-0', usage_metadata={'input_tokens': 14, 'output_tokens': 30, 'total_tokens': 44})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Templates\n",
        "\n",
        "Models are useful on their own, but it’s often convenient to parameterize inputs so that you don’t repeat boilerplate. LangChain provides [Prompt Templates](https://python.langchain.com/docs/modules/model_io/prompts/) for this purpose.\n",
        "\n",
        "![prompts](https://www.freecodecamp.org/news/content/images/2024/04/prompt_and_model--1-.png)"
      ],
      "metadata": {
        "id": "ymtCZ0ApUhtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt templates in LangChain\n",
        "\n",
        "A simple example would be something like this:"
      ],
      "metadata": {
        "id": "kc022LqSVF1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "xYIm7FY6PJzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joke_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a world class comedian.\"),\n",
        "    (\"human\", \"Tell me a joke about {topic}\")\n",
        "])"
      ],
      "metadata": {
        "id": "llONJUVPPJwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can apply the templating using the same .invoke() method as with Chat Models:"
      ],
      "metadata": {
        "id": "dgT8GzcJWEZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joke_prompt.invoke({\"topic\": \"beats\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zWFGOO8PJt3",
        "outputId": "9c3e5d01-67bd-49a8-bda5-ccb66719ef99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are a world class comedian.'), HumanMessage(content='Tell me a joke about beats')])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s go over each step:\n",
        "\n",
        "- You construct a prompt template consisting of templates for a `SystemMessage` and a `HumanMessage` using `from_messages`.\n",
        "- You can think of `SystemMessages` as meta-instructions that are not part of the current conversation, but purely guide input.\n",
        "- The prompt template contains `{topic}` in curly braces. This denotes a required parameter named `\"topic\"`.\n",
        "- You invoke the prompt template with a dict with a key named `\"topic\"` and a value `\"beets\"`.\n",
        "- The result contains the formatted messages.\n",
        "\n",
        "Next, we'll learn how to use this prompt template with your Chat Model.\n",
        "\n",
        "\n",
        "### Chaining\n",
        "You may have noticed that both the Prompt Template and Chat Model implement the `.invoke()` method. In LangChain terms, they are both instances of [Runnables](https://python.langchain.com/docs/expression_language/interface/).\n",
        "\n",
        "You can compose Runnables into “chains” using the pipe (`|`) operator where you `.invoke()` the next step with the output of the previous one. Here’s an example:"
      ],
      "metadata": {
        "id": "p-2qE5BaWkcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = joke_prompt | chat_model"
      ],
      "metadata": {
        "id": "z-ObTlyKPJrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMcf5wG2PJoH",
        "outputId": "7c9acc49-531f-40d3-f6e2-97c883602ced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['topic'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a world class comedian.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='Tell me a joke about {topic}'))])\n",
              "| ChatAnthropic(model='claude-3-sonnet-20240229', temperature=0.0, anthropic_api_url='https://api.anthropic.com', anthropic_api_key=SecretStr('**********'), _client=<anthropic.Anthropic object at 0x7a0958f26620>, _async_client=<anthropic.AsyncAnthropic object at 0x7a0958f26dd0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting `chain` is itself a Runnable and automatically implements `.invoke()` (as well as several other methods, as we’ll see later). This is the foundation of [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language/get_started/).\n",
        "\n",
        "Let’s invoke this new chain:"
      ],
      "metadata": {
        "id": "dYFCHtFKX02t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"topic\": \"beets\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ON5cmOXePJi7",
        "outputId": "3d8e13e3-8457-4375-90dd-dbdc7193698d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Here's a beet joke for you:\\n\\nWhy did the beet blush? Because it saw the salad dressing!\", response_metadata={'id': 'msg_01PJML6wq7vAes7sWoeUF5zn', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 21, 'output_tokens': 30}}, id='run-70636f8f-8d4b-42e7-8ab2-543badca3cfc-0', usage_metadata={'input_tokens': 21, 'output_tokens': 30, 'total_tokens': 51})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s say you want to work with just the raw string output of the message. LangChain has a component called an [Output Parser](https://python.langchain.com/docs/modules/model_io/output_parsers/), which, as the name implies, is responsible for parsing the output of a model into a more accessible format. Since composed chains are also Runnable, you can again use the pipe operator:"
      ],
      "metadata": {
        "id": "rwqfjvhubimv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "yj4zhqQ4PJgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "str_chain = chain | StrOutputParser()\n",
        "\n",
        "# Equivalent to:\n",
        "# str_chain = joke_prompt | chat_model | StrOutputParser()"
      ],
      "metadata": {
        "id": "kplccMAgPJdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joke = str_chain.invoke({\"topic\": \"beets\"})\n",
        "joke"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SbRs5MpVPJa3",
        "outputId": "03ecf789-35a8-4312-d577-7c7a18db5a43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Here's a beet joke for you:\\n\\nWhy did the beet blush? Because it saw the salad dressing!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(joke)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6cuST_5PJYR",
        "outputId": "24073773-e0ac-4382-878c-342834184ef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a beet joke for you:\n",
            "\n",
            "Why did the beet blush? Because it saw the salad dressing!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You still pass `{\"topic\": \"beets\"}` as input to the new `str_chain` because the first Runnable in the sequence is still the Prompt Template you declared before.\n",
        "\n",
        "**Prompt model and output parser**\n",
        "\n",
        "![prompt chain](https://www.freecodecamp.org/news/content/images/2024/04/prompt_model_and_output_parser--1-.png)"
      ],
      "metadata": {
        "id": "kBzAVndMfvxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streaming\n",
        "One of the biggest advantages to composing chains with LCEL is the streaming experience.\n",
        "\n",
        "All Runnables implement the `.stream()` method (and `.astream()` if you’re working in async environments), including chains. This method returns a generator that will yield output as soon as it’s available, which allows us to get output as quickly as possible.\n",
        "\n",
        "While every Runnable implements `.stream()`, not all of them support multiple chunks. For example, if you call `.stream()` on a Prompt Template, it will just yield a single chunk with the same output as `.invoke()`.\n",
        "\n",
        "You can iterate over the output using `for ... in` syntax. Try it with the `str_chain` you just declared:"
      ],
      "metadata": {
        "id": "l0GREjhyglen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in str_chain.stream({\"topic\": \"beets\"}):\n",
        "  print(chunk, end=\"|\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fei-ZsrPJVo",
        "outputId": "78951f40-20dc-46ab-bb57-a88b2708af40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|Here|'s a b|eet joke for| you:\n",
            "\n",
            "Why| did the beet| bl|ush? Because it| saw| the sal|ad dressing!||"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chains composed like `str_chain` will start streaming as early as possible, which in this case is the Chat Model in the chain.\n",
        "\n",
        "Some Output Parsers (like the `StrOutputParser` used here) and many LCEL [Primitives](https://python.langchain.com/docs/expression_language/primitives/) are able to process streamed chunks from previous steps as they are generated – essentially acting as transform streams or passthroughs – and do not disrupt streaming.\n",
        "\n",
        "### How to Guide Generation with Context\n",
        "LLMs are trained on large quantities of data and have some innate “knowledge” of various topics. Still, it’s common to pass the model private or more specific data as context when answering to glean useful information or insights. If you've heard the term \"RAG\", or \"retrieval-augmented generation\" before, this is the core principle behind it.\n",
        "\n",
        "One of the simplest examples of this is telling the LLM what the current date is. Because LLMs are snapshots of when they are trained, they can’t natively determine the current time. Here’s an example:"
      ],
      "metadata": {
        "id": "R6b1hso_huar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# chat_model = ChatAnthropic(model_name=\"claude-3-sonnet-20240229\")\n",
        "\n",
        "chat = chat_model.invoke(\"What is the current date?\")\n",
        "chat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcr3iUOqPJSy",
        "outputId": "d4be63af-edd8-4165-d3a1-bd36bf7e76ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Unfortunately, I don't actually have a concept of the current date and time. As an AI assistant without an integrated calendar, I don't have a way to track the specific date. I can only provide responses based on the conversational context provided to me.\", response_metadata={'id': 'msg_019MwjWTjy99fSN3WUCoYEkX', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 55}}, id='run-afb5a49d-9b31-4780-8c09-e1d7b51265b8-0', usage_metadata={'input_tokens': 13, 'output_tokens': 55, 'total_tokens': 68})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "zRvmUF1oPJP7",
        "outputId": "877fd180-1ddd-455b-c6a2-7ee2e14fa764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Unfortunately, I don't actually have a concept of the current date and time. As an AI assistant without an integrated calendar, I don't have a way to track the specific date. I can only provide responses based on the conversational context provided to me.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s see what happens when you give the model the current date as context:"
      ],
      "metadata": {
        "id": "5W7iRHdJk7M-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import date"
      ],
      "metadata": {
        "id": "LXr8sZK3PJND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You know that the current date is '{current_date}'.\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "chain = prompt | chat_model | StrOutputParser()\n",
        "\n",
        "chain.invoke({\n",
        "    \"question\": \"What is the current date?\",\n",
        "    \"current_date\": date.today()\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "B4s_14ZjPJKf",
        "outputId": "a26d611d-e586-49f3-dd3f-2c55d83973c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current date is 2024-08-16.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice! Now, let's take it a step further. Language models are trained on vast quantities of data, but they don't know everything. Here's what happens if you directly ask the Chat Model a very specific question about a local restaurant:"
      ],
      "metadata": {
        "id": "J7nwH1RTl7tW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.invoke(\n",
        "    \"What was the Old Ship Saloon's total revenue in Q1 2023?\"\n",
        " )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhxWlk8cPJHk",
        "outputId": "e4a59333-f843-4e7f-83d3-195a0c448e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm sorry, I don't have access to specific financial data for a particular business like the Old Ship Saloon. As an AI assistant without direct connections to private company records, I don't have information about their revenues or other confidential financial details.\", response_metadata={'id': 'msg_015qFped3YvjAeSxv3dt1pUx', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 25, 'output_tokens': 55}}, id='run-bb3a8b2f-4fd3-4fcf-bdf9-54149c96fd08-0', usage_metadata={'input_tokens': 25, 'output_tokens': 55, 'total_tokens': 80})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model doesn't know the answer natively, or even know which of the many Old Ship Saloons in the world we may be talking about:\n",
        "\n",
        "However, if we can give the model more context, we can guide it to come up with a good answer:"
      ],
      "metadata": {
        "id": "Ds-V_p2JmMt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE = \"\"\"\n",
        "Old Ship Saloon 2023 quarterly revenue numbers:\n",
        "Q1: $174782.38\n",
        "Q2: $467372.38\n",
        "Q3: $474773.38\n",
        "Q4: $389289.23\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", 'You are a helpful assistant. Use the following context when responding:\\n\\n{context}.'),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "rag_chain = rag_prompt | chat_model | StrOutputParser()\n",
        "\n",
        "rag_chain.invoke({\n",
        "    \"question\": \"What was the Old Ship Saloon's total revenue in Q1 2023?\",\n",
        "    \"context\": SOURCE\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gMzF_15uPJEu",
        "outputId": "78b94d66-9c44-4ad7-e782-b1c44dbf5764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"According to the provided context, the Old Ship Saloon's revenue in Q1 2023 was $174,782.38.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result looks good! Note that augmenting generation with additional context is a very deep topic - in the real world, this would likely take the form of a longer financial document or portion of a document retrieved from some other data source. RAG is a powerful technique to answer questions over large quantities of information.\n",
        "\n",
        "You can check out [LangChain’s retrieval-augmented generation (RAG) docs](https://python.langchain.com/docs/use_cases/question_answering/) to learn more."
      ],
      "metadata": {
        "id": "utrhWvHPm6Q9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Debugging\n",
        "Because LLMs are non-deterministic, it becomes more and more important to see the internals of what’s going on as your chains get more complex.\n",
        "\n",
        "LangChain has a `set_debug()` method that will return more granular logs of the chain internals: Let’s see it with the above example.\n",
        "\n",
        "First, we'll need to install the main `langchain` package for the entrypoint to import the method:"
      ],
      "metadata": {
        "id": "8xfQZFbBnnRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain"
      ],
      "metadata": {
        "id": "iWNWxsXWPJCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.globals import set_debug\n",
        "\n",
        "set_debug(True)\n",
        "\n",
        "from datetime import date\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", 'You know that the current date is \"{current_date}\".'),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "chain = prompt | chat_model | StrOutputParser()\n",
        "\n",
        "chain.invoke({\n",
        "    \"question\": \"What is the current date?\",\n",
        "    \"current_date\": date.today()\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YdaskSoAPI_R",
        "outputId": "2dfe0c9f-2293-4876-fcc1-c6b84b39a0fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [2ms] Exiting Prompt run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatAnthropic] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: You know that the current date is \\\"2024-08-16\\\".\\nHuman: What is the current date?\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatAnthropic] [698ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"The current date is 2024-08-16.\",\n",
            "        \"generation_info\": null,\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"The current date is 2024-08-16.\",\n",
            "            \"response_metadata\": {\n",
            "              \"id\": \"msg_01Eeuo1JCELTr8tdDgTVsahV\",\n",
            "              \"model\": \"claude-3-sonnet-20240229\",\n",
            "              \"stop_reason\": \"end_turn\",\n",
            "              \"stop_sequence\": null,\n",
            "              \"usage\": {\n",
            "                \"input_tokens\": 28,\n",
            "                \"output_tokens\": 15\n",
            "              }\n",
            "            },\n",
            "            \"type\": \"ai\",\n",
            "            \"id\": \"run-94b73623-97b3-4b6a-a91e-364936b2419e-0\",\n",
            "            \"usage_metadata\": {\n",
            "              \"input_tokens\": 28,\n",
            "              \"output_tokens\": 15,\n",
            "              \"total_tokens\": 43\n",
            "            },\n",
            "            \"tool_calls\": [],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"id\": \"msg_01Eeuo1JCELTr8tdDgTVsahV\",\n",
            "    \"model\": \"claude-3-sonnet-20240229\",\n",
            "    \"stop_reason\": \"end_turn\",\n",
            "    \"stop_sequence\": null,\n",
            "    \"usage\": {\n",
            "      \"input_tokens\": 28,\n",
            "      \"output_tokens\": 15\n",
            "    }\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"The current date is 2024-08-16.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [706ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"The current date is 2024-08-16.\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current date is 2024-08-16.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see this [guide](https://python.langchain.com/docs/guides/development/debugging/) for more information on debugging.\n",
        "\n",
        "You can also use the `astream_events()` [method](https://python.langchain.com/docs/expression_language/streaming/#using-stream-events) to return this data. This is useful if you want to use intermediate steps in your application logic. Note that this is an async method, and requires an extra `version` flag since it’s still in beta:"
      ],
      "metadata": {
        "id": "ckCFbpxrok-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn off debug mode for clarity\n",
        "set_debug(False)\n",
        "\n",
        "async def astream_events():\n",
        "  stream = chain.astream_events({\n",
        "      \"question\": \"What is the current date?\",\n",
        "      \"current_date\": date.today()\n",
        "  }, version=\"v1\")\n",
        "\n",
        "  async for event in stream:\n",
        "      print(event)\n",
        "      print(\"-----\")"
      ],
      "metadata": {
        "id": "fpiO73cKPI8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await astream_events()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHcY_0s8PI50",
        "outputId": "f4d6baf7-e8dd-44ca-8f84-2658b8685050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'event': 'on_chain_start', 'run_id': 'c924aadc-b89a-4818-ae83-0619c4beea15', 'name': 'RunnableSequence', 'tags': [], 'metadata': {}, 'data': {'input': {'question': 'What is the current date?', 'current_date': datetime.date(2024, 8, 16)}}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_prompt_start', 'name': 'ChatPromptTemplate', 'run_id': '37a31e6c-47e7-41fb-aeaf-cfb85808126a', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'question': 'What is the current date?', 'current_date': datetime.date(2024, 8, 16)}}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_prompt_end', 'name': 'ChatPromptTemplate', 'run_id': '37a31e6c-47e7-41fb-aeaf-cfb85808126a', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'question': 'What is the current date?', 'current_date': datetime.date(2024, 8, 16)}, 'output': ChatPromptValue(messages=[SystemMessage(content='You know that the current date is \"2024-08-16\".'), HumanMessage(content='What is the current date?')])}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chat_model_start', 'name': 'ChatAnthropic', 'run_id': '779601d2-d8c1-444f-a1df-b46322b3e9b3', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'data': {'input': {'messages': [[SystemMessage(content='You know that the current date is \"2024-08-16\".'), HumanMessage(content='What is the current date?')]]}}, 'parent_ids': []}\n",
            "-----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: This API is in beta and may change in the future.\n",
            "  warn_beta(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': '779601d2-d8c1-444f-a1df-b46322b3e9b3', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content='', id='run-779601d2-d8c1-444f-a1df-b46322b3e9b3', usage_metadata={'input_tokens': 28, 'output_tokens': 0, 'total_tokens': 28})}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_parser_start', 'name': 'StrOutputParser', 'run_id': 'd96de59a-630b-4279-93c9-0773394d24a7', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_parser_stream', 'name': 'StrOutputParser', 'run_id': 'd96de59a-630b-4279-93c9-0773394d24a7', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'chunk': ''}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chain_stream', 'run_id': 'c924aadc-b89a-4818-ae83-0619c4beea15', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': ''}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': '779601d2-d8c1-444f-a1df-b46322b3e9b3', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content='The current date is', id='run-779601d2-d8c1-444f-a1df-b46322b3e9b3')}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_parser_stream', 'name': 'StrOutputParser', 'run_id': 'd96de59a-630b-4279-93c9-0773394d24a7', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'chunk': 'The current date is'}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chain_stream', 'run_id': 'c924aadc-b89a-4818-ae83-0619c4beea15', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': 'The current date is'}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': '779601d2-d8c1-444f-a1df-b46322b3e9b3', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content=' 2024-', id='run-779601d2-d8c1-444f-a1df-b46322b3e9b3')}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_parser_stream', 'name': 'StrOutputParser', 'run_id': 'd96de59a-630b-4279-93c9-0773394d24a7', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'chunk': ' 2024-'}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chain_stream', 'run_id': 'c924aadc-b89a-4818-ae83-0619c4beea15', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': ' 2024-'}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': '779601d2-d8c1-444f-a1df-b46322b3e9b3', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content='08-16.', id='run-779601d2-d8c1-444f-a1df-b46322b3e9b3')}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_parser_stream', 'name': 'StrOutputParser', 'run_id': 'd96de59a-630b-4279-93c9-0773394d24a7', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'chunk': '08-16.'}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chain_stream', 'run_id': 'c924aadc-b89a-4818-ae83-0619c4beea15', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': '08-16.'}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': '779601d2-d8c1-444f-a1df-b46322b3e9b3', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content='', response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None}, id='run-779601d2-d8c1-444f-a1df-b46322b3e9b3', usage_metadata={'input_tokens': 0, 'output_tokens': 15, 'total_tokens': 15})}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_parser_stream', 'name': 'StrOutputParser', 'run_id': 'd96de59a-630b-4279-93c9-0773394d24a7', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'chunk': ''}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chain_stream', 'run_id': 'c924aadc-b89a-4818-ae83-0619c4beea15', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': ''}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chat_model_end', 'name': 'ChatAnthropic', 'run_id': '779601d2-d8c1-444f-a1df-b46322b3e9b3', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'data': {'input': {'messages': [[SystemMessage(content='You know that the current date is \"2024-08-16\".'), HumanMessage(content='What is the current date?')]]}, 'output': {'generations': [[{'text': 'The current date is 2024-08-16.', 'generation_info': None, 'type': 'ChatGenerationChunk', 'message': AIMessageChunk(content='The current date is 2024-08-16.', response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None}, id='run-779601d2-d8c1-444f-a1df-b46322b3e9b3', usage_metadata={'input_tokens': 28, 'output_tokens': 15, 'total_tokens': 43})}]], 'llm_output': None, 'run': None}}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_parser_end', 'name': 'StrOutputParser', 'run_id': 'd96de59a-630b-4279-93c9-0773394d24a7', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'input': AIMessageChunk(content='The current date is 2024-08-16.', response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None}, id='run-779601d2-d8c1-444f-a1df-b46322b3e9b3', usage_metadata={'input_tokens': 28, 'output_tokens': 15, 'total_tokens': 43}), 'output': 'The current date is 2024-08-16.'}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chain_end', 'name': 'RunnableSequence', 'run_id': 'c924aadc-b89a-4818-ae83-0619c4beea15', 'tags': [], 'metadata': {}, 'data': {'output': 'The current date is 2024-08-16.'}, 'parent_ids': []}\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, you can use an external service like [LangSmith](https://smith.langchain.com/) to add tracing. Here’s an example:"
      ],
      "metadata": {
        "id": "vOOtCh4xp3Ht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -U langsmith"
      ],
      "metadata": {
        "id": "EGKEbcmLtiFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign up at <https://smith.langchain.com/>\n",
        "# Set environment variables\n",
        "\n",
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "set_debug(False)\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = f\"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"pr-somber-escalator-100\"\n",
        "\n",
        "chain.invoke({\n",
        "  \"question\": \"What is the current date?\",\n",
        "  \"current_date\": date.today()\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Gvd-MCYFPI3L",
        "outputId": "bf26501c-15d4-4b38-97b4-da50267e4caf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current date is 2024-08-16.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangSmith will capture the internals at each step, giving you a result [like this](https://smith.langchain.com/public/628a15bb-45c8-4d39-987a-2896684a66c2/r).\n",
        "\n",
        "We can also tweak prompts and rerun model calls in a playground. Due to the non-deterministic nature of LLMs, you can also tweak prompts and rerun model calls in a playground, as well as create datasets and test cases to evaluate changes to your app and catch regressions."
      ],
      "metadata": {
        "id": "_ic_72EH3Qx1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qsBSFIwQPI0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "raWGseeQPIx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wPaV9tMgPIvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aA03Hi5EPIsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ite-VPgPPIpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jJHd6TGvPImQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-e7vByz_PIi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "661sowJKPIfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ELFs_J5VPIbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OvAbAbKqPITh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p_k7y_xdPIKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oVEZdueLPIAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W52VLiQkDKBx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}