{
    "52ecatIqjUA": "Welcome to the Real Python Podcast. This is episode 133. What if you didn't have to worry about managing user passwords as a Python developer? That's where the WebAuthn protocol and new hardware standards are heading. This week on the show, Dan Moore from Fusion Auth returns to discuss a passwordless future. WebAuthn is a way to authenticate users using biometric secure authentication methods. Dan dives into pass keys, ceremonies, authenticators, and hardware standards. We also cover several projects and libraries that could help you get started with WebAuthn in Python.\n\nThis episode is brought to you by CData Software, the easiest way to connect Python with data. SQL access to more than 250 cloud applications and data sources. Alright, let's get started.\n\n[Music]\n\nThe Real Python Podcast is a weekly conversation about using Python in the real world. My name is Christopher Bailey, your host. Each week, we feature interviews with experts in the community and discussions about the topics, articles, and courses found at realpython.com. After the podcast, join us and learn real-world Python skills with a community of experts at realpython.com.\n\nHey Dan, welcome back to the show.\n\nHey, thanks for having me.\n\nYeah, so we were talking back in episode 99, diving into OAuth and setting that up. You recently reached out to me after I guess you had just written this article about WebAuthn. Is that how it's pronounced?\n\nYeah, I pronounce it \"WebAuthn\" for sure. Good, because there's lots of acronyms and stuff we'll be diving into today. I know it's not the deepest Python subject, but I am fascinated by this technology and I want to know more about it. I think anybody who develops stuff on the web should have this on their radar, hopefully getting us beyond a password-based future.\n\nYeah, it definitely makes it easy to leverage strong forms of authentication like biometrics or things that are built into your phone fingerprint readers, etc., into a web application without you having to do the heavy integration lifting. That's why I'm really excited about it.\n\nYeah, I think that's really cool and it attacks the problem from a slightly different angle. We're going to provide a lot of resources, and your article was really great. It was a WebAuthn explained on your Fusion Auth blog. Is it just Fusion Auth developer expert advice for developers sort of series there?\n\nSo we're going to have lots of little acronyms. We're going to kind of work through, talk about maybe some differences there. What I want to start with first is I had heard about, I think it's called the Fido Alliance, and then heard more about WebAuthn after that. Maybe you could describe the difference between those two to get some people up to speed as to what's happening there.\n\nSure, the Fido Alliance started in the early 2010s, and I think PayPal was a member and some other folks. Fido stands for Fast Identity Online. They were a group of companies that realized we needed to have better ways to authenticate quickly, mostly because they wanted users to have a better and more secure experience. They came out with a couple of standards, and one thing that may be familiar to some of your listeners are UB keys.\n\nYeah, I was thinking about that.\n\nSo in the mid-2010s, the Fido folks got together with the folks at the W3C, I want to say 2016-2017, and came up with a second version of this passwordless system. This one leverages some of the work they've previously done, and there's a portion of it that's built into the browser. You can imagine there's a pipeline between the end user, some kind of authentication device like a UB key or face ID, and the browser and then a website. WebAuthn handles the interactions between the browser and the website, and then there's a second protocol called CTAP, which handles communication between the browser and the authenticator, which is the thing that actually does the authentication, secrets management, and some other things. That's the secret sauce behind why WebAuthn is passwordless and secure.\n\nIt's worth noting that CTAP can be actually used not just by a browser, but it could be like a desktop application or something like that.\n\nOkay, so we were talking about Apple TVs and some of these other third devices in the home. They could be used for something like that too?\n\nI think those third devices are probably if they're actual real applications, then yes. If they're web applications, then they're probably going to be using WebAuthn.Okay, maybe we could describe what a Yubikey is. The other question I have is how many of them are out there? How popular are they? There was a recent action in the Python space where the Python Packaging Authority took the top one percent of all packaging projects that are most downloaded and offered each of those projects a Yubikey. They could still use another form of two-factor or multi-factor authentication if they chose not to, but they wanted to ensure all those projects at least had that option.\n\nI was intrigued by this. Is it something huge in the corporate world? I don't have a great grasp on that. I know Yubikeys are aimed at consumers, mostly serious about security. All browsers and operating systems support it, which is exciting. Hardware like Android fingerprint, Face ID, Touch ID, Windows Hello, all leverage the same WebAuth APIs.\n\nIf you're looking at consumer logins, asking someone to buy a Yubikey is a lift. They're not super expensive, maybe $50 to $60 depending on the type of connector. Many people already have phones or computers, so that's where Yubikey shines.\n\nThe surrounding ecosystem is a step above what Fido was doing in the early 2010s because there's more hardware available. Touch ID was first made available in 2013, and the first phone with a built-in fingerprint reader was the Motorola Atrix in 2011. Android phones also have some sort of visual recognition or fingerprint.\n\nTo set up Yubikey, you need to enable fingerprint recognition on your device. Web apps work with public and private keys, with the private keys held on the device. There are different ways to associate the public key with your account, like a registration flow or using single sign-on.\n\nWe'll talk about the risks of existing standards as we go, like signing on with Google or GitHub.Well, I'll just have a single sign-on for all those, and that was one of the solutions that has been out there for a while for trying to remember lots of passwords. Yes, you basically delegate that because I don't know about you, Christopher, but like I pay a lot more attention to my Gmail account than I do to the jbandom site. Yeah, I have MFA enabled for it. So, that's the premise behind single sign-on or social sign-on. There are ways you can set up web auth so that you don't have to have that initial account creation. They're just a little more lit in terms of the way the hardware works. My experience is definitely more focused on the re-authentication flow right now because at the end of the day, it really is about reducing user friction and authentication is a big friction point in my life.\n\nMaybe we could dive into that a little bit further, like the idea of a public key and a private key, which is a kind of similar behavior to exploring HTTPS and that workflow. In fact, in your article, you use the same kind of example with Alice and Bob, which is sort of like a textbook example. Now, we're doing it with this other form of authentication. The idea of you sending this public key out and then only you are able to create something that is privately signed. It's very hard to tamper with because it won't look like you anymore. You have to sort of prove that you, as the username, please sign this challenge with your private key and here's the signature. Maybe the piece of hardware I was talking about is called an authenticator. That's a generic term for anything that holds the private keys and can verify who the user is. And then there's the ceremony of it.\n\nI thought of an analogy. You've given out your username, this public key, and it's like you've given the entire world a flashlight that can read your invisible ink only. So, I can write in that invisible ink, and if someone else tries to write with a different kind of visible ink, that flashlight won't show them the message or it's going to be easy to tell it's been tampered with. It's like having your own unique way of looking at it.\n\nOne of the things I found interesting is the chart mentioning browser support and where things are with authentication API stuff. I was wondering if Internet Explorer was holding this back in some ways. That chart is from caniuse.com, which is an amazing website for specifics. I.E. probably decided that they were all in on Edge and it wasn't worth doing that update.\n\nThis helps with the idea that you are no longer holding onto passwords and the whole ritual potentially saving passwords in your system. That's why people prefer to use single sign-on on their sites.Well, the third party can authenticate without needing to collect all that information. The whole process of ensuring proper hashing or salting can still result in information being attacked or taken from the site, as we often hear about breaches. In this case, there's no secret saved, just the public key, which is something no one cares about, similar to a username.\n\nWithout any server-side secrets, it becomes less of an attack vector. The website has introduced other measures as well, making it impossible to use Bubba FN over HTTP. Each registration ceremony binds the public key to a domain name, making it harder for attackers to gain access to credentials or signed payloads.\n\nPhishing attacks become more challenging with webAuthn, as it ties authentication to the domain name system. This makes it harder for attackers to impersonate users or steal credentials. Code-based MFA can be phished for, but with webAuthn, the signature is bound to the device, making it more secure.\n\nI have an Instagram account with a single dictionary word as the username. Despite not using it much, I constantly receive emails about trouble accessing the account. Without setting up SMS-based authentication, the account remains secure, as SMS can be easily spoofed.\n\nSMS-based authentication is vulnerable to attacks, often involving social engineering to trick mobile operators into switching things around. Moving to more secure multi-factor authentication methods like Google Authenticator or Authy is advisable, despite the challenge of switching devices.\n\nWith webAuthn, each new device needs to be registered, requiring users to go through the registration process again. This ensures security but can be cumbersome when switching devices. Some websites may offer choices for logging in with multiple accounts associated with the user.In the Apple World, they are trying to use iCloud and they have a term called a passkey that they're using. If you're in the Apple ecosystem, I used to work for Apple so I have a lot of their stuff. Plus, I bought a lot of music equipment that only works on it. I was checking out one of the developer accounts and they had a WWDC talk in 2021 titled \"Move Beyond Passwords.\" It was a good explanation of how a hardware provider is looking at setting up their developers for it. They showed a bit of code at the end.\n\nI think what they are working on is still under development or has been released in 2019 and finalized. They are now working on the next version to try to solve some problems. They are working with Apple and Google to ensure that private keys never leave devices, which is a premise of WebAuthn. They are also working on secure storage in the cloud for private keys.\n\nThere are proprietary solutions right now, but the WebAuthn working group is trying to standardize and solve the user interface issue. Best Buy is one consumer-facing site that has implemented it, and I have heard of people using it for their GitHub account or Azure ID. Amazon is also using it, which shows momentum behind it.\n\nIn terms of shared accounts like on Amazon, it would be best to register both devices to avoid any issues with only one device being registered. It's important to have multiple devices registered for security reasons.\n\nThe rollout of this technology is like a chicken and egg situation where hardware needs to be ready for the operating systems, and websites need to figure out how to implement it. Intuit is also using it, which could be very helpful.So, I think it's not a chicken and egg problem because the chicken's already taken care of. For whatever reason, probably because they want to make our lives more secure, all the browser vendors and OS vendors have built this in. Now, it's a real question of the audience, the web developers, stepping up and actually implementing this on their web applications so that users can get the benefits of it. I won't speak too much about the benefits from a user perspective, but the first time I did it, it really seemed like magic because it is. You basically click on a button saying, \"Hey, this is my username essentially,\" and then you get a prompt that comes up. You pull your fingerprint over it and boom, you're logged in. For someone used to continually cutting, copying, pasting from my password manager, it was amazingly frictionless.\n\nI'm not proud of the fact that I have two password managers, but I got a new laptop this year and started to go all in on the touch ID and the iCloud keychain. It does prompt me to use my fingerprint to put in the password and username, which actually works pretty well. I also have one password, partly for work, and honestly, it's kind of clunky. I end up copying and pasting out of it because it would fight with the other one. I know a lot of people probably do the same thing, maybe saving passwords in different browsers, but that's still not completely secure. I don't know if there have been hacks of password manager services like LastPass, but a centralized repository of users' passwords and usernames would be very attractive to an attacker.\n\nWhat about 1Password being decentralized? If web browsers and LastPass were holding all these public keys, no one would care. Can we talk about the Python pieces? Let's move it to the stuck at the developer side because that's really the next step. How do we set this up? I think there are some different choices, being a new platform or technology for developers. I was wondering at what level these things are at, so let's discuss them.There are two distinct pieces to generating the options needed for a particular authenticator to be recognized. This is done by creating options in JavaScript. I found a library that does this for you in Python. In Python, you build what you want in terms of supporting authenticators that are built into a computer or phone (platform authenticators) or cross-platform authenticators like a UB key. This is taken care of by something called Pi_web_authen.\n\nThe other piece is credential management - prompting the user, saving the public key, and knowing how to attach it to the user. I found a framework called Pi_warp that works with Flask and another one called Web_fnrp that works with Fast and Chalice.\n\nI looked into Django, a big web framework with Python, but found two alpha-level projects called Django web auth and Django web often. These projects are still in development.\n\nSetting up multiple choices for authentication might be necessary for creating a Flask website with web authen. This is important for two reasons: having an existing account to hang the public key off of, and for the reset problem in case a device is lost. Self-service reset flows are convenient but can also be an attack vector.\n\nAnother Python tool called python-fido2 by Ubico allows manipulation and access to Fido stuff from the command line, working with hardware keys beyond just UB key specific tools.\n\nImplementing web authen support in an authentication service, like Fusion, is becoming more common among competitors. Keycloak also has a web authen implementation.I'm trying to think of the others that are kind of prominent. Ori does, Stitch does, so there are a couple of competitors out there that are off zero does. Although my understanding is that all zeros is hidden behind a talk to sales type of process. Okay, I think you absolutely could implement this yourself and hopefully some of the links to some of the libraries that will be in this podcast note will do that. But you absolutely could also rely on an off server too.\n\nI was just thinking, just like a state of affairs kind of thing. Like the more this is out there, the more public awareness of it is, and more sites that are using it, and hopefully we can all move toward this passwordless future. I would also say it's not just the passwords of the future that excites me, it's the standards-based password of the future. There have been passwordless solutions out there that have been proprietary for a while longer, but this one has mass adoption.\n\nThe hope is that a lot more websites are going to start to offer this, maybe not the only way to log in, but a safe, secure way to let people re-authenticate or add an additional factor of authentication to just increase the ease and user security for everyone. So Dan, I have these weekly questions I'd like to ask you, and I know that you're not a regular Python developer at all times, but what is something that you're excited about in the world of Python that you've been hearing about?\n\nThere's two things actually. When I was looking around, the first is that PyCon 2023, the US one, just opened their CFP. I speak a lot at different conferences and I'm definitely excited about the possibility of submitting to PyCon. I would encourage anybody to pitch a talk to PyCon 2023 about web authent because I guarantee you if you get accepted, you will suddenly become one of the smartest people in the room about web authent because you'll put in that work.\n\nThe other thing is obviously I love to see program languages evolve and I noticed that 3.11 was released. Congratulations to Python folks for continuing to push that forward. It's pretty exciting, there's lots of fun changes and a lot of development happening. It almost feels too fast sometimes with a yearly release cadence.\n\nOne thing I want to learn is even in the short time we've been building out the lab, I've seen a lot of interesting edge cases. So I'm really looking forward to Fusion off getting that released in the wild and being able to continually refine that. And then I've really enjoyed pickling some things and my wife got me a book about pickling things, so I'm gonna be doing more of that, preserving foods.\n\nI've dabbled more in quick pickling different things like red onions for making fish tacos at home. It's just lime juice, a little bit of red vinegar, thinly cut onions, some salt, shake it up, and they're so good. I've done sauerkraut, kimchi, and tried to pickle turnips the other day. Maybe you can share the book name and we'll put it in the show notes. Well Dan, thanks so much for coming on the show again. It's really fun to talk to you again and thanks for sharing all this information about web authent.Thank you for having me. If I could leave your listeners with one piece of advice, it is to sign up for Best Buy's web account and see how easy and fun it is. Think about your users having that same experience on your website. \n\nDon't forget to see data software for simple cloud data connectivity to SAS, big data, and NoSQL from Pandas, SQL Alchemy, Dash, and Pedal. Learn more at cdata.com.\n\nI want to thank Dan Moore for coming on the show this week, and I want to thank you for listening to the Real Python Podcast. Make sure to click that follow button in your podcast player. If you see a subscribe button somewhere, remember that the Real Python Podcast is free. If you like the show, please leave us a review. \n\nYou can find show notes with links to all the topics we spoke about inside your podcast player or at realpython.com/podcast. While you're there, you can leave us a question or a topic idea. I've been your host Christopher Bailey. I look forward to talking to you soon.",
    "aEU3JNAbPbo": "Welcome to the Real Python Podcast. This is episode 150. What are the core lessons you've learned along your Python development journey? What are key takeaways you would share with new users of the language? This week on the show, Duart Oliveira Carneiro is here to discuss his recent talk, \"Four Years of Python.\" Duart works at the crossroads of machine learning, data science, and software engineering. He began using Python in his graduate studies and never looked back. He wrote a blog post in 2021 about some of the valuable lessons he's learned and decided the lessons and the concepts in the post might make a good conference talk. We cover the steps in this process of crafting the presentation, practicing at a smaller conference, and finally presenting it at PyCon Italia last year. We also dig into the four major themes of the talk, and along the way, we share a collection of resources to help you continue learning on your Python journey.\n\nThis episode is brought to you by Influx Data. The InfluxDB time series platform empowers developers and organizations to build real-time IoT analytics and cloud applications with timestamped data. Learn more at influxdata.com. Alright, let's get started.\n\n[Music]\n\nThe Real Python Podcast is a weekly conversation about using Python in the real world. My name is Christopher Bailey, your host. Each week, we feature interviews with experts in the community and discussions about the topics, articles, and courses found at realpython.com. After the podcast, join us and learn real-world Python skills with a community of experts at realpython.com.\n\nHey Duart, it's good to talk to you. Well, thanks for having me, Chris. Yeah, well, we have a little history, kind of going back. We had at Real Python a bit of an outreach program to try to find a co-host once David Amos had left, and you had applied. So we had done kind of a one-on-one interview, if you will, and talk. I had asked you early on, like, well, I'm not sure who we're gonna pick, and would you like to come on the show? And anyway, at some point, you had said yes, and then I sort of... and I hadn't contacted you for a year. And then I was like, okay, remember me, let's do it. Yeah, totally.\n\nSo I appreciate you reaching out, and you recently did the talk. I think it's a nice kind of coincidence as far as fitting in with the kind of stuff I like to cover on the show. And I think you realized that, and your talk was called \"Four Years of Python.\" I did some research and I found that you had done a blog post first. Maybe talk about doing the blog post.\n\nYeah, exactly. So first of all, I was reading a book called \"Practices of the Python Pro\" by Dan Hillard. Yeah, he was on the show, it's great. I loved that book. It was a really good book, like one of my first when I still got having fun with technical books. I was thinking about... I actually had been programming at that point for about four years, and that I was getting paid for it. And then I started learning some things, like some guidelines. I'd observed some things, and I decided to write a blog post about them. I like writing for my blog, and that's something I like to do regularly. So I thought, okay, why not put together these four main things? And I got super good feedback on the blog post. I think the author even retweeted it, so that was cool. Eventually, what happened is that I wanted to kind of make it into a talk. So basically what happened is that I pitched it to the local PyData Meetup here in Copenhagen, and they accepted it. Okay. And then it ended up being a talk at PyCon Italia last year, and yeah, it was super fun to do it. So yeah, the blog post basically escalated really quickly and turned into a massive thing, but it was super fun to do the talk.\n\nLet me ask you a couple of questions kind of related to that. So first is, of course, why did you start writing a blog? What was your reasoning for creating a blog as a programmer?\n\nThat's a good question. First of all, just having fun. Enough language, static site generators, everyone on the web has a blog and stuff like that. So mainly curiosity, of course. But I also believe, and this is, I think, Simon Wilson, one of the Django contributors. He talks about that when you learn something, a way of internalizing what you've learned is to write about it. It's a little bit like kind of closing the process whenever you learn something. And so I've kind of been using my blog for that. Whenever I learn something cool, I try to write about it. But it's also my blog, so I get to write about whatever I want.\n\nYeah, I mean, you kind of... I think you do mix topics quite a bit.\n\nI like to mix them a bit. I mean, sometimes you'll see things about programming or machine learning, which is my area of focus, but you'll also find things about running, about other stuff that I'm interested in. So, I mean, it's my space.\n\nExactly, like it's my space, so there's no rules, right? So it's... yeah.It's an extravaganza of things I like. That's cool. Yeah, what's the, you said it's a static site. What are the tools you're using? I'm using Python. I'm using Pelican. Okay, great. And I met the maintainer at my first PyCon in Italy. I actually met the maintainer, which is Justin Mayer. Oh, that's so cool. I was like, \"Hey, I mean, I use Pelican, your static site generator and stuff, but yeah, it's a great static site generator. Of course, I had to use Python on the back. That's what Pelican's based on. So yeah, basically that's it. And then there's like some Cloudflare and stuff on top. Yeah, okay. Yeah, nice.\n\nI had an experience early on on the web, and I guess it depends on the site that you try to do this on. But I felt that I was writing or, in my case, I was really doing it through YouTube. I was kind of an early wannabe YouTuber around 2008, 2009. And I created a thing called \"Creativity to Spare.\" It's still out there. It's really ancient, and please don't go look for it. But that's exactly what I'm doing. My problem with it was it was all over the map. I didn't have a focus to it. Like, we talked to affer outline just before I started that I'm into lots of creative different things. Yeah, and so I'm like, how can you do better sound for your videos? How can you light your videos? This is pre-LED tech. It was like right as LEDs were kind of becoming way more popular. I know that sounds strange, but there was a time when they weren't everywhere, and ring lights were new. Like, all that stuff was just kind of really expensive and on the custom. So I was excited to share a lot of this stuff. So I'm like, \"Oh, let's talk about time lapse. Let's talk about audio. Let's talk about making a Rock Band drum kit controller control GarageBand, something like that.\" You know, all these kinds of weird things. The problem is it never got traction, and I always kept hearing from people, they're like, \"Oh, you should just focus on one of those.\" And I've always thought about that for blogs. Like, I am interested in getting into blogging myself pretty soon here, and so I'm intrigued by what you said that you kind of just write about whatever. Do you see lots of ups and downs on it? I mean, that's always going to be the case.\n\nBut yeah, I think I don't know who said this, but I think there's an actor out there. It might be one of the famous Hollywood actors that says, \"You do one for them, and then you do one for you.\" Yeah, I try not to apply that too much, to be very honest. It's a little bit, of course you look at the metrics, and of course you want to have hits on your blog post, but that's not the reason why I write it. Yeah, sure, I write it because it's, I write it more for me than if someone else can relate. Great, that means it was a great article, right? Or if it gets retweeted or people talk about it, that's great. But I also don't do it so much for other people as I do it a lot for me as well. That's why I have, for example, no comments on my website and stuff like this because, yeah, for me, it's the process of putting it out there that is really rewarding to me. And then if it can be rewarding to other people, that's great. But that's not the main point for me to write. Okay, well, that's good. Do you know what I mean? So that's why I'm a little bit protective of, I'll write about whatever I find interesting. But yeah, hopefully I've gotten an audience that has their interests kind of aligned with mine as well, and they might be surprised if they subscribe, basically. Yeah, okay, cool, awesome.\n\nGoing back to the talk, so you had written the blog post, and then did you need to pitch it per se to PyData to get it there? Yeah, so basically what happened is that I wrote the blog post, I got some traction, I was excited about it. Then I had never been to a PyCon in my life, and I wanted to go to my first PyCon. So I applied, I filled in a call for proposals for PyCon Italy, and PyCon Italy has this, I don't know if it's special about PyCon Italy, but they basically have a voting system where everyone can vote on their favorite talks for them, and then they select from the pool of the most voted talks. I completely forgot about my call for proposals. I completely forgot I had pitched this talk, and something like three months later, I got an email saying, \"Congratulations, your talk was accepted.\" And I was like, \"What? I was like, what talk? I'm not ready.\" And what happened after that is that, okay, then if I got accepted, of course I'm going to do the talk, and I used a local PyData Meetup to kind of have rehearsed the talk.We have a great local piety meetup here in Copenhagen, amazingly run by a super passionate guy who organizes awesome meetups. I had the chance to present first in Copenhagen and then a couple of months later in Italy. It was a super good experience, and I would repeat it without any problem.\n\nTo give people an idea of what Pi data is, can you give a little background on that and how you joined it? Yeah, so how I see it, you mean by what the organization is? Yeah, kind of the organization and how they're in different places. You have the PSF, the big Python Software Foundation. I see Pi data as something a little bit more related to NumFOCUS. All these packages like NumPy, Pandas, and others are sponsored or maintained by NumFOCUS, who has a good relationship with Pi data. Pi data is much more related to the scientific and data space. My talks and interests are in Python and the Pi data side, where you get a mix of science and Python instead of just web development.\n\nYou still have a Jupiter conference, right? I think they do something in Paris, but one cool thing they do in Italy and Berlin is they join PyCon and Pi data. My favorite thing about Python conferences is the wide range of topics you can explore, from web development to machine learning to personal finance systems with Python. I love the mix of ideas and the variety of topics at these conferences.\n\nI'm going out of my way this spring to attend a few conferences. I enjoy talking to people from different backgrounds, beginners, and intermediate folks. It's interesting to hear their stories and experiences. I find it really engaging to talk to people who are preparing their first talks and getting ready for the conference. The conference was helpful with preparation. There are many resources available, like PyCon US, which offers mentorship opportunities and guidance on preparing for talks. I had a practice run at a local meetup, which was super helpful for getting feedback and adjusting my presentation.\n\nI spoke too quickly at the first presentation, so I had to include more interesting stories. People enjoyed the conference, so I didn't need to make many adjustments. It also helped increase my confidence, having done it once before. Participating in the local Python community is beneficial for me, as I enjoy being surrounded by people who share my passions. I try to participate in local and international conferences whenever possible.\n\nEurope seems to have a larger grouping of Pi data events, but hopefully, people in the United States can find one close to them. Pi data is a worldwide organization, which is great.Yeah, it is a worldwide organization. Of course, you need to be - I mean, it depends on the city in Europe. I'm very lucky that we have those here. But maybe you have a local Meetup, but we don't have a PyCon Copenhagen. Yeah, there are a bunch of conversations throughout Europe, and of course, Europe is a smaller aggregate of these things, but there are so many cool conferences here. I can't wait to go through some of them this year.\n\nTo take us back a little bit and get a foundation for the ideas that you have in your talk, at the time of the blog post four years was from 2021, is that right? Might be, let me check. I don't know, the blog post was in 2021. Yeah, so I have been programming for about four years, right? Okay, 2017. So, maybe give us a little background. I don't usually do big background stories on the show, but I think it kind of makes sense to cover here. Did you start doing programming in school or how did you get into Python?\n\nYeah, so I moved, my weird name comes from Portugal. I'm originally from Portugal. I live in Copenhagen now, but that's where I did my bachelor's. I did a very general bachelor's in engineering, where I had a class in C++. I did not like it at all, but I have always been very interested in computers. I always liked computers a lot. I got the opportunity to do my Master's abroad, and I could choose between Switzerland and Denmark. In Denmark, they had this thing where they let me select 75 or 50 of my courses. I could select whatever classes I wanted, which is not very common in Europe. Normally, you have an established master program. And one of those, and I said, \"Okay, I'm gonna take the Denmark master's degree because they let me choose whatever I want.\" Flexibility is cool. Exactly. I mean, for me, that's great because I also believe that in your master's degree, you're a little bit more sure about what you like, right? And you're a little bit less scared of going for it. So, I took an introduction to programming, even though I had one before. I took an introduction to programming in Python in my first year of Masters, and I just completely fell in love with it. I remember I have these flagships just being hours and hours on end, just solving the problems that they gave me. And so that's where I got into it. And of course, then I could choose all machine learning and artificial intelligence-related courses in this university. And so, yeah, that was something that stuck and never stopped till this day. It's awesome.\n\nWould you say that the techniques that you're talking about in your talk specifically were things that you developed along the way or realizations kind of after the time?\n\nFor sure, there were things that developed along the way. I think the first two years, when at least I started programming, because of course, I didn't have a background in full computer science, more of a general engineering. A lot of these concepts of testing, good coverage, simplicity of things, those are concepts that I think really start appearing once you start dealing with code in companies. When I left university, I first started with a little more business-focused positions, and I realized that no, no, I really like this thing. So I continued to funnel more and more towards the engineering side as the years went by. I really started learning some of these lessons. I think you need to have some hands-on experience to get some of these learnings, basically. That makes sense, yeah.\n\nDevelopers love the InfluxDB time series platform because it handles large time series datasets and provides low latency SQL queries, which helps them build real-time applications and provides insights that they otherwise miss. InfluxDB Cloud is a performant, elastic, serverless time series platform that can ingest billions of data points, such as metrics, events, and traces in real time, with unbounded cardinality, and store, analyze, and act on that data all in a single database. Check it out and start for free at influxdata.com.\n\nThat kind of covers the background and getting you going and started with Python and your background there. I think in your blog post, there were four ideas in there. I think there's five in the talk. Maybe we could start from the top there. Your first one was \"Reading is better than Googling.\" What do you mean by that?\n\nYeah, so in the blog post, five, I'm not consistent, what can I say? No, that's okay. You got another idea, unless something popped up last minute. No, okay. So, yeah. I think in terms of Googling or reading is better than Googling, I mean, everyone uses Stack Overflow or Google for things all the time. It's really common. I use Stack Overflow regularly daily. And we have this impression that if you're a super expert programmer, you don't use Stack Overflow. I think that's kind of a flawed statement.Yeah, what I realized is I was getting more and more into my work. Whenever you take your time to read the documentation, information tends to stick a bit more. We all have this thing where you check something all the time, just Google it. I have at least two or three of those. But if you read the whole documentation, there's a higher chance that it sticks to your brain.\n\nIt's like a good analogy for me. We live in this information age, and however you read the news, you're bombarded by tutorials all the time. But if you take some time and read a technical book on the subject, your brain is more likely to capture something in a more calm way. That's my concept with reading being better than Googling. If you take a moment to read docs on certain libraries you're using, you might get something to stick more often.\n\nThere's a lot to drill into, like your concept of being in the heat of the moment while programming. Going to documentation, especially for beginners, can be hard. It may look foreign, but it's a skill that needs to be developed. It can be rewarding to get past that initial hump of it looking strange. Engineers often go to the docs to look at how an API functions. Documentation writing is a big subject, and not all docs are created equal.\n\nIn my area of data science and machine learning, there are a lot of default behaviors. Not all documentations are created equal, but some projects have impressive documentation. Taking time to read the docs will give you a better understanding of the power you have.\n\nFor example, scikit-learn has incredible documentation. They provide function definitions, parameter descriptions, examples of how to use it, and methods available. Well-maintained documentations are easier to operate and work with, especially as you gain more experience programming.\n\nThe standard library docs have evolved well over time and are a good resource. They serve as a Rosetta Stone for understanding the language design and layout.I would agree that there are probably unique writing styles depending on the area of Python that you're focusing on. Someone designing web APIs is going to have a certain design and feel, with automatic processes happening. For data science, there may be specific ways that make sense to do things automatically, unless you specifically look for them.\n\nSome of my favorite documentation includes things like Scikit-learn and Pandas. A good user guide goes a long way in explaining how to use something. Scikit-learn stands out as one of those good ones, as well as FastAPI, which is not machine learning-focused but well done. PyTorch is another one that stands out for being well-explained and going the extra mile in documentation.\n\nIn some early conversations on the show, people were asking about good resources to read, like looking at the code repositories on GitHub. It's interesting to see how code is written and decipher it directly, but documentation can also be an easier way to understand.\n\nIt's great to talk about these skills and how they develop as you gain experience. Understanding what's happening underneath the hood is important as your experience grows. There are advantages to really understanding the external dependencies you're adopting.\n\nThere's a lot of discussion around AI tools like GPT and GitHub Copilot. These technologies are incredible, but it's important to take them with a grain of salt. While GPT can sometimes produce unnecessary code or invent API methods that don't exist, Copilot can be more successful with specific use cases and requires your approval for suggestions.\n\nI've recently started using Copilot and find it worth the price for its autocomplete features. It provides context from around my code and is like autocomplete on steroids, but still requires my involvement and approval.\n\nOverall, these tools are very different and serve different purposes, but they can be valuable in assisting with code development.I think that's going to be a later conversation. I'm thinking about bringing some people on to talk about a little bit more. I agree that they kind of go in slightly different directions. I haven't had every little free time moment where I'm like, \"Let me play with chat GPT,\" they're like, \"Nope, sorry can't get on right now.\" So I haven't had much experience. Yeah, I've developed the product with them. So I developed a product that actually offers the opening API and yeah, you got a little more access. It's been a struggle, let me tell you.\n\nSo your next point that you had after reading is keeping things stupid simple, the KISS. Keep it simple, stupid, or whatever. Getting stupid simple. For me, it's a little bit about in the blog post to first make it work, then make it pretty. Those are kind of related too, aren't they? Exactly, exactly. Let me tell you, I hate writing code that doesn't get run. I remember once I was interning in Boston, and my manager, I was working on this algorithm that scanned everything around us and allowed us to sell things quickly. I remember telling him all of these cool things, but he was like, \"Does it work yet?\" And I was like, \"No.\" Then, first make it work. For me, it's a little bit about keeping pragmatism in the things that you do, which might be counterintuitive for more senior engineers. Senior engineers are traumatized by their past experience, so they always try to anticipate whatever is possible. But my point is, even if you try to catch all the exceptions in your code, even if you have 100% code coverage on your tests, do users care that much about it? Users just want it to work, of course, right? But if the app breaks down, they're there, right?\n\nSo for me, it's this concept of trying to be pragmatic about the code that you write or the project that you do. One thing that I've seen in the staff engineers that I work with is that they know what code not to write. That's what a super experienced engineer is, right? They know the use cases that will not solve because you will not cover all of them. There will be exceptions, bugs, errors. So get comfortable and just do the things that really bring value. I feel like we've been kind of moving in that direction in some of the tutorials that we do at Real Python. We've been doing these step-by-step projects. The one I recently covered was about creating a Wordle clone, that word game. He initially got it up on its feet, made it run, and said, \"Okay, this is the core code of functionally what it's going to do. Now let's turn it into a program, refactor it, and do some of the stuff that you were talking about in your talk, the idea of, well, it's really hard to build on this code because it really just does one thing. But if we were to rip it apart and think about functionally what these different sections would do and call this sort of stuff, and do repeatable types of code things, you can kind of start to use some of those techniques. I'm guessing that's where the book you were talking about, Dane Hillard's book, comes in a little bit.\n\nA lot of times, especially in the companies I work with, which I really like to work with smaller startups or things that move a bit faster, it's really hard to anticipate what's going to stick. So if you start thinking about all of the edge cases really early, which is something that a lot of engineers do, you'll write a lot of boilerplate. I'm a little bit afraid of using this word, but you'll learn a lot of conceptual things that might change next week when someone has a different idea. So you shouldn't be afraid of writing something that works, make it work, and then if the use case is proven, then there you go and refactor. I actually did it in a recent work of mine where I built a CLI, and then next week the client wanted an API, and there I was having to refactor things. But at least the use case is proven, right? A lot of the core functioning code is there, and you can modify what's there.\n\nOne of the key things that he talks about in the book, and I had him on the show to dive deeper into it, and I've kind of probed him a lot on this, is what is extensible code, what does that mean?And maybe you can give me your take on it. Always one of my favorite concepts from that book, such a well-written book. Honestly, it's extensible code is about adding a new behavior to your code without changing existing behavior. He gives a good example in the book about web browsers, where you have the concept of extensions. Web browsers are made to be able to download extensions and use those extensions on the browser. The idea is similar, so whenever you have to change something in your code, you don't have to change every single thing about your code. Doing the big rewrite exactly.\n\nOf course, it's normal to do big rewrites when you're starting a project. There's a lot of insertion, for sure, but then your code base will tend to stabilize a bit. You'll hit that sweet spot where adding new functionality is super easy with the way you've thought of your code base. This is an art. You're never going back to the fact that first make it work, okay? First make it not work. That's not great. I think this is something that comes with a lot of good experience, anticipating what might go wrong or what the future needs might be. I still get this wrong all the time. It's a thin balance between making things work and making things functional. \n\nI had a use case this week when we had an API that only supported a single inference and needed to support batch inference. The change for that support was two lines of code, and I was like, wow, this is really what the new then-hillary meant, changing two lines of code for more functionality. It was a good feeling, but it's not like that every day, but I try.\n\nThis week, I want to shine a spotlight on another Real Python video course. It's about building a project using a package we mentioned during this week's episode. It's titled \"Building a URL Shortener with Fast API and Python\" and is based on a Real Python step-by-step tutorial by Philip xeni. In the video course, Darren Jones shows you how to create a REST API with Fast API, run a development web server using uvicorn, model a SQLite database, investigate the auto-generated API documentation, interact with the database with CRUD actions, and optimize your app by refactoring your code. This URL shortener project is intended for intermediate Python developers who want to try out Fast API and learn about API design, CRUD, and interaction with a database. \n\nAnd then going back into some of these core concepts of software engineering, which you said you didn't really have the background. That's what was nice about that book, the shift of working in a workplace and testing being a big part of that too. For me, I don't have the formal computer science background, but I have a big passion for computer science. Compared to most machine learning engineers out there, I believe one of the big secrets of machine learning is to be a good software engineer. A lot of these concepts of testing, unit testing, continuously deploying really knowing the software side gives you a boost on the machine learning side. \n\nIn this post, I was talking about tests. The fact of the matter is a lot of projects that I've seen in the wild don't even have tests. There's a big debate surrounding tests, should you write tests or shouldn't you write tests. Users don't see tests, but if something goes wrong, they'll notice it. I try to have this approach to testing to increase my confidence when I'm shipping. I test to cover my own back, especially for complex functions or critical pieces of code. Writing good tests minimizes the chance of bugs in critical parts of my code. They don't eliminate bugs, but they minimize the chance of them. I try to have a pragmatic balance on testing.\n\nI feel like data science ends up very often, just as a generalization.Based on my experience working with different people and projects, I would define self-work as work that I do for myself or for my small team. It becomes more interesting when it involves shared work, which has more requirements and is put out into the world for clients or customers to interact with.\n\nI remember working at a school for recording engineers, and the technician always said, \"I can make things student-resistant, but I can't make them student-proof.\" Things will happen to the equipment, and that's the reality.\n\nData scientists often work in small teams, but it's essential to remember that what works on your machine may not work in production. Jupyter notebooks are great for exploration, but for production code, adherence to established software development practices is crucial.\n\nTests are valuable in machine learning projects, especially when collaborating with others. Ensuring that new changes do not disrupt existing work is important. While machine learning is not always deterministic, tests can still provide value in ensuring consistency.\n\nContinuous learning is crucial in any field, including data science. It's essential to stay updated on the latest trends and techniques to keep your skills sharp. My upbringing in a family of academics instilled in me the value of continuous learning, and I believe it is essential for personal and professional growth.In fields like software, it is important to know how to do things and question your previous methods. Continuously learning and evaluating past work is key to improving. Personally, I focus on machine learning, but Python is my tool of choice. I need to master it to excel in my craft. This requires dedicating time to staying updated on industry trends.\n\nMaking time to learn is essential amidst the overwhelming amount of information available. By committing to learning, the benefits will compound over time. Utilizing resources like Python newsletters, podcasts, and technical books helps me stay informed. I prefer physical books for technical topics, as it allows me to add notes and highlights.\n\nDespite the convenience of Kindle for non-fiction, I still prefer physical books for technical subjects. I use Kindle to highlight snippets and then parse them into a repository using Python. I also built a newsletter system that sends me highlights from my Kindle every Friday. This project, Kindle-highlights.email, is available for anyone to sign up.\n\nIn the morning, I reflect on the quotes I've highlighted in books to set the tone for the day. It helps me focus on my goals and revisit important insights. RSS feeds are a valuable tool for gathering information, and I use Feedly and Reader (with two e's) to manage my feeds. It's important to be mindful of adding too much content and overwhelming yourself with information.That's where it is. The overall, exactly so easy to overwhelm yourself. Like, you'll notice that some of these sites, right, hundreds of articles every day. And you want to read your news in the morning, right? So, I would say be extremely skeptical of adding stuff to your RSS feeds and clean them up regularly. I always try to keep them to, like, I don't know, 60 articles daily that I can go through and see what I like. So, every morning, I just go through it, see what I like, and then, yeah. It's not a chore, it's more of, yeah, exactly. I'm sure if you're looking forward to, right? Exactly, exactly. Otherwise, like, you know, 300 articles every day, I have to skim through. I'm like, no thanks.\n\nAs programmers, sometimes feel like we must go through all of them and come to completion. Yeah, then it's like you're not learning. It's like Googling, right? You're not, nothing is sticking to your mind. Are there some resources specifically for ML? Oh, yeah, like machine learning is also a big part of what I do, basically. So, all of the conferences I recently attended, one called Normcon, okay. And the idea is that we have this kind of imposter syndrome where everyone is doing crazy things in GPT, right? But there's something called Normcon, which is basically, let's discuss the machine learning problems that we really face day-to-day. Hey, sometimes you're dealing with a five-gigabyte JSON file and you don't know how to parse it correctly. So, it was a bunch of talks about normal everyday machine learning problems someone faces. So, yeah, like Normcon was a great conference. Some people that I follow in the field, I think, not the people that are so hypey about the field, but like people like Vicky Boykis, Vincent Warmerdam, Sebastian Raska, people that actually do it well and calmly. And I follow their blogs and they're great. Probably with them books like \"Machine Learning Design Patterns\" is a great book. Designing ML Systems, there's a couple of books I read recently. Podcasts, Practical AI by the Changelog Network. This Week in Machine Learning is also a great pod. Um, and yeah, what was the one that you were on? The one, it was ML Ops, basically. It's called Envelopes by Neptune AI. It's also a super good podcast. So, you'll for sure find stuff out there. But I think also a good point is get involved in the community. I mean, there's nothing better than being in the sea of nerds. So go to conferences, get involved.\n\nYou're reminding me of something that I had heard at PyCon last year, how there was, and I didn't follow up on it, I want to again, but it was kind of a similar thing where we were talking about an ML problem being presented and them talking through potential solutions. This one was like a particular data set drops and then people have to kind of think about that data set and what's in it and how to organize it, how to clean it, whatever. And I thought it was kind of interesting, a weekly sort of data set drop. I'll have to look it up, like a weekly data science kind of challenge. Yeah, yeah, but it's a little different than the website like Kaggle. It was some different organization that was kind of doing it as a way to talk about working through data and that kind of stuff. There are so many good resources out there, really. Yeah, but keep growing all the time. Exactly, because ways to stay on top. Yeah, exactly, there's no ways to stay on top of it. And I mean, coming back to your question of learning continuously, right? The best way of learning for me is to build, honestly. Yeah, if you build a project you're passionate about. The cool thing about Python is that you could basically do a project on anything you want because there's a way of applying Python to most areas in the world. So building something normally for me is the best way to learn. Just recently, I was doing a data analysis of my Spotify playlist to figure out why has Spotify been recommending some things to me or really understanding, like trying to build a recommendation, like in a bad way or a good way, I mean, in an okay way. I know, like I mean, I was just plotting the popularity of songs I've been adding to a single playlist. So I can plot the popularity, the average popularity of songs over the years. And I've noticed that as years go by, the popularity increases every year. Yeah, and it's not that I like very popular music, I never did. So I don't know if someone is recommending to me more popular stuff. I don't know what's happening. But yeah, like I was, yeah, I feel like they've changed it a little bit. I feel like the things they suggest me have changed over the last two years. I'm sure they insert here and there are some things, you know, they're more like. But yeah, I feel the algorithms modify. Oh yeah, that's for sure. I mean, even in the random, right? I think people were complaining in their random shuffling mode of Spotify that they were listening to the same artist twice.Then, the random shuffling aspect of Spotify's shuffling button is actually not completely random. It's random with a rule that ensures the list doesn't play the same artist twice. Do you know what I mean?\n\nYeah, totally. Alright, so I have these questions every week. The first one is, what's something that you're excited about that's happening in the world of Python? It could be an event, book, package, editor, what have you.\n\nI'm excited about a lot of things. First of all, it's conference season coming up. I'm a big fan of conferences, so PyCon Italy and PyCon in Berlin are a couple of them I'm super excited to go to and see what's going on. Another thing I'm excited about is rough, the new linter. It's a rust-based linter that I think is super fast for Python projects. I've been using it and getting great results. The whole rust meets python duo is really exciting to see grow. I think it's a good companion to Python.\n\nI also dwelled in Rust a bit. I tried Advent of Code last year in Rust, and I was pleasantly surprised. It's a good way to practice.\n\nWhat's something you want to learn next? It doesn't need to be programming-specific.\n\nI'm reading a book called \"Efficient Python,\" and I'm falling in love with it. It's by Brett Slatkin. It's such an amazing book, and I'm super excited to apply what I learn from it.\n\nHow can people follow what you do online? What's the best way?\n\nThe best way is my website, Duarteocarmo.com. Twitter and LinkedIn are also okay, but all the links are on my website for easy access. You can catch up on my blog and see what I'm up to with conferences and more.\n\nThanks for coming on the show, Duarte. It's been fantastic to talk to you.Thanks so much for having me, Christopher. It was an incredible talk. I really had a lot of fun. Don't forget, easy to start and scale companies like IBM, Cisco, and Red Hat all rely heavily on InfluxDB. Check out why they chose InfluxDB. Get started for free today at influxdata.com.\n\nI want to thank Duart Oliveira and Carmel for coming on the show this week. And I want to thank you for listening to the Real Python Podcast. Make sure to click that follow button in your podcast player. And if you see a subscribe button somewhere, remember that the Real Python Podcast is free. If you like the show, please leave us a review. You can find show notes with links to all the topics we spoke about inside your podcast player or at realpython.com/podcast. And while you're there, you can leave us a question or a topic idea. I've been your host, Christopher Bailey. I look forward to talking to you soon.",
    "f2i2NvZBxc4": "Welcome to the Real Python Podcast. This is Episode 154. Are you familiar with the different versions of WebAssembly? Could WASM be the right \"run everywhere\" solution developers have searched for? Where does distributing Python applications fit in the narrative?\n\nThis week on the show, we have CPython core developer Brett Cannon to discuss his recent articles about WebAssembly and MVP. Brett has completed his \"Syntactic Sugar\" series, which we discussed in a previous episode. He details the origin of the series and his process of unearthing a minimum viable version of Python. Brett shares how he updated his PyCon US talk on the subject after feedback from presenting it at Podcastades. \n\nWe also dig deep into WebAssembly, specifically WebAssembly System Interface or Wasi. Brett explains the concept of a platform Target Triple and the importance of defining what system CPython is compiled for. We also discuss WebAssembly becoming a ubiquitous distribution system.\n\nThis episode is brought to you by Courier. Courier is the developer platform for notifications, providing powerful API Primitives for building notifications that perfectly fit your app's UX. Get started for free at courier.com.\n\nAlright, let's get started.\n\n[Music]\n\nThe Real Python Podcast is a weekly conversation about using Python in the real world. My name is Christopher Bailey, your host. Each week, we feature interviews with experts in the community and discussions about the topics, articles, and courses found at realpython.com. After the podcast, join us and learn real-world Python skills with a community of experts at realpython.com.\n\nHey Brett, welcome back to the show. Thanks for having me back. It was so cool to meet you in person up there in Vancouver a couple of weeks ago. Yeah, it was a lot of fun. It was actually this year's Podcastades 2023 in Vancouver, and it's actually the first conference since PyCon US 2019 for me, if you don't count the Python core Dev Sprints. So it was exhausting but great.\n\nNice to be around that many people and talking that long and standing on my feet that long. So yeah, is that the first sort of presentation, or have you done any virtual ones? Good question. I honestly don't remember. Okay. Yeah, I don't know because you gave a talk that we'll discuss a little bit here in a minute. Yes. Yeah, and I want to thank you for the thorough tutorial on Marvel Snap. That was really nice. Thanks, you're quite welcome.\n\nOne thing we talked about, I guess it was just over a year ago, we were talking about how you would use the Internet to survey ideas and things that you wanted to propose to the community and ask questions. You used to use a former social media site, and you've sort of transitioned to Mastodon, I think completely. I wonder how that's going. I guess first. Yeah, so I actually tried a couple of times to see if I could get the Python community off of Twitter and failed. And then certain decisions and actions in late 2022 made that decision for the community on my behalf, so that worked out. Yeah, but yeah, I completely jumped ship over to Mastodon, and I think I completed the transition at the end of November. It honestly has gone well for me. Follower counts dropped significantly, but I honestly was never on Twitter for follower count. I was there really just to engage with the community and talk with people, and enough of those wonderful nice people in the community moved over to Mastodon that it's worked out great. I still run polls on occasion to try to get feedback from the community.\n\nYeah, that's what I was going to wonder, is if you got enough input or information from your polls with a smaller follower count, is it still working good? Yeah, I mean, I never expect the polls to be definitive. There's selection bias, obviously, with who chooses to follow me and thus respond to the polls. The numbers are never large enough to match the, let's be honest, probably tens of millions of Python users there are out there at this point. Yeah, so I never view them in any way in terms of anything other than to see if I happen to think I have a blind spot or a bias that at least some people might not hold, or to honestly see what the responses are and to see if there's something I hadn't thought of. But it's mainly just to see, like, I might think something is not that popular, does that hold true, or is the community going to shock me and go like, \"Oh yeah, actually out of 100 people, half of them actually do use this thing or technique or approach or whatever that I actually thought hardly anyone did.\" So it's more just trying to verify that I don't have a blind spot, and yeah, it continues to work even over at Mastodon. Cool. Yeah, it seems like that process is still something you're engaging in, and back and forth between like should this be potentially proposed as a PEP, and that feedback loop seems to still be working pretty well for you. Yeah, I mean, admittedly I do have a lot of random ideas, and a lot of the questions I ask are for projects that literally might not take on and come to fruition for years. But there are reasons I'm asking them. They're never completely superfluous, and if they are, all right, I try to be honest, like I'm just curious.And that's just asked just to see what the conversation goes, see if it sparks interest for anyone. But sometimes, honestly, most of the time there's a kernel of a purpose that just might not be obvious for quite some time until people realize why I asked. So yeah, and then it shows up on your blog or somewhere else. Congratulations on completing the syntactic sugar/unraveling series on your blog. Thank you. Yeah, that started back in June of 2020. Yeah, I think it was 2020. I just, to be honest, I just edited one of the posts yesterday. So yeah, well over two and a half years. You did a presentation at Pi Cascades, and it looks like you're going to do a slightly modified one based on our earlier conversation here at PyCon US. Yes, exactly. I actually gave that talk, and then afterwards, I had multiple people come up to me because in the talk, for the blog post series, for those of you who aren't familiar with it, I basically just looked at all the Python syntax in Python 3.8 because back in 2020, that was the latest release. And just kind of looked at what really has to be there and what could you actually re-implement yourself by hand in Python code with other rules and stuff we can talk about if you want. But as part of the final post, I listed the things I couldn't figure out a way to get rid of based on the restrictions I placed on myself. And when I presented that list of 11 things at PyCascades, people came up to me and said, \"On two or three of them, I actually don't know if you really need to keep that on that list. I think you can get rid of it.\" Okay. And lo and behold, on two of them, I got rid of them. And the third one, I'd have to change some things, or actually, I took away one of my rules that I bent for myself and put something back on that I had taken off. So honestly, I didn't have to unravel one of the things, so it's back to 10, but it's still changed. So if you're coming to Python and you saw my PyCascades talk, it will at least be tweaked in case you come here a second time. It'll be fresh. That's nice. Yeah, I think we talked about it in an earlier episode. Look at the number here and add it in. But one of the things I asked you back then was this concept of like, okay, as a beginner or intermediate person, you hear this term syntactic sugar, and you're like, \"Wow, that sounds very fancy. What is it?\" And I'm trying to remember your explanation. I can't remember honestly back then either. It changes constantly, but I think fancy is kind of a good point, right? Syntactic sugar is just a bit of fancy in your code to make your life easier. It's syntax. It's actually technically not necessary to end up at the exact same semantic result, which is a very fancy way of just saying, if you didn't have the syntax, you could still get your job done and still end up at the same endpoint. Right? It just might and hopefully does make your life way easier and better, right? Like, the canonical example I use is plus. Turns out you actually don't need plus in Python. You can totally make it work without it. It just is a lot of lines of code to make it work appropriately in the same way. Or you can just learn to write one little ASCII character that looks like a plus and have it do the same thing for you. That's syntactic sugar. Okay, so I'll generally, all the different operators are most of them would be considered potentially syntactic sugar then, if I'm not wrong, all but 10. Okay, all the different items. So then you talk about rewriting them, and I kind of think there's some interesting maybe rules there that I'm wondering, okay, so if you have these 10 items, you can use those 10 items in the rewriting, then of these other pieces of syntax. Yeah, I mean, basically, the rules I set up for myself in the blog series was the restrictions in terms of could I unravel something as in from syntactic sugar down to its base components was that it had to be in 3.8. It was okay to make things look a little wonky if you called globals or locals, those built-in functions, right? Because you're kind of peeking under the hood in terms of how things execute, and you might be surprised if you ever call the locals function inside a list comprehension. You're going to notice there's actually a magical little thing you didn't know about, a slight Easter egg for CPython users. And so I figured if we're already doing that in some places, I could cheat that way too. And then I didn't care about performance. This is not meant to be faster or comparative because that's just not possible, right? I'm taking a bunch of stuff that's done in C code, at least in CPython or stuff that PyPy gets JIT away at floor levels and certain things, and I'm unraveling it all. So it's just not quite equivalent. But then the last one is I said the translation from the syntactic sugar down to its base components had to be within a single file. Okay.The reason I did that is because I wanted to conceptually have a tool that I could pass a file to or pass a file to the tool and have it spit out a copy of that file with all the syntactic sugar unraveled. The goal was to have it still work the same as if you didn't know that happened, so you could still import that code and have it still lead to the same result. This tweak meant that I couldn't change how function calls work. If I could look at your entire codebase and change it as one big unit, I could simplify it by making everything return either okay with the results or error with the exception being raised, like how Rust does it. But, since I wanted to be able to take a file on its own and translate it without affecting other code that used it, I had to ensure that any functions, methods, or classes still work as normal.\n\nI ran the dis module to look at the underlying bytecode and then mapped it to the C code to show readers what was happening underneath the hood. By explaining how certain syntax unravels to bytecode and then to C code, I aimed to help readers understand the semantics. I have a repository on GitHub (github.com/BrettCanon/Dsugar) where I initially posted all the code. As the project progressed, I focused on unraveling things that could be mapped back to built-in functions and eventually tapered off the detailed unraveling as the project took over two and a half years to complete.\n\nI called it MVPI (Minimum Viable Python) to summarize the methodology in the 32 posts. Towards the end, the level of detail decreased as I needed to finish the project and my wife, Andrea, preferred to proofread fewer posts. It made sense to summarize the steps and methodology as readers progressed through the blog series.I got very interested in this year and a half ago. I was like, \"Oh, this looks like you have a goal in mind,\" which was to get things down to its core elements like what is Python. That had to do a little bit with the idea of maybe moving toward WebAssembly. A lot has happened in the last two years with all of that, and you're involved in a lot of it. I don't know if you want to talk about the beginnings of it and why you embarked on it.\n\nYeah, I mean, you're totally right. Basically, when my wife Andrea sent me down this journey on WebAssembly, that's no joke. My wife was doing a data science course at UBC (University of British Columbia) here in Vancouver. She completed it, and while she was doing it, in the first week, she was doing an assignment I think and it was all done in Jupyter notebooks. She stepped away for like an hour or something, I think to have dinner or something, and went back and she was disconnected. She was like, \"Why isn't this working?\" It's like, \"Oh, well, they're using a technology called Jupyter Hub, right? Where they've got some servers over at UBC that are running and hosting your Jupyter notebook. And when you hit the Run button, it's running on those servers and sending the result back to your browser, so you just got disconnected from the servers.\" I said, \"Well, I thought this was running in the browser.\" It's like, \"No, no, Python isn't running in the browser. There's this thing called WebAssembly that I've been thinking about and it's probably could work, but no one's done it yet.\" I'm thinking about it a little bit. I'll never forget this. I was in the car, if you're a local, you know where I'm talking about. We were on Hastings right next to the peony going east towards Burnaby and she literally just, while I was driving, I explained this all. She just said, \"You should fix that.\" I was like, \"Okay, I don't think you realize what you're asking here. Like, this is a multi-year project to get Python in the browser, etc., etc.\" And literally, split second after I said all that, she just thought for a second and went, \"No, you should fix that.\"\n\nSo, yeah, that led to this blog post series because the first thing I thought about was like, \"Okay, how do I get Python in the browser?\" And part of that was, \"Do I have to reimplement Python? Like, see Python in WebAssembly? Like, do I have to go all the way down to... Yeah, could I compile Python straight to WebAssembly? Could I even do that? Like, what would that entail?\" And so my brain went like, \"Well, what do I have to do? What really is Python? What do I have to do? What really is Python?\" This is when I had... I did some back when I was on Twitter. I had some tweets where I said, \"You know what? The REPL really isn't part of Python, right? So, because if I compile Python, I won't really have a REPL because the WebAssembly security model, you can't dynamically create and run WebAssembly code. It has to be all kind of known upfront because it scans it, makes sure it's valid. So, it's like, I guess I really don't need the REPL. It's not technically a part of Python. It's just a tool that's extremely useful. Don't get me wrong. It ships typically with it. But honestly, do you have to have a REPL to call yourself Python?\" And so when I just went down that road and inevitably just kept thinking about like, \"Really, what is that minimum viable version of Python that if I had that, everything else could be built from it?\" Yeah, and because that would be what I would have to reimplement if I was going to compile Python down to WebAssembly. And that's what led to this blog post. It was just like, \"How small could I potentially make an interpreter or compiler for Python and somehow get semantically back up to what Python 3.8 is?\" Congratulations. I don't know if I could actually make a compiler work exactly the way people might expect Python to function just because there's enough dynamicism to it that it might not. But it was still at least an interesting exercise regardless.\n\nYeah, what's interesting is I feel like there was a lot of cross-pollination happening right at that moment between multiple people and kind of the Zeitgeist or whatever this idea of like, \"Hey, we want to do this too.\" Were you communicating with other teams about the idea of Python in the browser in those early days of thinking about this and looking at their work and sharing notes, or was this done mostly on your own initially? The blog post series was mostly on my own. I was not hiding the fact that I was thinking about WebAssembly, but I wasn't seeking anyone out because I was still trying to figure out how this whole approach would work, like what would be required, what could I do short term, what could I do medium term, what could I do long term.When I think of Python projects that I take on, there's the little thing like 100 line projects like MicroPython that I can write in 100 lines and do roughly in a day. Then there are projects that are going to take literally years because they're required to come up with the idea, reach consensus with people, running the PEPs, getting them out, and then getting everyone to support what the PIP proposed. \n\nSo, I wanted to know what could be done in less than a year, what could be done in less than five years, and what would take like 10 years. A compiler would be like 10 years, if at all. I was always talking about what it would take to compile Python itself and all that. This led to Python Core Dev Sprints in 2021, and that's when Christian Hines got involved and helped with all that. \n\nUp until roughly the Corehouse friends and this past year in 2022, other people have come along and gotten involved at various levels. We're now at the current situation where we're at.\n\nOne of the things that has been around for a little while now is WebAssembly, or wasm. The big release last year at PyCon was PiScript on top of that and some interesting tooling. Probably the one that most people have seen out in the world, if they haven't talked about it much behind the scenes, is using a tool of MScriptum. Is Christian involved in that one or who's the team that does it?\n\nWell, it depends on how you define it. Actually, Christian's taking a break from open source right now. In terms of that stuff, that would be the people from Piodide. PiScript runs on top of Piodide, and then Piodide uses MScripting to make Python work. The whole stack works with CPython at the base layer, a tool called MScripting to compile C code to WebAssembly specifically for the browser, and then Piodide uses MScripting to compile CPython and add some extra niceties to make it run in the browser. \n\nPiScript is built on top of that to provide what it adds in terms of specifying packages to install and its GUI stuff. Nicholas Tolerby, who works at Anaconda and drives all the PiScript work, got MicroPython working on top of PiScript. They've taken PiScript and made it kind of Python runtime agnostic. You could plug in MicroPython or Piodide, for example.\n\nWith these set of tools, what you were looking for is solved in some ways. There are still some things missing that need to be a replacement for, say, a Jupyter or what have you. It depends on your view, like Jupiter Light which runs in the browser and is actually Jupyter fully self-contained running in the browser. The real trick is always going to be the packaging story, which has always been the story for most languages these days.Yeah, it seems to be a problem that is still being worked on on the browser side of things. The Quant stack folks are trying to create something called Inscription Forge, where they take a Conda Forge and use Conda build to try to build all the Conda packages for Inscription. This would mean it could work with Piodi then anything else built on top of that. I don't know the status of that project. They announced that in a blog post last year in 2022, maybe mid-2022. But I don't know the status of that. \n\nI think the key status point for the browser stuff is probably still trying to figure out the packaging story. You can install stuff from Pypi that's pure Python and doesn't use anything that's not supported by encryption. Okay, so it's definitely not zero. Can you get an extension module compiled for it, yes or no, and that's always the question. \n\nThere's a subset of things that you're going to have to just sort of look at and depth, make sure it's an approved list of stuff. Just because it works on CPython on your desktop does not mean it will naturally work in the browser. \n\nYou've had a couple of blog posts recently that are in this realm too. The one you did in December was about WebAssembly and its platform targets. This is something that I've been wondering a lot about. You talk about the differences between Wasm and Wazi. Wasm is Web Assembly, basically a weird mashup of those letters, and then Wazi is Web Assembly System Interface. \n\nPartly the idea of like, okay, we want to input between packages and we want to have things be able to talk, but a browser is a sandbox. There are walls around it. Potentially, the idea with Wazi is to make an application that goes maybe a little bit further and has more permissions to break outside the sandbox. If that's part of what Wazi is trying to do.\n\nI think the best way to explain this is that there's another hierarchy here that's Web Assembly specific. WebAssembly itself is what's called an ISA, an Instruction Set Architecture. It defines a virtual CPU's instruction set. It's just saying, here are the instructions that you expect things to run. \n\nWhen you compile an app, typically you think about this as the platform target triple. Rust and other languages have the same concept. You're compiling down to a platform triple where it goes CPU architecture, machine, and then operating system. In WebAssembly's case, the platform or OS is where you'll see it say Inscription or Wazi. \n\nThis dovetails nicely into the players for our WebAssembly. It's designed for security and to be cross-platform, so it's agnostic.It's not like you can take x86 code and run it on your iPhone because that's ARM, or vice versa. You either have to do all the crazy stuff that Apple did to make Apple silicon work, where they had the whole Rosetta technology to translate everything and slow things down a bit, or you have to compile natively. This one was just designed to work well in a virtualized way, like running via software.\n\nThe next step after this is the platform. Because we're not really running on Windows or Mac, we're running on some platform like WebAssembly. M scripting is one platform, and this browser typically covers that for the browser. Inscription provides the APIs that you might want to use via the browser, so typically Inscription is listed as the platform.\n\nWazzy is an abstraction that defines the API to access things that your platform or OS would normally provide. It lets you have the implementation provided by a runtime. For example, Wazzy provides an API for reading a file, but the runtime provides the actual implementation for that function.\n\nThe cool thing about using runtimes like Wazzy is that they can provide security. They control what functions can be accessed and provide a sandboxed environment. This prevents arbitrary file access and ensures that code only accesses designated directories.\n\nWith Wazzy, you can't delete files or access sensitive areas of your machine. The runtime limits access to specific directories and ensures that code can only interact with designated areas. It's like having a sandboxed environment for code to run in.\n\nWazzy is an abstraction that can be used in the browser, as it's just an API. There is a proof of concept or demo shim available that shows how Wazzy can be shimmed for the browser.It still runs, works, and uses the web's API, right? It's like, \"Oh, I want to read a file,\" and it pops up the thing asking you to choose the file to read. So it's not really a browser versus Wazzy thing, it's just that Wazzy is a different concept and typically breaks down to scripting because it doesn't necessarily restrict itself as much as it can to be very clear, but it will go beyond that. It doesn't restrict itself to Wazzy. Wazzy is specifically designed to be an API that people can target and say, \"I compiled for this file for Wazzy, so I know as long as this stuff's provided, it'll work and be sandboxed and protected.\" It's interesting that it feels like the virtual machine in this case, wasm32, needs a docking point, if you will, that is the runtime in order for it to have access to the machine and do the stuff it's going to do. So, it could be docking inside of, not using it, just a term, but if you will, like connecting to the browser or connecting to the runtime, this thing that is able to basically launch the virtual machine and give it its ability to. It's sort of like a brain connecting to a body or something like that, you know, having the ability to connect the stuff to it.\n\nNo, actually, that's a great analogy, and I think it's a better analogy than you realize because if you go from the term docking to Docker, it's a similar concept, right? Docker lets you mount things from your machine into the Docker container in a very specific way, like this directory mounts into the Docker container at that directory. You can restrict what network access it has, it's all the same thing. There's a somewhat famous tweet out there from one of the co-founders, I think, of Docker itself who said, \"If web assembly existed when we started to think of Docker, we just would have done web assembly, we would have gone that way.\" Okay, yeah, so it provides that kind of similar layer of control and security that people have come to rely on Docker for in terms of controlling how that code running in this sandbox or in this runtime has access to everything else on your machine.\n\nI had Russell Keith McGee on not too long ago to kind of follow up with him. I hadn't talked to him in two years and kind of wanted to get an idea of what's going on with Bewear and what's the state of that. We discussed it a little bit. Like he's looking at all these different ways of deploying. That's the goal of Bewear, getting your app, your Python app on other people's machines running. Not necessarily for them to develop, but just to be able to have a runnable app. And the targets very often are like an iPhone, an Android device, Windows, or what have you. He's solved a lot of them and occasionally bumps up against the problems that you have on a phone and so forth and has lots of tooling that has to get going there. I asked him about the tool a while ago for deploying to the web, and it was a whole other solution. Now he's like, \"I'm really kind of watching to see what happens with Wazzy and Wasm and seeing if maybe that is the direction for Bewear in the future.\"\n\nWhich I think is interesting. It's all very interesting because it's leaning into some potential interesting directions. As I said, there are two sides to this. There's the inscription side of this, which has its own approaching solutions to things. It plugs nicely into the way packaging currently works because pure Python code that doesn't try to use something that you can't have access to just works. The Python interpreter just reads the bytes, runs it for you, it totally works in the web assembly. No problem. The tricky part is all those extension modules, where it's written with some native code, whether it's C, Rust, Fortran, whatever. That's the tricky part now for the inscription. The reason this is tricky is the Python community, and this doesn't matter whether you're talking Conda or Pip, all of it, because at the fundamental level when you import something that's an extension module, what you're doing is something called dynamic linking. And what that is, is if you're on a Mac or Linux, you'll look and sometimes you'll see a .so file, and if you're on Windows, that's a .dll. What's happening in Python is both platforms, both the Unix and the Windows side, both have functions where you can say, \"Hey, you've got some compiled code over there. I want you to read that and let me call stuff in there.\" So the way import works is when you import an extension module, what's happening is...I'll just use the Unix side because that's the one I'm more familiar with. There's a function called DL open which stands for Dynamic load open. What you do is give it a path to a DOT so file, and it'll open it and give you back a thing. Then, you can use that to get to functions inside. \n\nWhen you write an extension module, you have to Define the init function, which is named a very specific way. It's pionit underscore and then the name of the module. So, when you import spam, if we find a spam.so file, the import system of Python and C python can go, \"Oh, okay, you're trying to import spam. I found a spam.so or spam module.so.\" I'll call DL open on that file, read it, and then look for pionit underscore spam function and call that. The thing that returns will be a module for spam. That's how the entire ecosystem is designed. \n\nThe problem is webassembly is not designed for that in any way, shape, or form. Webassembly wants to know about all your code ahead of time for security purposes. Dynamic loading and linking happens dynamically when running Python, it may or may not happen, you don't know if the file exists ahead of time. \n\nToday, we need to do this thing, exactly as yesterday. You just grab the wheel off of Pi Pi or install the condo package, and there may not be dot py files and dot so files there tomorrow. Webassembly does not want that world, it wants to know everything up front to verify everything works together nicely. \n\nThe tricky bit here is how do we resolve the problem of webassembly wanting to know everything up front and the Python world being designed differently. Python can't build to a single file, unlike go and rust. Python can just download a random package and immediately use it, unlike go and rust. Webassembly has pros and cons similar to go and rust. \n\nIn scripting, they have a trampoline at the JavaScript level. Your webassembly code gets compiled and encrypted to load up into another web assembly thing and call that code from JavaScript. It's a hack, but it works to provide a solution to the problem of dynamic loading in the Python community and any dynamic language. \n\nConversations are happening at the wazzy level about how to solve this. My day job is to get out of the trampoline park because it's not a standard solution, it's just the inscription tool coming up with its own solution.But it's not standardized at all. Scripting is a tool that generates its own things; it has no compatibility guarantees between SDK releases. You have to compile your whole world with the same version of in script in to get the result, which fits well with condo. They're trying to take Honda Forge to do it because you can compile all of condo Forge using one version of in script in and spit out all the webassembly for that part. They've all just worked together because it's all using the same tool chain.\n\nThe problem is that doesn't necessarily mean that's the only way to do it, but once again, you have to do that and it's not a standard. For my day job, I'm in charge of the Python experience for VS Code. We're starting to look into using webassembly as a way to run Python in the browser for vscode.dev. We decided to go with Wazzy because we have these APIs. VS Code can implement those APIs so that when you try to read a file in webassembly, it actually reads from the code loaded up into VS Code itself. \n\nSo, if you're on vscode.dev and you open up your GitHub repo, you can suddenly read those files even though they're not physically in your browser. We're actually talking through running JavaScript code that's then talking to GitHub to get those files pulled in, and then you can read and process them. The problem is how to get all those extension modules because as soon as you want to pull in numpy, for instance, you hit a roadblock.\n\nI have been tasked to figure this out. I'm talking with the Bytecode Alliance, who are effectively the people in charge of Wazzy. They formed a Dynamic Languages Sig for Python and other stuff. We're starting to have early conversations about what webassembly and Wazzy have to do to work better with these Dynamic languages. Webassembly has been heavy on Rust and C++ code, but now they're looking into supporting Dynamic languages like Python, Ruby, PHP.\n\nMy dream is to find a way to provide all the extension module code at runtime for webassembly, so you only have to compile your extension module once and it'll work on any platform running webassembly. It's an intriguing story because it simplifies the process of compiling for different platforms. Imagine if you only had to do it once for webassembly. It's an interesting story potentially because if we package everything up as webassembly, it could run on various platforms.\n\nMy dream is to have the wazzy runtime provide all the extension module code upfront, so it can run on different platforms. If there was a way for the wazzy runtime to do ahead of time Dynamic loading, it would simplify the process.Do that, pull it together at runtime and just go. Like, okay now go run right. I might have to download all my stuff up front, but at least I don't have to compile it with Python together at the same time like you do with Rust or Go, right? I don't have that overhead. Yeah, I could just give you the file and you just have the listed up front that you need to load all this together and it'll just work, right? That would be fantastic, right? Because once again, portable on any OS as long as you have a runtime on it. You would be able to use it and that's pretty much all of them, yeah. So that'd be a dream of mine, right?\n\nThis week, I want to shine a spotlight on another Real Python video course. We've covered a recent course on OOP or object-oriented programming, the method of structuring a program by bundling related properties and behaviors into individual objects. The course I'm featuring this week builds on the previous Python Basics course on object-oriented programming, and both courses are built from a section of the Real Python book \"Python Basics: A Practical Introduction to Python 3\". This particular one is titled \"Python Basics: Building Systems with Classes\". In the course, Ian Curry takes you through how to compose classes together to create layers of functionality, how to inherit and override behavior from other classes to create variations extending a parent class, and how to use the super function and then how to creatively mix and match these approaches.\n\nObject-oriented programming can be a bit intimidating for someone starting on their Python journey. This course continues with a steady hand to lead you deeper into the topic. Like all video courses on Real Python, the course is broken into easily consumable sections, plus you get additional resources and code examples for the techniques shown. All of our course lessons have a transcript, including closed captions. Check out the video course, you can find a link in the show notes or you can find it using the enhanced search tool on realpython.com.\n\nOne of the things that you mentioned in the article is the WebAssembly and its platform targets for it. We talked about the target triple and all that sort of stuff. Is this PEP 11, which I think is kind of wild how many PEPs are living documents that way? Are many of them that same kind of way where they're just constantly need to be updated? I guess maybe we should discuss what PEP 11 describes and lists, sure.\n\nSo, PEP 11 is basically the list of officially supported platforms by CPython, and it's what's known as an informational PEP. If you go to peps.python.org, you'll see the list of all the PEPs that have ever been submitted. Yes, on your definition, depending on your definition of recent, in my time frame of Python, it's very recent. In terms of Internet time, it's not that recent, sure.\n\nWe were very lucky that about a year ago, we got some various people like Will Turner and Cam who got involved, who really took to the PEP repo and just really wanted to put the time in to clean it up and make it easier to work with and added a lot of tooling around it. We were able to make the website a bit easier to work with. It's very easy to search and navigate now. Yeah, we even have subtopic sections. I think if you go to peps.python.org/packaging, you'll see all the PEPs are only about packaging. I think there's a similar one for types and typing. They've done a great job and made everything read a lot more easily.\n\nBut yeah, so the deal is informational PEPs are ones that are kind of living documents. They're meant to provide information because we, the Python or Dev team, just didn't have, well, I mean, we're so old, we didn't really have a website, right? I mean, there's python.org, but the dev team itself didn't really have a Dev section. I wrote the Dev guide, devguide.python.org, that was one of the first dead-specific things they really kind of wrote out and covered all of our processes and all this stuff. That postdates PEP, so for a long time, we were using PEPs as a way to record decisions we had made and decisions that we would revisit, and those are informational PEPs where it just outlines what the PEP is for, but we will update those PEPs versus accepted PEPs that are standards track that are updating the language or something. In which case, they just become historical documents, we don't update them anymore.\n\nBut informational PEPs are meant to be living documents, and PEP 11 specifically, as I said, lists the platforms we support officially to some degree. It's tiered, and WebAssembly, yes, tiered, and it's tier three. What's the idea between the tiers? I guess we should say, yeah, so there are three tiers.\n\nFirst off, if your platform's not in a tier, it basically means that core Dev is allowed to remove code that might break that platform, okay, right.It's just like AIX is a good example. We don't officially support AIX, so if there's some code in there that we want to make a change and it's going to break AIX support, that's totally fine. It doesn't matter, ARS is not officially supported so it can just go. Is it kind of like we not guarantee, but we planned this level of support for these tiers. Like you could certainly expect this is what's going to happen. Tier one mentions CI failures will block the release of a release from coming out if the continuous integration won't go. I remember watching releases of beta versions or what have you and so forth, and it's like, okay, well I guess this isn't going to go this time. We've had a problem and we need to go back and make sure that these have to pass and get through before we move on. Is that right? A way to describe it? Yeah, so these are the tiers just increases the promises that we make so people can set their expectations for support for that platform, right?\n\nTier three basically says you can't remove support for this platform. Okay, it can be broken, but you can't proactively remove the code saying, \"Oh, this is annoying, I don't want this.\" Like, no, you can't take it out. It might be broken, but you got to leave it there. Okay, technically to reach that level, all this requires Steering Council approval to be at any of these tiers, but for tier three other than that, you have to have at least one core developer listed as the contact point. So the person who basically owns the support for that platform and you have to provide a stable build bot that basically helps people detect when they accidentally break something.\n\nThe next level up is tier two. For that one, you can break that platform for up to 24 hours, but after that, you either have to fix it or you need to revert your change. The reason is that level is because most of those platforms are only available as a build bot so our whole testing fleet. So you can't test it during CI, so that's why you're giving a 24-hour window. And that one you have to have two core devs because if there's a question and someone happens to be on vacation, the backup person basically has to make sure they're available. The things that are in there, those are Apple's the M1 series, the A arch 64, and Power PC for Linux. Most of them are Linux and then basically M1 Apple stuff. I'm hoping the M1 Apple stuff eventually moves into tier one, so we'll see. So the reason that's not there is because tier one is CI-based. Basically, tier one says you are not allowed to even commit code if it might break something, right? It has to have already passed all of the continuous integration tests by running the whole test suite before you merge a PR for it to be allowed on, right? Like, okay, tier two is just separate just because there's no way for us to know if you're going to break CI.\n\nSo because we don't have CI support for Apple's M1, because Apple being Apple, they didn't make the hardware available to run in basically in some data center somewhere. Right? So for instance, in our case, we use GitHub actions. GitHub actions is available on Linux as x64, Apple for Mac x64, and for Windows 64-bit. So that's why those platforms are up there because we can just guarantee that everything turns green before we hit that merge button on a PR. You can just see it and it should always be green and clean on main. Basically, the key promise there is if you check out the main branch on CPython repo for those tier one platforms, it should always build cleanly and pass the whole test suite. Okay, tier two doesn't make that promise. Tier two just says you might have to go back 24 hours, but within 24 hours it should be fixed. But we can't promise that if you check out main it's going to work because there may be, as I said, someone may have accidentally broken something after they committed on CI because it passed for the top three of tier one. Yeah, but you don't know until we commit because there's a lot of machines. These machines are all donated. Some of these machines literally sit under people's desks and they're not always the most powerful machines. So we don't want to flood them with every PR and every change that people push. So yeah, that's why they're at tier two. Okay, and in our case, for our discussion, WebAssembly is currently at tier three, although I have maybe you might say delusions of grandeur, but basically a goal of getting WebAssembly to tier two before the end of the year. Yeah, other things that are in three are like Raspberry Pi, a particular like Linux. Actually, most of them are Linux. There's the ARM version of Windows and then FreeBSD. Okay, cool. Yeah, there's so much interesting stuff underneath the hood. Got to kind of pay attention to and your name's on there as far as the tier three contact for wasm32.I am lucky that I am getting more and more involved with Cordes. It's crucial for the show's support. I just haven't needed to update that list because I'm already on it, so it's covered for tier three. For tier two, like Coachella, I think he'd be happy to be on there. Eric Snow, because VS Code wants to see Python and web assembly work well, especially for Wazzy. So, at least for Wazzy support, there will be coverage to get the stitcher too.\n\nHonestly, I don't think scripting will necessarily get there. This will be a topic at the web assembly Summit at PyCon US in two weeks. I didn't know there was that Summit. It's been invite-only because I had to organize it and could only handle so much. We have a room for 20 people, and I wasn't sure how many would want to come. So, I posted on discuss.python.org under the web assembly category, saying who would like to come. We're getting about 10 people, and I'm sure there will be some random people who will pop in. Luckily, we won't go past 20, but I didn't want to have to play bouncer at the door. \n\nI purposely didn't broadcast to the world that there's a web assembly Summit. It's very focused on people who are involved. I'm hoping to get Wazzy at least to tier two. I don't know about inscription; we'll have to discuss that. For work purposes, I'm directly interested in Wazzy, and it's easier to develop for. You don't have to involve node or a browser in the background. We'll talk at the web assembly Summit about how we want to support that.\n\nWasmtime is written entirely in Rust, so as long as there's a Rust toolchain, you can compile and run it on that OS. Eventually, you have to get down to some software to run this stuff because there's no web assembly CPU physically plugged into your machine. At some point, you have to hit that platform, but at least in this case, you just need that one runtime for any web assembly, whether it's Python or not. It's gaining more traction in IoT because of its security promises. It's starting to be used in many places. Disney Plus, for example, uses web assembly for its smart TV app. \n\nThis is what Java tried to do on their own, but hopefully, with all the learnings we've had, this will be the \"write once, run anywhere\" scenario. Docker is also gaining interest for cloud deployments. Instead of orchestrating entire VMs, you can run specific pieces of code easily.I'll find Alpine Linux for Docker containers exist because of the overhead and bloat to run a small thing. Why do I have to go through all this, especially if it's just a generic Rust or Go binary, or a Python interpreter with your Python code? There's a lot of overhead to download a multi 100 Meg thing to then spin up Docker to load that thing and provide all that. Or, you know, here's the WebAssembly file, just go run it. The overhead is significantly less in terms of orchestrating all this stuff. Some people have suggested that if WebAssembly existed back when Docker started, it may not have been necessary. People have realized this since WebAssembly was created. It was created for the web, but it's not tied to the web. People are starting to lean into just calling it wasm, rather than the whole WebAssembly name, to disconnect it from the web. It has to be distributed, and there is a WebAssembly runtime in everyone's device because every browser now supports WebAssembly.\n\nIt does mean it's prolific, but it's not restricted to the web anymore. There are other benefits to it, and if they can make it work in ecosystems, there will be a lot of wins. It's one of the faster-moving technologies I've seen in a while, outside of AI and llms. There's too much money involved now for it not to go somewhere. Too many people have sunk engineering costs into it. It's considered early enough that Wazi is in preview one, with a preview two and async to come before a 1.0 release. It's not difficult for Python to provide support, as the hard work was done in terms of patches and tweaking. Bugs were found in the tool chains for WebAssembly and reported. The momentum is there to keep going, but the question for the Python community is whether they will be given the tools to make it a smooth experience or have to build workarounds for limitations. For instance, dynamic loading of extension modules is a challenge for Wazi currently. If they don't have it, it's not the end of the world, and workarounds can be found.I have some people at Single Store Labs who are participants in all this on the Python side. They have actually done a proof of concept where they wrote a mini C app that embeds Python because you can actually embed Python in a C app as part of its design. They statically linked in and compiled NumPy into the binary and used it. Then they just pointed the Cling as the web Summit Commander at that C file to compile it, and it acts like a Python interpreter with NumPy built into it. It's not the worst thing in the world, but it's not the easiest thing either.\n\nIf we somehow get my dream to come to fruition, where I compiled a DOT so version of WebAssembly that these runtimes can consume at runtime, it would make everyone's life way easier. So that's where I'm at in this whole story. I don't think WebAssembly itself is not going anywhere, Python web is the way. I don't think it's going to go away. I think the question is how much the community will latch onto this and how nice of a story we can make it for the community. That's where I'm currently at in this story. I'm now participating at the level of the Bicode Alliance on behalf of Python, and I love that name.\n\nI'm in the dirt, in the mud, playing around, and helping out as best I can. We're also talking about standards and companies involved in all this stuff, so it's not going to move at the speed of AI. It's definitely still moving forward, and everyone's trying to be very conscious and considerate about what happens. We're playing in someone else's sandbox, in this case, WebAssembly sandbox, trying to help out and be good participants. It's not going to move as fast as someone can write a PEP, but as fast as everyone can come to a consensus in the Bicode Alliance of the W3C. All this has to bubble up that way, and people are talking and working, slowly moving forward. I have hope that Bicode Alliance makes me think of this company I saw once called the Bit Cartel, and it's the same kind of idea but awesome.\n\nWe could talk a bit about PyCon briefly, where you are scheduled to do your talk on Saturday, the 22nd, and you're involved in the language summit. You'll be attending as a core developer and a Python steering council member for the last time. You'll also be presenting a topic on what the standard library should be. As a core developer of almost 20 years and a Python steering council member for five years, you still can't answer the question of what should or should not go into the standard library based on what the whole team thinks. There are varying opinions, and there are no guidelines as to what we expect the standard library to represent and do. This conversation will help create guidelines for proposing modules for the standard library based on themes.\n\nIt's an interesting conversation, and it might be pretty animated as people have opinions on this.Well, you had done some research. I don't even remember when, but about looking at what are the sort of dead batteries or things that are in the standard library that are potentially not being touched or used. Maybe there are multiple people involved in that process. How's that? There's things that are marked for deprecation. I don't think that date has happened yet. That has not happened yet, okay.\n\nSo, there was a dead batteries Pep. Christian Heimes wrote, and then I became a co-author of to help finish. And it basically stripped out a bunch of modules that we just thought weren't getting used enough to warrant the cost of maintaining them. Because something I think everyone kind of forgets is someone's got to keep that code working right, and it falls on the core Dev team. And there's only so many of us who actively participate, let alone one of fixed bugs in some random module that doesn't really affect our day-to-day lives or our personal projects or anything that we get paid to work on.\n\nFor those lucky enough to get paid to work on Python. So, it was basically going through and just on a very loose conservative way just going like that seems too old or not useful enough to stay here and not used enough to warrant sticking around, yeah. And then going and asking the community and other core devs, would you cry if we took this away from you? And then cutting out some things that were on that list because too many people were gonna go, \"No, no, no, please don't, no, no.\"\n\nAnd based on the volume of tears, kind of yeah. And then trying to be extra conservative here because we did this once when we moved from Python 2 to 3. And it just seemed like it was a good time to do again because as I think I blogged about once before we did this, there were more modules in the standard library than there are countries in the world. Wow. I mean like it's nearly, I can't remember if it broke 200, but if it didn't, it was just shy of 200 modules. That's a lot. I don't think we'll quite realize the breadth of it if you go look at that module index, it's huge.\n\nSo, that PEP actually got landed, and I believe PEP, I think they all that in PEP in Python 3.11. So, I believe Python 3.13 is when we're going to have a nice culling of the standard library for a bunch of stuff that's due to be taken out, yeah.\n\nWell, I'm looking forward to talking to you, hopefully seeing you there in Salt Lake City, and thanks for coming on the show again. I look forward to seeing you too in about two weeks and thanks for having me on. And don't forget to start sending relevant and timely product notifications for your web and mobile apps for free with courier.com.\n\nI want to thank Brett Cannon for coming on the show this week, and I want to thank you for listening to the Real Python Podcast. Make sure that you click that follow button in your podcast player, and if you see a subscribe button somewhere, remember that the Real Python Podcast is free. If you like the show, please leave us a review. You can find show notes with links to all the topics we spoke about inside your podcast player or at realpython.com/podcast. And while you're there, you can leave us a question or a topic idea. I've been your host, Christopher Bailey. I look forward to talking to you soon.",
    "E06DuwV1yxI": "Welcome to the Real Python Podcast. This is episode 155. How can you ensure that your project's required dependencies are appropriately declared? How do you determine what dependencies are missing from a third-party project? This week on the show, Christopher Trudeau is here bringing another batch of PyCoders Weekly articles and projects.\n\nWe discuss the new Python dependency checker called Faulty Depths. The tool helps you determine if you declared too few or too many packages for your project. Christopher has brought several developer resource collections, including a list of assured open-source Python packages from Google, test databases with interesting data sets, and multiple Django third-party packages.\n\nWe cover several other articles and projects from the Python community, including a news update, advice on how to pitch yourself as a guest to the podcast, how to submit articles and projects to PyCoders Weekly, Pipi's introduction of trusted publishers and organizations, a tool for tracking package history, a pixel art paint program written in Python, and a project for efficient string matching with regular expressions.\n\nAlright, let's get started.\n\n[Music]\n\nPython Podcast is a weekly conversation about using Python in the real world. My name is Christopher Bailey, your host. Each week, we feature interviews with experts in the community and discussions about the topics, articles, and courses found at realpython.com. After the podcast, join us and learn real-world Python skills with a community of experts at realpython.com.\n\nHey Christopher, welcome back. No, I should be saying welcome back to you. You were off at a conference. Yes, this is true. I spent a week away in Salt Lake City. So that was fun. We'll talk a little bit more about it and then some other things that kind of came up that I thought about a lot during the conference. So we'll dig into that. We have our usual, actually kind of an unusual set of stuff, lots of little groupings of things today starting with a couple news things, right?\n\nYeah, a couple quick bits of news. The first one is something we've kind of touched on before with respect to Python and performance, and that's PEP 684, a per-interpreter GIL. So this has been approved. It's slated for Python 3.12 in the fall. For most of Python's life, you've been able to run multiple instances of the interpreter in a single process, but because of there being a fair amount of global state, there's the global interpreter lock, GIL, to its friends, except I don't think it has any. So this PEP, it doesn't really remove the GIL, but it draws boxes around it, meaning you should be able to get better parallelism, especially in conjunction with things like C-based extensions. So this is a step towards more performance. The 3.11 and 3.12 have both been very performance-conscious, and this will be another chunk in that. So this will be interesting.\n\nYeah, beyond the release, I'm interested to see where things like the NumPy and Pandas kind of folks interact with it. Because when you get into some of these libraries that are very C extension-based, it could be interesting what they're able to do with it.\n\nYeah, maybe that'll marry with some of that stuff of the arrow, the data frames in memory.\n\nYes, yeah. It'll give them more choices as to how they interact with Python for performance reasons rather than having to do a lot of that management themselves. It could simplify some other code. So it'll be interesting. It'll be a very interesting release this year, huh?\n\nYes, yeah. There's going to be a lot in it. A lot of it's underground, so it may just be like, \"Oh, here's these tiny little things,\" and it's like, but the performance goes through the sky. So it'll be neat. It'll be I'm always looking forward to it.\n\nThen the second piece here is the AWS Lambda service is now supporting Python 3.10. So if you want to write some serverless code, you can now do that with the match statement, which you wouldn't have been able to do a few days ago.\n\nIt takes them a little while to catch up, but that's good. We're not too far behind.\n\nI kind of want to do a little report on PyCon US 2023. They're in Salt Lake City. I drove to the conference from Colorado. I picked up Jim Anderson, who's I think been on the show once a long time ago. I'd like to get him back on the show, but he's also an author on the site and he lives in Fort Collins. And then we drove across Wyoming, and it was so funny. He had prepared all this stuff. He's like, \"Okay, I got podcasts and audiobooks,\" and so forth. And we just talked the entire way. So that was actually like, at the way home, we ended up finally diving into a little bit of that stuff.\n\nBut I want to say it was fantastic meeting listeners and fans of the show. Thank you so much for coming up to us at the Real Python booth and saying hi to not only myself, but also other members of the team. And it just was really great. Thank you so much for sharing what you liked about the show and what you like about the site and asking me questions. It was really fun. And just to let you know, a lot of people were wondering where Mr. Trudeau was and wanting to be, he was still in Canada.So I don't know, we'll have to see if I can drag you out someday to a PyCon. But someday, maybe on the East Coast, maybe it'll be easier. But yeah, really nice feeling. Thank you so much again. We were in the back of the expo hall, but people found us and a lot of new people checked out the site. That was really great. I let a lot of people know that we do a podcast. So if you're a new listener, it's great to have you. Thank you for listening.\n\nOne thing that I was excited about that was a little different from last year is I got to see a lot of previous guests who were unable to attend last year for whatever reason or were nervous about it. It was really great to connect with them again. A lot of people that were in the first two years of doing the show. Savannah Ostrosky, Sumina, Mike Driscoll, Tanya Allard, Russell Keith McGee, Wukush, Nina Zakarenko, and Josh Burnett.\n\nI briefly saw a handful of other people, Al Swaggart, Brett Cannon, and so lots of previous guests. It was really neat to see them and connect with them. One person that I talked to very recently is the director for the PSF, Deb Nicholson. We were discussing kind of getting the word out for the PSF, and there's a lot of things happening with the PSF this year. They're looking for a security developer in residence, and there's going to be a second developer in residence for Python, working with Lucas, and a lot more upcoming news and stuff happening.\n\nI offered her to come on the show whenever she wants to get that kind of news out. We can always carve out a little spot for her to come in and talk. The other person I saw that was great was Jody Burchell, and I got to hang out with her for a little bit. So that was nice. We have plans for her to come back and do some more data science stuff.\n\nMichael Kennedy stopped by from Talk Python, and I actually got to sit and have lunch with him and fellow podcaster Sean Tibor, who does Teaching Python. I've met a bunch of other potential guests, hooked up with them, and talked about it. I did a lot of work to wrangle future guests for the show. I'm going to be spending much of the next month plotting out and figuring out the summer and the fall of how to get in there.\n\nI wanted to do a lightning talk, but it didn't get accepted. The idea of it was something akin to what I was just talking about. If you are interested in coming on the Real Python Podcast or maybe you're interested in going on another podcast and you have a project, a story, a book, something that you want to get out in the world, how can you pitch that successfully? What kind of things would work and get our attention?\n\nI sat in on an open space that Paul Everett from JetBrains organized as an \"ask me anything\" with Brian Auchen and Michael Kennedy. I was able to ask them this question, and they answered pretty much the same things that I had thought about and was going to do in my lightning talk. So that worked out really well.\n\nOne thing I wanted to point out is you need to know that I get a lot of automated stuff that comes in my transom. The audience of this show is very educational, and my goal is to try to teach people new concepts or at least make them aware of these concepts and where they can learn more about them. My favorite thing is to give people resources so they can put their hands on and play with these things themselves.\n\nWhat you should pitch should be more of a topic or an idea as opposed to the person. Is there a learning aspect? How can the audience learn a little bit more? Things that I love when somebody's pitching this stuff to me is: have you ever presented this before? Have you thought about it to the level that you have maybe done a meetup and presented it there, maybe it was recorded and you can share that talk with me? Or conference talks or something you did at a meetup, some kind of presentation. If not, maybe you've written an article or a blog post where you've thought about this idea.You want to share it again geared toward teaching and exploring the language and stuff. Books are fantastic because they're just filled with that kind of idea. Typically in this platform, trying to teach. I will mine those resources for questions and send them in advance usually to try to get us really deeper into the topic and kind of learn about it. Obviously, the conversation can go lots of different ways. What tends not to work is really pitching the person, like literally sending me a resume, and the person really not understanding that kind of connection. I've had somebody recently send something where it's like this person's a CEO of this company and they could talk about python or devops or whatever and it's like those are so vast and so general that it's not gonna help me a lot. It's gonna end up being like a real hard time for me to dig something out of it. So if there's something that you've narrowed it down, it really helps and makes it much more interesting.\n\nIt sounds like an oversimplification, but you have to have a topic. I've been on Talk Python, primarily because Michael Kennedy and I had written a course for Talk Python. The course is an intro to Django, so you can't talk for an hour about an intro to Django without teaching Django itself. Nominally, the reason I was there was self-promotion and even in Michael Kennedy's own promotion as well because it's of course on his site. Instead, we ended up discussing a collection of packages. If you're a Django programmer, these are some packages that you might use and why. Listeners are going to have some interest in it and it was realted to what we were promoting, so it didn't come out of left field. You've got to have a topic. You may be the most fascinating person on the planet, but none of us are George Clooney. Even with Clooney, on late night talk shows, they seldom are showing up to do anything but promo the movie. You got to have a topic.\n\nSome people may be nervous about coming on a show, but if they've done the legwork, it helps. For those who may be nervous, I do edit the show and I will do my best to make you shine and sound great. My goal is to provide questions in advance and allow you to do some prep work. I will share a preview if there's something that needs to be changed. I try to include as many resources for you as possible in the show notes. Take some time and dig into those show notes. You wanted to talk a little bit about PyCoders, which is a format that we talk about. I've mentioned it at the end of the show. Tell people about PyCoders and how people could submit things for being included.\n\nIt's yet another newsletter in the python space. The content that Mr. Bailey and I are talking about every couple of weeks is mostly articles that have been posted in there. There is a submission form, and I'm probably rejecting 15 to 20 percent of submissions. The article has to have some content. Lately, unfortunately, there's been a lot of articles written by chat GPT with very light content. That quickly ends up in the bin. There needs to be enough content, a few hundred words at bare minimum, in order to be considered, unless it's a project.We do have a section on projects and if you submit a project that is interesting, we just add the project chunk at the bottom. But if you're looking for a link through to an article or something like that, then it has to actually be an article. We generally avoid anything off-wall, so if you have to be a member, most people clicking through are not interested, so we tend to bypass that as well. \n\nIf you're thinking, for example, if you want to promote a project, write a blog post on that project. Then you're more likely to get an article on it than just sticking the project at the bottom. But if you're going to do that, submit the article first because if you end up in the projects and then the next week I get the article, I'm probably going to go, \"Yeah, I just covered this,\" and it's going to look like a repeat.\n\nUnderstanding who your audience is and what you're after is important. We do occasionally include general programming stuff, but a vast majority of content is Python and Python-focused. So, if you're trying to push something that isn't really Python-related, you're much less likely to get in the listing because it's a Python newsletter.\n\nIf you've been listening to the show, you can get a good vibe of the kind of things that we're excited to have you include. And awesome, well, I just wanted to share that stuff, and again, thanks for anybody coming by at PyCon US. It was fantastic for you to reach out and I hope to see you in my inbox or at the next conference.\n\nSo, what's your next topic here? I'm starting out with a trifecta, which is just fun to say. Sure, so three different articles, they all have something in common. They're all resource lists. This kind of content is actually great, exposes me and other readers to libraries that you might not otherwise come across, but they don't have a lot of details so that I don't have a lot to say in the podcast about them. \n\nBut I figured we'd go over a couple of them and just highlight that they were there. The first one is Google's assured open-source software list. If you haven't come across this before, Google publishes a list of packages that they've verified and that they use in their own supply chain like in the cloud, etc. But even if you're not using their cloud, you can take advantage of this because you can look up and go, \"Oh, well they've validated that version.\" The list has libraries for Java and of course Python. The Python work has been done on Linux and Python 3.8. So kind of like that AWS comment you made a little bit, they're a little bit behind because it takes time to validate all of this stuff. As of the recording, there are 574 Python packages on this list.\n\nNow to fill airtime, I will read them all in a slow voice so you can write them all down. The list starts with AB Sale Pi, a library from Google for building Python applications. Next is and the last is zip with two P's and it wouldn't surprise me if Mr. Bailey edited some of that out. In seriousness, I was able to find a couple libraries I regularly use that aren't on the list, but only a couple. Pretty much everything I could name off the top of my head was there. So interesting little thing to see.\n\nThe next article in my collection of collections is called \"Groovy Datasets for Test Databases\" by Esther Schindler. If you've ever needed some data to give your program a little push while testing, this is the article for you. A lot of the content here was gathered from Jeremy Singer-Vine's \"Data is Plural\" newsletter. It includes stuff like info gathered from the Star Trek API, FIFA World Cup, World music, and more. This kind of data is fun to play with, but you can also use it to help exercise your code instead of working with say a few lines of test data, you can actually deal with something that's representative of real content.\n\nThe last article is Will Vincent's \"Top 10 Django Third-Party Packages.\" The title more or less says it all. It includes some of the common stuff like the Django Rest Framework and the Debug Toolbar, as well as some less common libraries like Django Storages and Django Environ. Although the article says top 10, there's also a bit at the bottom where he adds another dozen and then there's another dozen that are Python libraries which are commonly used when coding with Django. So even if you're not a Django person, there's a bunch of interesting content in here. Sounds like a good collection.\n\nI've got a new item on my bucket list. I want one of my open-source libraries on Google's list. I'll have to come up with a name that shows up before AB Sale Pi alphabetically so that I'm first. But and underscore or something, something. Yeah, I don't know what AAA is gonna do, but it's going to be my Python package on Google's list. We'll get there.Google was represented very well at Vicon, a lot going on with packaging on their side also in assuring trust of packaging. I think it's going to be an interesting year. We talked a lot about last year paying attention to your dependencies. There are going to be lots of resources coming up, so that seems like a nice one there too.\n\nThis week, I want to shine a spotlight on another Real Python video course. It fits in nicely with all the news and stories we had this week about the Python Packaging Index (PyPI). It's titled \"Publishing Python Packages to PyPI.\" The course is based on an article by Garana Yella, and my co-host Christopher Trudeau is the instructor. He's going to take you through why packages and virtual environments exist, how to use build systems, what are the contents of the PyProject.toml file, how to use the build and Twine tools, and what Poetry and Flit tools offer. Most importantly, you'll learn all about the structures of a package and how to upload your own to the PyPI server. I think it's a valuable investment of your time to learn how to share your work using the standard tools within the Python community. Like all the video courses on Real Python, the course is broken into easily consumable sections, and you get additional resources and code examples for the technique shown. All of our course lessons have a transcript, including closed captions. Check out the video course; you can find a link in the show notes or use the enhanced Search tool on realpython.com.\n\nThere is an announcement plus a project. The announcement of the project is called \"Faulty Depths.\" Faulty is spelled F-A-W-L-T-Y. You might get the joke right away; it's the Monty Python adjacent Faulty Towers sitcom that John Cleese was on. The \"Depths\" part of it is a dependency checker for Python to find undeclared and/or unused third-party dependencies in your projects. The article is on the Tweag blog. It's got four people listed on it: John Hurland, Nor Al Mwas, Maria Norps, and Vince Reuter. The first line of the article is, \"It is a truth universally acknowledged that the Python packaging ecosystem is in need of a good dependency checker.\" The idea of the declarations in your projects can go wrong in kind of two directions: you declare too little or you declare too much.\n\nYou might think to yourself, \"Well, my linter should be the solution here, right?\" The linter is going to look for references in your code and make sure there's like an import statement or potentially the reverse if something is imported but never actually implemented in your code. This is actually kind of taking almost like a layer, if you will, if you think of the layers of networking things like that. It's more on the overall project and the reproducibility of your project as a whole, trying to avoid the dreaded \"hey, it works on my machine\" kind of thing. Where the dependencies can be defined, you might be familiar with the idea of a requirements.txt file that somebody can use to pip install the things that you need in this project to be able to run it properly. It's more of that kind of thing: a PyProject.toml file, setup.py, or setup.cfg. These are all things that Faulty Depths will look at to determine what's been defined or declared as dependencies in there and what's missing or what's included and isn't being used, and so forth.\n\nSo if we can kind of see how it's kind of like a layer above what a linter would do, it's more looking at those types of files in that kind of relationship with the overall project. I tried it out with a project I'm going to talk about in a little bit, and I found two legitimate missing dependencies. For this case, they were installed actually by me, but I had to install them after the fact because of an error that appeared. So when I ran this on it, it popped up and said, \"Hey, there are two packages that are required by this project but they're not there.\" So I just went down what probably is a familiar path for a lot of people working with pip. If not, we've got a lot of resources on Real Python that can help you out with doing this. You can freeze a set of requirements so even though I've installed this project by a third party person, it needed these files. They were not included in something like a requirements file for me to rebuild this exactly how they had done it. And so by running that pip command freeze and sending it into a requirements.txt file, I had solved the issue.\n\nThe article is a bit of an announcement, as a few cases and a few examples, and kind of really sort of saying you know what this thing does and how it will do it and some graphical examples of it and a few of the commands. But I really liked the readme on GitHub; it's way more detailed. Dives much, much further into what this could do. I think this might be another useful tool. Have you used anything like this, Chris? No, most of the time I'm dealing with my own stuff, so I'm already there. I think it's more valuable when like you were saying when you're trying to splunk through somebody else's code, and I fortunately haven't had to do that in Python.Okay, I could have used this for other languages in the past with my career where trying to figure out what they were doing and why. Does it make sense? Is that different than what a linter is doing? Yeah, they serve different purposes. I guess it's a subset of what linters do, and not all linters do it. \n\nIt seems very specific toward this one goal. A linter may have that as a feature, looking at import statements and stuff like that, whereas this is really trying to make sure that all your dependencies are there and so forth. One of the claims is it can help reduce the size of repos by not adding things to a project that are not needed or never touched. Programs change as people go, and very often we might have things saved in requirements files or done in earlier configurations, and then they never got removed. Dead code, dead imports, that kind of stuff.\n\nIt's an interesting project, faulty depths, so you have another collection here, it's my meta week. This is two more articles, also interrelated. These are the first two real posts on the PyPI blog, and I'm saying real because I'm not counting their Hello World entry. The most recent is called PyPI introduces trusted publishers and was posted by Dustin Ingram. \n\nEssentially, it has to do with the addition of new security features at PyPI. What they're calling trusted publishers is OpenID Connect, which is a layer on top of OAuth 2.0. So we're just flying with the buzzwords here. This essentially is a way of entering credentials and allowing you to get PyPI to trust a GitHub repo and a workflow rather than putting in your username and password for PyPI. It essentially is another way of authenticating for the publication package, and tokens go back and forth instead of transmitting credentials. \n\nBecause this is tied to a repo, it means you can manage your code security through the repo's security. Now I don't have to worry whether or not he's got credentials on PyPI at all because it all links together. The intent is that this is a first step towards a bunch of new security features. There's more in this space that's coming. The other piece, which smells like it might even be connected, they're not explicit about it, though, is a blog post from E. Durbin called Introducing PyPI Organizations.\n\nThis is a mechanism for creating accounts around organizations rather than individuals. It allows for self-managing teams. Community projects will be able to get this feature for free, while corporate projects require a small fee, and the revenue is going back to adding better PyPI support. Like the security stuff, this is a base ground for more interesting future features.\n\nWhile I'm on the topic of PyPI, I've got a three to go with my two here, a quick call out to a project that I've come across called PyPI diff. It's a bot that regularly looks at PyPI's catalog and stores a diff from one day to the next in the repo. So you don't even have to run the bot yourself, you can just go check out their different repos and programmatically figure out what has happened for package releases. If you're trying to monitor whether or not there's been a patch for something in your chain, looking at these repos would be a way of scripting that. \n\nThis might add on top of that the idea of like activity and it's this thing currently still active and being developed on that might be another resource there. My need for it would generally be with patching packages, right? You don't want to get too stale. I'd love to be able to sign up for this is my list of 10 packages associated with this project and then somebody email me when 1.10.11 becomes 1.10.12 so I can go and see if I want to use that or get on top of it. This data would be the first step to that, it just needs the extra please email me part afterwards. So if you're out there PyPI diff and you're listening, email me.\n\nThat gets us into projects this week, speaking of things we cover, and this one is a lot of fun. You shared it kind of last minute with me, and I was like, oh, this looks really cool. I'm old and a fan of laugh because I'm also old. I'm a big fan of pixel art, and there's a paint program that was developed by Electronic Arts back in 1985.And so, you can kind of imagine the types of computers that it ran on, but Commodore had a popular machine at the time called the Amiga. This paint program was also available on the Gem platform, which was Atari's ST computers, and that's where I had seen it before. It's called Deluxe Paint.\n\nJust seeing the picture of the Tuton Commons death mask on there, it just totally brought back memories. Like, oh my gosh, I remember that image. There are a few others that were used very often in Deluxe Paint. But this individual created a project, his name is Mark Reali, and he has faithfully created a reproduction of it. The key word he has in there is the first line, a usable pixel art program written in Python. Its main tool that's using kind of under the hood is Pygame, and then it also uses NumPy. Those are my things I needed to add, by the way. And it did warn me when I ran it the first time.\n\nPixel Art is really popular right now, and that kind of retro look of many games, like maybe you've heard of Dead Cells or Vampire Survivors, or any number of games that's not on Steam right now. How do you create those kind of Pixel Perfect little individuals? Most modern tools are kind of geared toward photography or vector graphics, or other kinds of things. And this is way down in the raster individual pixel kind of thing, and it has all these kind of funky interesting tools. He has a really nice tour of the program. It doesn't speak a word, he's just touring through it and showing you all the sort of features on a YouTube video, which I'll include a link to. Mark feels that the old tools may be a better solution than some of the more modern tools that are overly complicated or crude in design. But he's recreated the menus in the file format, and I got suggested a thing on YouTube as I was looking at it, of this gentleman who's running an Amiga emulator. He's basically running the original version of Deluxe Paint on it, and he was like, \"Well, can I import a file, move it across, and feature-wise and so forth?\" And he was super impressed with it. And he's like, \"Okay, this foregoes me needing to run this emulator. I can just run this from a command line, or potentially you could package it up because he did package it up as an exe file to run on Windows if you want. But I think you could probably package it up yourself.\"\n\nI cloned the repo, set up a virtual environment, and installed those two additional requirements, and it was really neat. I played around with it for about half an hour and I sent some screenshots to a friend of mine who is really into art back in the late 80s and 90s. His response was, \"No way,\" which of course I had to say, \"Way.\" He's got a coffee or k05 donation page, and he's looking for people if you're interested in helping out. I don't know if I mentioned even the name yet, but it's called Pied Painter. So, pot pyd is the pied, and then painter. If you're interested in helping out on the project, he's looking for help on GitHub. There, I'll include that link. He also has a Facebook group where people were talking about the program. It's a neat project. I was impressed with just how complete as a paint program it was and definitely brought me back to the late 80s.\n\nSo, the sequel that we're actually four or five of them, but Deluxe Paint 2 ended up on DOS, and so it was available along all the PCs and stuff. And if I remember correctly, the actual executable was dePakEd. A lot of people referred to it as D paint, so hence, Pi D Paint ER, yeah. His logo is definitely pied, which makes me think of Silicon Valley.\n\nAlright, what's your project? So, I've got a quick one this week. This is called TR Rex, although note that the GitHub repo is called t-rex with a single R, so it's a little confusing. This is by Daniel Masejo. The core of this is quite simple. It's a regex utility where you give it a bunch of words and it builds a regex like union that can find each of those words. The function returns an actual Python regex pattern object, so once you've constructed it, you can use it just like a regular regex.\n\nThe advantage here is he's doing some tricky efficiency stuff underneath, and so you're getting a 300 times performance boost than say naively sticking a regex together in a union. He's taken some ideas from some similar other regex tools and essentially stuck them in an easy to use Python-only library with no dependencies. This really has one particular use, but it does it really, really well and it's screaming fast. So, if you're finding that you are using regexes to search for multiple words and multiple occurrences of those words, then this Library will make your code way easier to understand and a lot faster, and your head will hurt less. Go check out TR Rex.\n\nThe one thing I haven't delved really deep on is Chat GPT and these other kinds of things.I think that might be one of the most useful things I've seen as a use for it is to help write reg X for you as a prompt. The old thing, right? If you have a problem and you use regex to solve it, now you have two problems. Something tells me adding chat GPT is adding at least an accident in that problem somewhere. I don't think it goes to three; yeah, I think it goes to some other. There might be a 10 to the power of in that joke now.\n\nSo, what string are you searching for today? Yes, exactly, exactly. Thanks for bringing all these articles and projects this week, Christopher. Okay, a pleasure to do it. And like I said, go submit always happy to see new stuff coming in. We'll include links in the show notes for all of that and we'll see you all soon. Thanks.\n\nI want to thank Christopher Trudeau for coming on the show again this week. And I want to thank you for listening to the Real Python Podcast. Make sure that you click that follow button in your podcast player. And if you see a subscribe button somewhere, remember that the Real Python Podcast is free. If you like the show, please leave us a review. You can find show notes with links to all the topics we spoke about inside your podcast player or at realpython.com/podcast. And while you're there, you can leave us a question or a topic idea. I've been your host Christopher Bailey. I look forward to talking to you soon.",
    "-0iIa1Xt7rg": "Welcome to the Real Python Podcast. This is episode 156: How do Python Virtual Environments work under the hood? How does understanding these concepts help you with managing them for your projects? This week on the show, CPython core developer Brett Cannon returns to discuss his recent articles about virtual environments and the Python packaging landscape.\n\nBrett talks about his recent article on how virtual environments work. He was researching the topic to solve an issue with a Linux Python distribution that doesn't provide the tools to create virtual environments. We talk about how he solved the problem by creating a tiny library named microvim. We also take a look at the Python packaging ecosystem.\n\nBrett talks about the early days of Python where these tools didn't exist. He contrasts that with the current packaging solution explosion and how each one attempts to solve unique problems. We also discuss the Python packaging user survey and the plans for packaging Summits at PyCon US.\n\nAnd a programming note: we recorded this episode two weeks before PyCon US 2023.\n\nAll right, let's get started.\n\n[Music]\n\nThe Real Python Podcast is a weekly conversation about using Python in the real world. My name is Christopher Bailey, your host. Each week, we feature interviews with experts in the community and discussions about the topics, articles, and courses found at realpython.com. After the podcast, join us and learn real-world Python skills with a community of experts at realpython.com.\n\nHey Brett, welcome back. Thanks, it's good to talk to you again. A little bit longer about... I thought we could talk about virtual environments and packaging a little bit. I know we were already kind of discussing packaging a little bit along with the waziwasm stuff that we were discussing earlier. You've had a handful of posts on your blog about packaging and virtual environments, and I think this kind of lands a little more squarely in your day job kind of stuff. Is that correct to assume?\n\nYes and no. It depends on what you're talking about. If it's the virtual environment stuff, then definitely yes. If it's packaging, kinda okay.\n\nWell, you had a bunch of little posts going back a little while. I think I'll maybe start with one of the more recent ones. I did discuss this on the show recently, bringing it up as a topic. The blog post was about how virtual environments work, and I found it really fascinating, this idea of under the hood what's truly happening. It's very different from how to use them, which is always the instructional stuff that we do on Real Python, and we have lots of courses on that. Just like, how is the actual thing structured? If you were gonna yourself build a virtual environment from the ground up, and you went down this path for an interesting reason that is related to your day job.\n\nYes, for the Python extension in VS Code, we want to help users be successful. Part of that, from our perspective and my perspective as well, is getting people to use environments more, whether they're virtual environments or conda environments. We have found enough people who have messed up their machines or python installations or just wondering what's going on because they didn't set up an environment that we wanted to do what we could to push people in that direction. We know it's a stumbling block for people when they don't know about it to begin with, or if they do, they don't quite know what to do.\n\nSo, we're starting to take the first steps towards that. The first thing we did was we added this \"create environment\" command to VS Code, which will ask you what kind of environment you want. Right now, it asks if you want a virtual environment or a conda environment.\n\nInteresting. And for this conversation, we'll go down the virtual realm path. After you say that, we ask which Python interpreter we found. We call them globally installed interpreters. Basically, the ones you've installed on your machine. Like, which one do you want to use to create this virtual environment? And then we create the virtual environment using the venv module in a .venv directory in the workspace. Slightly controversial for some people, but the current workspace setups are working.\n\nYeah, the directory you opened inside of VS Code itself, we call that the workspace. Basically, we put it in the .venv directory because it's the closest I can find to commonality in the community around this name. It's either that or just venv without the period. And that's actually something I found out through one of my polls that we talked about on a previous episode. Fast Community, like do people do this? And there was no consensus, no majority of anything. So I went with .venv because it hides it in VS Code from searches. Usually, people when they search code, they don't want to look inside their virtual environment, they just want to search in their own code. So, it shrinks the search space, makes it easier to navigate their code. Yeah, that's why we did that. And honestly, the community seems in general to do that as well. Roughly, it's not 50-50 absolute, but relative 50-50. The .env to .venv without it. So we just went with it, that hidden file. Just to kind of pause you for a second.I know it's usually like a system-wide thing as far as how the operating system Windows or Mac, I'm not sure on Linux, chooses to show hidden things. Does VS Code pay attention to that and show it, or does it automatically show it? It automatically shows it. It just grays it out a bit. Okay, can I just signify that? Well, sorry, I take that back. It will show it, and then whether or not it's tracked by your version control system will dictate whether it's grayed out. But on VS Code, we always show it. Actually, I think Windows shows dot files consistently at this point. The Mac, by default, has it off, and it's like Shift Command period or something like that. \n\nFrom our perspective, the virtual environment is borderline implementation detail for getting your work done. So it's not something we want staring people in the face either. This is once again one of the disagreements in the community when I've brought this up. Some people dislike it, people have opinions, and that's kind of the thing with the recent PEP as well, right? The one that suggested to you, \"Hey, looks like you're installing things and you aren't putting them in a virtual environment. Are you sure you want to do that?\" Yeah, so that PEP actually just got rejected. I can't remember if PEP actually rejected it himself or if he withdrew it. \n\nThere is actually an environment variable you can set. I think it's `pip_requires_Venv`, and that will cause Pips to fail if you try to do an install into anything but a virtual environment. It also suggested to go with the name `.venv` if you're going to create the virtual environment in the directory with the code, trying to standardize it. So we create in there, we also put it in the workspace. I had a whole blog post on kind of where virtual environments exist. \n\nI hear people go, \"Oh, I want to use Virtualenv wrapper and keep it in this global directory over here,\" and all that's where all my workspaces are, etc. And then I've always just kept it in the directory with the code. Am I the weird one here, or is it? But I definitely do what you do. I have a really odd workflow because I manage a group of people and all of their projects, so it made sense that I'm going to stand something up and then I'm going to just delete the whole thing. I'm not necessarily going to have a reusable environment. So there seem to break down into kind of three groups. \n\nThere are the people who do what it seems like you and I do, where they create the virtual environment in the directory with their code. It's tied to that code, it's related to that code, so let's keep it in the director with the code. It's temporary, and it's not going to get committed. There are other people who want to keep everything in a global directory, usually for cleanliness reasons or because they prefer having it all in one place. They have one directory to delete if they want to delete all their virtual environments at once for space reasons.I personally am one of these people who has a repositories directory and the directory structure follows the org repo structure on GitHub. So I actually know exactly where all mine are, so they match pretty well. For me, it's not a big deal, but some people obviously just don't work that way. I totally fine, so they want an all-in-one directory. Then there's the other one where, as you said, people have a single environment that they reuse between projects.\n\nI found content users and scientists fall into this camp a lot more than I was expecting. They don't want to have to set up or reset it up, and honestly, their toolbox is very consistent. If someone is just doing a lot of data analysis at a company and constantly looking at the same kind of data, it's not totally unreasonable to think, \"Alright, I'm gonna need numpy, pandas or polars, matplotlib, Seaborn, or whatever tools they use.\" It's the exact same tool set for everything every single time for their analysis. They just want to set it up once and reuse it because otherwise, they're reinstalling the same thing over and over again, which seems wasteful.\n\nPersonally, my concern with that is always reproducibility. But once again, if you're using this for yourself to just get a number, like when your boss has said you need to calculate something, you figure it out and then you're done. You don't have to show your work or rerun the numbers. Another wrinkle there is maybe the cloud versions of Jupiter things where all that stuff is pre-installed, which feels similar.\n\nThinking back to the Anaconda distribution, which is massive and most people have moved away from at this point because it's so big, it is interesting. For people who are still doing a lot of work on their desktop, they have it set up this way. But when you start to think about running stuff in the cloud or having GitHub code spaces where you have an ephemeral machine to work in, the persistence becomes a problem.\n\nI personally live my life in a way that if my laptop were to break, it would not take me more than half a day at worst to get set up on a new machine because everything I have is backed up somewhere. The idea of having a magical environment that I keep reusing seems like asking for trouble. That's not how I mentally work.\n\nThe groups broke down into people who do it locally in their workspace and people who do it globally for cleanliness reasons or easy maintenance. It's really hard to connect your code to your environment if it's not in the workspace. We have this problem of having to ask you, which kind of sucks.It's an extra step in terms of getting started. We'd much rather have you be able to just open up VS Code and everything works as best as we can. Especially if you already had it set up before you launched VS Code. If you had a directory with a project set up, we'd love it if you just launched VS Code for it and it picks up everything that's going on. You're ready to go, you're already set up, we're just setting up VS Code to work with what you have.\n\nWhen you put in a global space like I don't know which one you want, so we have to ask you, we have to list all of them. So if you have a bazillion environments in there, we have to say sorry, we don't know which one to use, you got to tell us out of your 200, which one you want us to pick up. This is why we personally put it in your workspace because it completely disambiguates its purpose. And then after that, we ask, we do installation if you have a Py project file and a build system in that table in there. We do an editable install for you and then we also ask which optional dependencies, which extras you want to install from that. We give you a list of that. And then if you also have requirements files, we basically mention the glob star requirement star dot txt. We list those and say which ones do you want us to pass to PIP and then we pass those to PIP in your virtual environment and say install it all.\n\nThe reason we went down this whole road though is back to MicroVim. The reason I started to blog about this and look into this and how it works is because on Debian, you do not get Venv in your Python install nor do you get PIP. That's really strange, it's a policy that Debian holds. Basically, for Debian, if you install something from apt-get or apt, it's supposed to represent a single application. To Debian, PIP are separate apps compared to Python, so it goes against Debian policy once again. If you install Python and then you get this second app called Venv and this third app called PIP. So they broke it out. That's why when you do apt-get install python3, you don't have them, then you don't have PIP.\n\nBut as I said, we're trying to make this easier for beginners and telling beginners who are using a Debian-based OS, which is everyone on Ubuntu and by the way everyone who's on a Chromebook that launches Linux because that's what it's going to have, they're not going to have it. Now luckily, the Debian project is going to be launching a Python 3-4 package which is kind of an umbrella package that includes all of this stuff. But once again, A, that's got to get released, B, it's gotta trickle out whether if you're not using Debian to the Linux distro you're using that's based on W and C, you've got to know it exists and have the hopefully have your documentation updated so you know to use it. It's going to be a while.\n\nWhen I took a look, I was asked at work to solve this. The problem we have at work is the whole VS Code team for one week a month every release stops coding and we do what's called endgame. All we do is test the product. This is literally like leading to the word experience in your time. But I mean this is the whole VS Code team, the core side, every extension, we're in charge of. Anything we do gets tested during this week. The problem we run into is people who were not working on the Python side of stuff were being asked to test the Python code and the Python experience stuff. They were going like, well, how do I get this working? We eventually just started to tell people to use Dev containers, which is an easy way to launch a Docker VM with stuff in it. It's easy to install Python in a new Docker container. People were doing this for Ubuntu because a lot of people like Ubuntu. I'm a Fedora person personally for various reasons.\n\nThey constantly were running into problems, especially with the create environment command because, well, I'm getting this error message. It's like, oh yeah, well, I'm on Ubuntu. Oh yeah, you got to install this other stuff because once again they're not typical, I mean VS Code is written in TypeScript, so they're not day-to-day Python developers. So they didn't know they had to install them, they didn't know they had installed, they didn't need these two others.Yeah, they didn't know about the two extra steps, the two extra things they had to install. So my manager is like, \"Yeah, we kind of had to fix this, right? More and more people are probably going to be using Dev containers, the default container image that's kind of the kitchen sink one. Even GitHub Code Spaces provides is Ubuntu-based. Luckily, that one does install Venom and pip because they asked me, and I told them, and sell them, and pip. So at least that's taken care of. Make sure you do this. But in general, people run into this, right? Same thing as I said with Chromebooks, right? Like education place. If you say, \"Oh, you're going to use Linux, we'll just install the Linux feature in Chromebook.\" Well, that's Debian, so we realized we had a problem. It was like, \"Well, can we just give a better error message if it's not there for these users?\" Like, \"Yeah, we could, you know.\" But I was like, I basically know how this works. This doesn't seem like this should be that complicated, right?\n\nAnd that's what led to that blog post. I looked into it and really read the code closely, and it was like, \"Yeah, this is literally just a directory structure and a config file, which is plain text and stupid simple to construct, right?\" Like, it's in the blog post. It's five lines at most. Yeah, and it's like, \"This has got it. This is pretty easy. It seems too easy.\" I effectively nerd-sniped myself, saying, \"Hey, this is going to be hard to make work for Debian users. We'll give them an error message. I don't want to go down the road of trying to work around them not shipping it.\" And then I nerd-sniped myself into solving the problem in under 100 lines of Python code.\n\nOkay, and that's why that came about. Was hopefully in the next stable release, which would be the one at the beginning of May. The hope is we'll have the work done such that even if you don't have venv installed, we'll still be able to create you a virtual environment. We won't have the activation scripts available because I don't want to maintain activation scripts for multiple shells, and technically they're not necessary because you can actually run a virtual environment by path. The activation script is just nice to have when you work in a terminal. Okay, so you just don't need it.\n\nSo, yeah, VS Code can be active, quote-unquote, then the environments it could be using it. So yeah, we don't actually require it. So as an example, when you hit the Run button in VS Code, what we do is we just specify the full path of the virtual environment, and then Python just knows that it's a virtual environment and just handles it. There is no requirement that you actually activate that terminal. You don't have to put Python on path. I mean, honestly, all those activation scripts do is three things: they tweak your prompts, they put the bin or scripts directory, depending on which OS you're on, at the front of path, and they set a virtual environment variable to the director of the virtual environment. That's it. And that virtual environment variable Python doesn't even use it. It's just a thing to let you easily tell back how to go back to the directory, but you can even calculate that without it. So none of it is necessary for Python. It's purely just for your usability if you find it useful. I honestly never use it.\n\nBut you're primarily in VS Code. I'm in VS Code, and honestly, I designed the Python launcher for Unix. That's specifically so I didn't have to activate it anymore. So it's actually designed for my workflow perfectly because which does the same kind of thing. Yeah, exactly. Py, exactly. The py command tool that I wrote for Unix, there's the one for Windows, it's slightly different, but that one is detected. Is there a venv directory? If so, just automatically find the Python in there and just run it. Because I don't have to do anything special to make that execute.\n\nSo, yeah, so I wrote microvenv so that we can just embed it in VS Code in the extension so that you will just always have a way to create a virtual environment. And then on top of that, I realize that there's a cool little trick we can do to get you PIP. So there's a website called bootstrap.pypa.io which contains PIP and virtual files, right? Just basically the wheels and other things. But on there, there's also a pyz file, which is basically a zip file containing Python code that Python can execute. And one of them is pip.pyz. Okay, so what we can do is after we create your virtual environment, if you don't have PIP, we can actually go on, go to bootstrap.pypa.io, download that pip.pyz file into the internal extension cache, plop it down, and just point your virtual environment at it and just say, \"Hey, go install PIP.\" And then that can grab it from PyPI, install it into the virtual environment, and then effectively, short of those activation scripts, it's no different than what venv would have given you.\n\nAnd it all just bootstraps itself up, right.And then suddenly it's just fully usable. VS Code can use it fine. You can use it fine. As I said, the only thing that's not there are those activation scripts. Now there's a myriad of ways to not have to use them. As I said, they're not required, it's just a nicety and it would grab the latest one. Also, as opposed to potentially having to update it.\n\nYeah, I mean basically for simplicity's sake, we will probably literally just download that file and just say install pip from PyPi. That way we can cache it. So if we download it on day one of a release of the extension and then pip makes a new release halfway through the month, we will always install the newest version for you into the virtual environment. But you don't have to worry about the one being cached out of date. It also just means we don't have to think about installing what's in the zip file into your virtual environment or you having to rely on VS Code to do installs for that virtual environment. It makes that virtual environment self-contained.\n\nOkay, right. Like a key thing for us, at least for VS Code, is we want to be a tool and an editor that works with your setup. We don't want to require you to use us to be your setup. So we don't want to make it so that if you stop using VS Code, your setup, your workspace, and everything you set up stops working because we were using some magic we provided. We try to hook into what you already want and just give you a nice UI for it and just integrate among other things. So we wanted to make sure that if you said you could go, yeah, if you want to use them correct as a terminal or terminal, yeah, Emacs, whatever you want. If for some reason you walk away from VS Code, that virtual environment will still have pip installed in it.\n\nVersus, \"Oh, I don't have pip right now. VS Code was giving me that feature, but I stopped using it. So now I'm kind of in a bad spot. Now I got to recreate this virtual wire.\" It's like, yeah, we didn't want you to be in that situation. Hence why we're installing pip into the virtual environment itself. Okay, that makes sense.\n\nOne of the things that you did in your sort of experimentation going into it as you used the event tool to say, \"Don't install pip.\" In your exploration of that, you realized, \"Wow, this is really fast.\" And tiny here like, and I did the same just to try to practice it, and I was like, \"Oh, it was like instantaneous because it's not downloading pip and installing it.\" And then I thought, \"Okay, well, I know I have this virtual environment. What can I do?\" And I can't really do much. Like, you can't without pip. I was trying to think, \"Well, can I call the system pip or do other kinds of things?\" And it felt like that was kind of broken. Like, that was difficult for me to do. And maybe there is a reason why you wouldn't want pip in your virtual environment, but I'm trying to think of kind of pluses and minuses there. It definitely would make it smaller, but in some ways it breaks a lot of functionality.\n\nSo, yeah, so I pointed that out in the blog post because I wanted to make very clear the difference between creating a virtual environment and installing things. People conflate the two because historically they happen simultaneously or you do them one after the other. People kind of don't think about the fact that when you create a virtual environment, it historically automatically installed a set of tools in pip as part of the creation. But those are steps that it's doing, yeah, exactly. But if you choose not to do that, the actual creation of the virtual environment is instantaneous, almost like on Linux on any Unix-based system, it's three symbolic links, one file, and three directories. Yeah, maybe a fourth in lib64, right? Like literally, that's it. That is all. It's crazy fast. And on Windows, instead of those symbolic links, those three symbolic links, you're copying over like two files or three files. I can't remember. I didn't bother supporting Windows with MicroVim because Windows installs all come with Vim and pip, so it's just not a problem.\n\nBut as you said, like without it, how do you make that useful, right? Because the whole point of virtual environments is to isolate what you install. Well, so there's a couple ways to deal with that.One way is that PIP actually has a `--python` flag that allows you to specify the path to the Python you want to install into. So, you can do that as one way to do it.\n\nAnother way is if you point to the `pip.pyz` file I mentioned earlier, you could just point your Python at that and have that copy be the one used for PIP to install into. This way, you don't even have to install it separately.\n\nThe third way is maybe you don't want to use PIP. PIP happens to be the tool we all use for installs, but it doesn't have to be the only option. There are alternatives out there, and maybe there will be more in the future.\n\nFor me, the key thing is to help the community think about the fact that just because something has always worked a certain way doesn't mean it has to continue that way. There are alternative ways to work with it to make things easier.\n\nOne idea I have is to support subcommands with the Python launcher for Unix. This would allow for more flexibility in using different Python interpreters and environments. It could simplify the process of managing PIP and keeping it updated.\n\nMy dream is to eventually add subcommand support to the Python launcher for Unix and have a `pip` command. This command would ask the Python launcher which interpreter or environment it is using and then run PIP with the necessary arguments.\n\nI can also automate the creation and updating of virtual environments behind the scenes, making the process seamless and efficient. This personalized workflow could open up new possibilities for handling PIP installations and updates.I just happened to have written the right tools to make this all happen for myself someday. This is why I had this whole concept of not installing pip into the virtual environment because technically you don't have to and there are some interesting opportunities if we don't. We can kind of get the tooling around it to make it so it's not necessary. Some things to think about for upcoming stuff. Plus, as I said, you also just have to deal with the fact that it's not on Debian, so you gotta kind of figure out some solution to it.\n\nThis week, I want to shine a spotlight on another Real Python video course, the Python Standard Shell or REPL. The Python Standard Shell allows you to run Python code interactively while working on a project or learning the language. This tool comes with every Python installation so you can use it at any moment. In this course, it dives deep into the topic and is titled \"Getting the Most Out of the Python Standard REPL.\" It's based on a Real Python tutorial by Leodanis Pozo Ramos and presented by Darren Jones. He takes you through how to run the Python Standard REPL for an interactive shell, how to write and execute Python code within an interactive session, how to quickly edit, modify, and reuse code in a replication, how to get help and introspect code within an interactive session, and how to tweak some of the features of the Standard REPL using a startup file. He also covers how to colorize output using the rich library and how to identify the Standard REPL's missing features that might be covered by an alternative REPL and showcases a handful of them. If you've been using Python for a while, you've most likely spent some time within a Python REPL, but maybe you aren't getting the full use out of it. This course is a good investment of your time if you could use some of these tips and invaluable if you're just starting your Python journey.\n\nOne of the things that I think we touched on a little bit was classifying Python virtual environment workflows. That was your one where you were sort of questioning people online and I think we hit that pretty well. We touched on how you typically create virtual environments in the kind of mirroring that you do of your GitHub layout as far as the directory structure. What I'm wondering about is kind of the reverse of all this. Okay, now I want to share my code, I want to distribute it, and I want to package it. It's been interesting watching as a commentator on the Python world to see a lot of conversation happening. There was a survey that the Python Packaging Authority did, and that generated even more conversations. I don't know, some people are more upset than others about the idea that it's complex. I don't have that level of agitation about it. I find a tool and I use that tool, and as long as it's still around and supported, then I'm doing okay. I kind of wonder some of your thoughts on it, but also what tools are you using? There's one weird thing about it was when I went to look at this stuff and I hadn't surveyed it in quite a while, and there were three new tools I've never heard of. Speaking of Russell, who we mentioned in the last episode, he had commented on it and he had said, \"These are ones that are now considered supported and are sort of suggested by the Python Packaging Authority.\" They were new, which was very interesting. I don't know, just maybe we can start with what are you using for packaging yourself and has that changed a lot recently? My needs are a little weird because of the projects I work on. For instance, I'm one of the co-maintainers of the packaging library. That's a base low-level library that tries to implement a bunch of the packaging peps. It is the library that most people use to parse version numbers and to parse requirements specifications. Because a lot of Linux distros like to bootstrap up from a clean system, everything we personally use Flit for packaging. But that's because Flit has no dependencies. So, a Linux booster can then just download a zip, download the wheel file, unzip it manually, copy the files in the right spot, and then suddenly they can build packaging from scratch, which is one of the baseline requirements for a lot of projects. Then kind of work their way up to getting it fully working Python system that they built from source code directly. To be clear, I have some weird requirements around this stuff.You do not consider what I do normally. You're a special case. I use foot on that project. I've used Hatchling as others just because Hatchling automatically cares about what's in your Git repo versus not wanting to create the source distribution, which is kind of a cool thing to have because then I don't have to think about it. My source distributions for projects like MicroVam just include everything that's in the Git repo. It puts it up in Turbo, and it builds the wheels as appropriate, so it's all taken care of.\n\nAfter that, honestly, everything else I do out of habit do manually. I use them to create my virtual environments. At this point, I use the `create environment` command because I get to help dictate the feature set. I work with my team to make it do what I want, and it works great for me. That whole flow is designed specifically to be opinionated because we're trying to do what we think would make sense for beginners. It isn't geared towards advanced users specifically because we know advanced users can use a terminal. Otherwise, my workflow is either using the `create environment` command or my Python launcher to run Python.\n\nI'm a Nox user personally for task runner. Hatchling is kind of newish, but maybe it's just me not being aware of it. The name feels like it came from eggs, which is interesting. Hatchling is the build system provided by Hatch, so that's the relationship. Hatch is the higher-level tool that will auto-create virtual environments and manage those for you unless you configure different virtual environment needs for testing versus other things. Hatchling is just a build system.\n\nI've been doing this for so long that I have my own workflow that I'm happy to type out manually. I don't use workflow tools like Hatch, Poetry, or Pipenv because I don't need them. It's not necessary for me. I use Starship, the Python launcher, and it all just works for me. Some people absolutely love Hatch and that kind of workflow tooling that manages all that stuff for them. Personally, I am literally a Python launcher and Nox to run stuff, and otherwise, it's whatever VS Code provides because I get to make that happen.\n\nThere's a packaging Summit happening at PyCon split between two days, Friday morning and Sunday morning. I have opinions on it. One thing I found interesting is how the complaints about packaging have changed over the years. Everyone complains about packaging, no matter who you are or where you're from.There's historical context missing here that's making people feel frustrated without realizing the historical context. Python was released in February of 1991, predating Linux and the wide usage of the World Wide Web. Guido, the creator of Python, was working on one of the first graphical browsers called Gradle while NCSA Mosaic was being developed. Back then, packages were downloaded from the Vaults of Parnassus website or from Usenet.\n\nAs time passed, tools like PiPA, Pipi, and easy install came about to address the need for setup tools. Pi project.tunnel was developed to make setup tools optional instead of a requirement. This led to a proliferation of tooling around Python, including workflow tools like Pipenv, Poetry, and PDM. New build backends were introduced to build wheels without setup tools.\n\nDespite the previous complaints about not having options, people are now complaining about having too many options. The proliferation of tools was intentionally done to provide flexibility for developers. The scientific community, for example, can now build their projects using their own tool chain without relying on setup tools.\n\nWhile some may find the abundance of choices overwhelming, having options allows developers to find the right tool for their specific needs. Packaging may seem complex, but it's only complicated when going beyond pure python. It's important to understand the history behind the tools and how they evolved to meet the needs of developers.Right, it's literally just a zip file, right? Pip can just install those instantaneously. Hell, you can work, you can just install those and use them with WebAssembly, right? They just plop down and they just run, right? That's the magic of .py files in Python. But as soon as you want to add that bit of C code, or that bit of Rust code, or the bit of Fortran code, things get really complicated really fast.\n\nI think the other reason beginners, by the way, want an answer is because they either are in a rush, yes, they don't want to take the time, or they're afraid of making the wrong decision based on a lack of knowledge. Yeah, the brain can only hold so many ideas in their head at the moment, right now.\n\nAnd honestly, I think it's a bit of, there's a term like, it's not abundance paralysis, but basically when you freeze up when you have too much choice, yeah. And I think that's what happens with beginners is that, oh my God, there's just too many options, as you said, like walking on a car lot and going, oh, there's too many choices here. It's like, or walk in the cereal aisle and yeah, exactly, like what kind of ketchup do I want to buy, there's so many options, right? Exactly.\n\nIt's unfortunate but technically it doesn't matter in the end. And honestly, for a lot of these tools, the cost of changing isn't huge. The end result still has to land and be available, you know, like there's still like a good single, well, primarily single distribution point, right? Like your configuration is going to go on your project about Tumble as much of that's going to be consistent. After that, they're going to create virtual environments which tools understand and they're going to install stuff which tools understand. So you might have to tweak your config to say like, oh well, Poetry wants it written this way while Hatch wants it written this way for how to create my environments. But otherwise, all the other stuff's the same and the concepts are all the same. It's still creating a virtual environment for some version of Python you have installed and still running the code.\n\nIt's just whose commands look nicer to you really at the end and who adds that little extra finesse or shininess to the commands that they provide or configuration options that you feel you need or want to let you get there. But the whole point is to try to get all this so that you're working with standards and artifacts that everyone agrees to at the bottom layer. So this is really just a UI choice, it's not a critical aspect, right? This is why I am on a very long-term, multi-year project to try to get us lock file support, yeah, so that people stop viewing that as a \"I have to use this other tool here because it has lock files and no other tool has that or it doesn't quite work the way I need it to work or what have you,\" right? Like, I feel like that lock files is the last artifact that we don't have standardized that if we could, all these tools just become just UX decisions, it wouldn't be a feature set decision at that fundamental level and all that conversation about security and typo squatting and all this other kind of, well, not just the type of squatting but reproducibility it runs the gamut.\n\nI'm working on it, yeah, but it's going to take a very long time. I already think two years into it and it led to a reject. I think a 10 months into a PEP that got rejected ultimately and that was, yeah, that was one of our last conversations, yeah, a year and a half February of 2022 I got rejected and so I've been working since then to try to build up more or less my own installer that can do its own lock files as a proof of concept to show what it looks like and show that this is feasible and it works and this is what I would propose and to try to at least get people to rally around something so that we can get accepted. So I am working on it. Have you done that before for other projects where you felt like, alright, I truly got to show what this looks like and what this workflow is like? It really depends, yeah, I know that works out at a job that's a common thing for me is I need to stand something up, you know, and kind of show like a bit of a concept in order to get buy-in. I think it depends on for me personally, I'm very lucky.I have a specific position in this community within the Realms that I run in. I have been around long enough to have the right connections in many places. I know who to talk to for things. If I have a fully formed idea and I feel it's good, and I know the right people to talk to about it, I can skip the proof of concept stage. But if it's unclear, I prefer to do it to figure out what's wrong. For example, with the import system, I had to prove that I could get performance up before proposing changes.\n\nSometimes I write proposals without creating anything beforehand because I have a solid idea and know it can be implemented. It varies, and I am in a unique position where I can propose ideas and get them accepted without a detailed proof of concept.\n\nPeople often misunderstand the complexity that comes from Python being used as the glue of the internet. Python's flexibility leads to people trying to make it work in various ways, causing complexity in packaging, especially with native code like C and C++. The diversity of tools and options can be overwhelming, but it's necessary to accommodate different workflows and needs within the community.\n\nChoosing one standard workflow may not work for everyone, as different tools and approaches suit different purposes. It's important to consider the historical context and diverse perspectives within the community before criticizing the complexity of Python packaging. Some people may be unaware of these factors and approach the topic with a narrow viewpoint, not considering the needs and preferences of others.Oh everyone must just work this way because that's what I was taught or what I learned. Everybody in my building does, right? And they won't work like this. This is something I tell my team all the time at work when we deal with GitHub issues. You have to approach every issue assuming this person probably has not had a conversation with anyone outside of their friends, family, and co-workers who all live in the same town somewhere that is not where you're from. It's not a crazy statement to make if you really stop and think about it.\n\nIt is very easy if you don't think about us traveling to PyCon US. During the pandemic, I was just here in BC. I actually have not left the country since the pandemic started. PyCon US is going to be my first international trip since February of 2020. And I had not left the province of British Columbia until I went back East to visit friends earlier in late 2022.\n\nI luckily have enough co-workers internationally and friends in the Python community that I talk to people all over the world all the time. But it is totally reasonable to think that people don't have that exposure, which also extends to their tool chains. It's very easy for them to come in and say, \"Oh, blah blah, you probably don't know the history of this.\" So just so you know, Python's really old. We predate like npm or Rush. They learned from us, but it doesn't mean we can't learn from them.\n\nYou have to understand they learn from our mistakes. It's not like we suddenly ignored what they did and came up with our own thing. It's like, no, we were already doing stuff. Check out this timeline. They looked at us like, \"Oh, yeah, maybe we should do it differently or something.\" And then they did it, and now you're getting this comparison. But yeah, it's not like in August of 2012, Python and the node communities both sat down independently and both chose an approach and didn't talk to each other. That's not how these timelines work.\n\nIt's understandable how people end up in this position, and some people are really good about understanding this when you explain it to them. Historical context, what we're working with here, what we're doing to be flexible. You wanted to simplify your lives, but you will probably not get the workflow you want. You're going to lose access to all those extension modules and integrations. All the data scientists, guess what? You would suddenly lose all of your AI integrations because that's mostly written in C++ underneath the bottom layers.\n\nAll that GPU access, they ain't doing that in pure Python. All this stuff would just disappear. All this stuff people wrote to be faster. Hopefully, the faster CPython team continues to make things faster so people stop reaching for C for that perspective. You don't need Cython as much anymore, and all that stuff to make things go faster. You just write your Python, and it gets faster for free every release, to some extent, or at least over time.\n\nI think people just have to take a step back. What you're doing might be simple, but your dependencies, what they're doing, might not be simple. Just trying to make all this work is a hard problem with this level of flexibility. It totally makes sense, and that's something that is hard to interject into the conversation and make sure that's happening. That's my feedback on the whole discussion in terms of outcomes.\n\nI think everyone's still processing what's going on. I think if we take a step back, it'll be interesting to see what comes up at the Packaging Summit at PyCon. If anything, any fruition from those discussions comes about. I think there is an interesting question about whether it's time to establish something like a council setup because, at least on the PyPI side of things, it's not complete anarchy, but it is very leaderless by design.\n\nPaul Moore and I think Donald stuff technically are both people who can be delegated to make decisions on packaging paths without talking to the steering council. But that's a lot to ask of one person, and Paul isn't directly driving visions or anything on purpose. He's just trying to go based on consensus and any gut feel he might have on whether a PEP gets accepted.Maybe it's time to have that discussion. Maybe it's time to have a very small, discrete group of people who are trusted to go off, think things through, and figure something out. Asking the world leads to multi-hundred post threads on discuss.python.org across multiple platforms, which run the gamut. Once again, everyone has an opinion, and everyone's opinion is different. Everyone wants different needs, especially at this layer. It might be time for some people to make tough calls and tell some people, \"Nope, we're not doing that. We're going to do this, and this is the way it's going to look.\" Maybe having multiple tools is okay, or here are some coordination points. I think that discussion is probably going to happen shortly, and a decision will be made. It's an interesting thing that came out of that discussion: do we want to have a more structured way of making decisions?\n\nIt sounds like you're going to have a busy PyCon. Leading the language Summit, leading the web assembly Summit, giving a talk, and participating in an \"ask me anything\" at the Microsoft Booth. Along with the hallway track talks and the packaging Summit, it's going to be busy. PyCons are fun but not relaxing.\n\nI have weekly questions to ask, and we skipped them in the previous episode. What's something you're excited about in the world of Python right now? The whole Wazzy stuff around Python is exciting, especially in the long term. Participating in the Bytecode Alliance and having conversations with thoughtful people about challenges in the Python community is intriguing. I'm excited to see where this all ends up and if my dream of making distribution easier can come true.\n\nI'm excited about learning more about Rust and concurrency. I'm reading the second edition of the \"Programming Rust\" book slowly. When I started learning Rust, I wanted a project, and the Python launcher for Windows became that project. I want to do a \"pip\" thing that does exactly what I want. I want to learn more about Rust, focusing on concurrency, and continue to make that my project.It's a good book, it's thick. You definitely will learn the language, though. Okay, yeah. \n\nHow can people follow along with the work that you do online? What are the best ways to do that? So, it depends on what you're exactly looking for. There's my blog snarky.ca. Fair warning, the blog posts are typically not short, hence why they're not frequent like weekly or necessarily monthly. \n\nYeah, I am on Mastodon. I'm Brett Cannon at fossadon.org. If you're more interested in the work stuff like the VS code related stuff, you can either follow the Python blog for that, that's AKA dot Ms slash python blog, or you can just follow the VS code release notes for the really high-level, important stuff that the Python team does. But more detail and smaller stuff ends up in our team blog that is separate from VS code just because it existed before we joined the VS code team. \n\nThat's pretty much it. So, yeah, personal blog, Mastodon, work blog. Awesome. \n\nWell, Brett, thanks again for coming on the show and having these couple conversations with me. I'm looking forward to seeing you in Salt Lake City again, catching up again. Yeah, guys, it's only two weeks from now. \n\nAll right, so my brain has not quite clicked into the fact that I'm going to be traveling that soon. Yeah, all right. Well, thanks again, see you then. \n\nI want to thank Brett Cannon for coming on the show this week. \n\nTo the Real Python Podcast, make sure that you click that follow button in your podcast player. And if you see a subscribe button somewhere, remember that the Real Python Podcast is free. If you like the show, please leave us a review. \n\nYou can find show notes with links to all the topics we spoke about inside your podcast player or at realpython.com/podcast. And while you're there, you can leave us a question or a topic idea. \n\nI've been your host Christopher Bailey. I look forward to talking to you soon."
}